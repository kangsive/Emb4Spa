{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, LineString, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 63, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample tensor\n",
    "tensor = np.random.rand(32, 64, 2)\n",
    "\n",
    "# Compute the differences along the second dimension\n",
    "diff_tensor = np.diff(tensor, axis=1)\n",
    "\n",
    "print(diff_tensor.shape)  # Output: (32, 63, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 63, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sample tensor\n",
    "tensor = torch.rand(32, 64, 2)\n",
    "\n",
    "# Compute the differences along the second dimension\n",
    "diff_tensor = tensor[:, 1:, :] - tensor[:, :-1, :]\n",
    "\n",
    "print(diff_tensor.shape)  # Output: torch.Size([32, 63, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        _, (h, c) = self.lstm(input_seq)\n",
    "        return h, c\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, h, c):\n",
    "        batch_size, seq_len, _ = input_seq.shape\n",
    "        z = h.reshape(batch_size, 1, -1).repeat(1, seq_len, 1)\n",
    "        input_seq = torch.cat([z, input_seq], dim=-1)\n",
    "        output, _ = self.lstm(input_seq, (h, c))\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Define the sequence-based encoder-decoder model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        h, c = self.encoder(input_seq)\n",
    "        output = self.decoder(input_seq, h, c)\n",
    "        return output\n",
    "\n",
    "# Training example\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# Generate random training data\n",
    "train_data = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(input_size + hidden_size, hidden_size, output_size)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(train_data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, train_data)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def compute_NN(X, Y):\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    neigh = NearestNeighbors(n_neighbors = 1)\n",
    "    neigh.fit(X)\n",
    "\n",
    "    knn_array = neigh.kneighbors(return_distance=False)\n",
    "    knn_array = knn_array.reshape(1, -1)    # for k=1, we can flaten the array\n",
    "    matches = Y[knn_array] == Y     # Match nns' labels with corresponding y\n",
    "    avg_acc = matches.mean()\n",
    "\n",
    "    return avg_acc\n",
    "\n",
    "def compute_FT(X, Y, k=None):\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    types, k_list = np.unique(Y, return_counts=True)\n",
    "    neigh = NearestNeighbors(n_neighbors = k)\n",
    "    neigh.fit(X)\n",
    "\n",
    "    avg_acc = 0\n",
    "\n",
    "    for i in range(len(types)):\n",
    "        X_t, k = X[Y == types[i]], k_list[i]\n",
    "        knn_array = neigh.kneighbors(X_t, n_neighbors=k, return_distance=False)     # Get knn for every instance\n",
    "        Y_t = np.full(knn_array.shape, types[i])    # create ground truth for every nn\n",
    "        matches = Y[knn_array] == Y_t    # Match knns' labels with corresponding y\n",
    "        avg_acc += (matches.sum(axis=-1)/k).mean()   # Calulate acc of each row (instance) and average them\n",
    "\n",
    "    return avg_acc/len(k_list)\n",
    "\n",
    "\n",
    "def compute_ST(X, Y, k=None):\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    types, k_list = np.unique(Y, return_counts=True)\n",
    "    neigh = NearestNeighbors(n_neighbors = k)\n",
    "    neigh.fit(X)\n",
    "\n",
    "    avg_acc = 0\n",
    "\n",
    "    for i in range(len(types)):\n",
    "        X_t, k = X[Y == types[i]], k_list[i]\n",
    "        knn_array = neigh.kneighbors(X_t, n_neighbors=2*k, return_distance=False)     # Now we look at top 2*k nns\n",
    "        Y_t = np.full(knn_array.shape, types[i])    # create ground truth for every nn\n",
    "        matches = Y[knn_array] == Y_t    # Match knns' labels with corresponding y\n",
    "        avg_acc += (matches.sum(axis=-1)/k).mean()   # Calulate acc of each row (instance) and average them\n",
    "\n",
    "    return avg_acc/len(k_list)\n",
    "\n",
    "    # return round(correct*1.0/k, 3), knn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "loaded = np.load(\"dataset/train_building_shape_5k.npz\")\n",
    "\n",
    "split_ratio = 0.2\n",
    "train_tokens, val_tokens, train_labels, val_labels = train_test_split(loaded[\"train_tokens\"], loaded[\"train_labels\"], test_size=split_ratio, random_state=42)\n",
    "# train_tokens, train_labels = loaded[\"train_tokens\"], loaded[\"train_labels\"]\n",
    "\n",
    "train_tokens = train_tokens.reshape(train_tokens.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.428367780917627, 0.5544760435345489)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute_NN(train_tokens, train_labels)\n",
    "compute_FT(train_tokens, train_labels), compute_ST(train_tokens, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prepare_dataset import prepare_dataset_mnist\n",
    "max_seq_len = 64\n",
    "batch_size = 64\n",
    "dataset_size = None\n",
    "with_mask = False\n",
    "\n",
    "train_tokens, train_labels, train_mask, val_tokens, val_labels, val_mask = prepare_dataset_mnist(file=\"dataset/building_shapes_5010.csv\",\n",
    "                                                                                                 with_mask=with_mask,\n",
    "                                                                                                 split_ratio=0.2,\n",
    "                                                                                                 dataset_size=dataset_size,\n",
    "                                                                                                 max_seq_len=max_seq_len,\n",
    "                                                                                                 train=True)\n",
    "\n",
    "train_tokens = train_tokens[:, :, :2]\n",
    "train_tokens = train_tokens.reshape(train_tokens.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15763092303377366, 0.257442129680529)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_FT(train_tokens, train_labels), compute_ST(train_tokens, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
