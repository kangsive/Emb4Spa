{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "from utils.vector2shape import reverse_vector_polygon\n",
    "from main_finetune import get_finetune_dataset_mnist\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"dataset/mnist_polygon_train_10k.csv\"\n",
    "train_tokens, train_labels, val_tokens, val_labels  = get_finetune_dataset_mnist(train_data_path, dataset_size=None, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = \"dataset/mnist_polygon_test_2k.npz\"\n",
    "test_tokens, test_labels = get_finetune_dataset_mnist(file=test_dataset, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flag = [1] * train_tokens.shape[0]\n",
    "test_flag = [0] * test_tokens.shape[0]\n",
    "valid_flag = [-1] * val_tokens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.cat([train_tokens, test_tokens, val_tokens], dim=0)\n",
    "labels = torch.cat([train_labels, test_labels, val_labels], dim=0)\n",
    "flags = train_flag + test_flag + valid_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = [reverse_vector_polygon(one) for one in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vectorizer import num_points_from_wkt\n",
    "for one in polys:\n",
    "    if num_points_from_wkt(one.wkt) != 64:\n",
    "        print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(1, len(polys)+1))\n",
    "coors = tokens[:, :, :2].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = polys[0]\n",
    "xy = a.interiors[0].coords.xy\n",
    "np.array(xy).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgon_list = []\n",
    "for pgon in polys:\n",
    "    ex_coors = np.expand_dims(np.array(pgon.exterior.coords), axis = 0)\n",
    "    if len(pgon.interiors):\n",
    "        in_coors = np.concatenate([np.array(interior.coords.xy).T for interior in pgon.interiors], axis = 0)\n",
    "        in_coors = np.expand_dims(in_coors, axis = 0)\n",
    "        coors = np.concatenate([in_coors, ex_coors], axis = 1)\n",
    "    else:\n",
    "        coors = ex_coors\n",
    "    pgon_list.append(coors)\n",
    "pgon_list = np.concatenate(pgon_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame({\"ID\": ids, \"TYPEID\": labels, \"SPLIT_0\": flags, \"geometry_norm\": polys, \"geom_coors\": coors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_pickle(\"dataset/mnist_resnet1d_12k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate launching multiple different jobs that log to the same experiment\n",
    "\n",
    "import wandb\n",
    "import math\n",
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "  job_type = \"rollout\"\n",
    "  if i == 2:\n",
    "    job_type = \"eval\"\n",
    "  if i == 3:\n",
    "    job_type = \"eval2\"\n",
    "  if i == 4:\n",
    "    job_type = \"optimizer\"\n",
    "\n",
    "  # Set group and job_type to see auto-grouping in the UI\n",
    "  wandb.init(project=\"group-demo\", \n",
    "             group=\"exp_\" + str(1),\n",
    "             entity=\"kangisve\",\n",
    "             job_type=job_type)\n",
    "\n",
    "  for j in range(100):\n",
    "    acc = 0.1 * (math.log(1 + j + .1) + random.random())\n",
    "    val_acc = 0.1 * (math.log(1+ j + 2) + random.random() + random.random())\n",
    "    if (j % 10 == 0):\n",
    "      wandb.log({\"acc\":acc, \"val_acc\":val_acc})\n",
    "  \n",
    "  # Using this to mark a run complete in a notebook context\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the Transformer model: 218807\n"
     ]
    }
   ],
   "source": [
    "from potae import PoTAE\n",
    "\n",
    "pot = PoTAE(num_layers=3)\n",
    "total_params_pot_model = sum(p.numel() for p in pot.parameters())\n",
    "print(f\"Total number of parameters in the Transformer model: {total_params_pot_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
