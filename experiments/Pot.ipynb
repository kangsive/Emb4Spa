{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer using Pytorch Transformer Encoder Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mask is different here, only (batch_size, seq_len) mask where True stands for invalid (mask) attention queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import importlib\n",
    "importlib.import_module(\"utils\")\n",
    "from utils.prepare_dataset import prepare_dataset, prepare_dataset_fixedsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pot(nn.Module):\n",
    "    def __init__(self, fea_dim=7, d_model=30, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(dim_feedforward, num_types))\n",
    "        self.projection = nn.Linear(fea_dim, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None, pre_train=False):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        x = self.projection(x)\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.projection(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.zeros((x.size(0), 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        if not pre_train:\n",
    "            x = x[:, 0, :] # grab the class embedding\n",
    "            x = self.mlp_head(x)\n",
    "        else:\n",
    "            x = x[:, 1:, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, dense_size, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(input_size, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(dense_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "geom_train, geom_test, label_train, label_test, gs = prepare_dataset_fixedsize(dataset_size=2000)\n",
    "\n",
    "train_tokens = torch.tensor(geom_train, dtype=torch.float32)\n",
    "test_tokens = torch.tensor(geom_test, dtype=torch.float32)\n",
    "train_labels= torch.tensor(label_train, dtype=torch.long)\n",
    "test_labels = torch.tensor(label_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(test_tokens, test_labels), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.139152002334595, Train Acc 0.13875, Val Loss: 2.1169678483690535, Val Acc: 0.145\n",
      "Epoch: 2, Train Loss: 2.097270174026489, Train Acc 0.154375, Val Loss: 2.0723682131086076, Val Acc: 0.1525\n",
      "Epoch: 3, Train Loss: 2.0457476806640624, Train Acc 0.18125, Val Loss: 2.0220488820757185, Val Acc: 0.1975\n",
      "Epoch: 4, Train Loss: 1.9825956678390504, Train Acc 0.213125, Val Loss: 1.9853921617780412, Val Acc: 0.195\n",
      "Epoch: 5, Train Loss: 1.9659615755081177, Train Acc 0.20875, Val Loss: 1.9311999593462263, Val Acc: 0.2275\n",
      "Epoch: 6, Train Loss: 1.9479502248764038, Train Acc 0.240625, Val Loss: 1.8938300439289637, Val Acc: 0.275\n",
      "Epoch: 7, Train Loss: 1.9116272449493408, Train Acc 0.24125, Val Loss: 1.885062643459865, Val Acc: 0.305\n",
      "Epoch: 8, Train Loss: 1.8945185375213622, Train Acc 0.255625, Val Loss: 1.9072721004486084, Val Acc: 0.265\n",
      "Epoch: 9, Train Loss: 1.90451171875, Train Acc 0.275625, Val Loss: 1.8680494172232491, Val Acc: 0.27\n",
      "Epoch: 10, Train Loss: 1.8773003435134887, Train Acc 0.283125, Val Loss: 1.870361362184797, Val Acc: 0.2775\n",
      "Epoch: 11, Train Loss: 1.8654389238357545, Train Acc 0.28625, Val Loss: 1.875125425202506, Val Acc: 0.3025\n",
      "Epoch: 12, Train Loss: 1.8607380342483522, Train Acc 0.2775, Val Loss: 1.884733864239284, Val Acc: 0.2925\n",
      "Epoch: 13, Train Loss: 1.8488388681411743, Train Acc 0.303125, Val Loss: 1.8515631641660417, Val Acc: 0.2875\n",
      "Epoch: 14, Train Loss: 1.8552076482772828, Train Acc 0.28875, Val Loss: 1.8490461962563651, Val Acc: 0.315\n",
      "Epoch: 15, Train Loss: 1.8364896535873414, Train Acc 0.29125, Val Loss: 1.8623710870742798, Val Acc: 0.3025\n",
      "Epoch: 16, Train Loss: 1.832700595855713, Train Acc 0.28875, Val Loss: 1.8442773818969727, Val Acc: 0.345\n",
      "Epoch: 17, Train Loss: 1.8411869859695436, Train Acc 0.299375, Val Loss: 1.8435136250087194, Val Acc: 0.33\n",
      "Epoch: 18, Train Loss: 1.8260341358184815, Train Acc 0.305, Val Loss: 1.827055743762425, Val Acc: 0.31\n",
      "Epoch: 19, Train Loss: 1.8205789518356323, Train Acc 0.305, Val Loss: 1.8523557697023665, Val Acc: 0.31\n",
      "Epoch: 20, Train Loss: 1.816336340904236, Train Acc 0.3125, Val Loss: 1.8527616603033883, Val Acc: 0.3175\n",
      "Epoch: 21, Train Loss: 1.8230456590652466, Train Acc 0.299375, Val Loss: 1.8416282279150826, Val Acc: 0.335\n",
      "Epoch: 22, Train Loss: 1.8129509639739991, Train Acc 0.315625, Val Loss: 1.8461143118994576, Val Acc: 0.32\n",
      "Epoch: 23, Train Loss: 1.8078702402114868, Train Acc 0.295625, Val Loss: 1.8327954837254115, Val Acc: 0.3425\n",
      "Epoch: 24, Train Loss: 1.812639489173889, Train Acc 0.319375, Val Loss: 1.8483595166887556, Val Acc: 0.2925\n",
      "Epoch: 25, Train Loss: 1.8088822841644288, Train Acc 0.3225, Val Loss: 1.8378136157989502, Val Acc: 0.345\n",
      "Epoch: 26, Train Loss: 1.7926507425308227, Train Acc 0.30375, Val Loss: 1.8351840121405465, Val Acc: 0.325\n",
      "Epoch: 27, Train Loss: 1.7878174591064453, Train Acc 0.309375, Val Loss: 1.8344717706952776, Val Acc: 0.31\n",
      "Epoch: 28, Train Loss: 1.7855802011489867, Train Acc 0.323125, Val Loss: 1.8408644029072352, Val Acc: 0.31\n",
      "Epoch: 29, Train Loss: 1.789639058113098, Train Acc 0.3125, Val Loss: 1.8600538287843977, Val Acc: 0.315\n",
      "Epoch: 30, Train Loss: 1.78082172870636, Train Acc 0.31125, Val Loss: 1.8599687303815569, Val Acc: 0.32\n",
      "Epoch: 31, Train Loss: 1.7802225923538209, Train Acc 0.315625, Val Loss: 1.8236358165740967, Val Acc: 0.33\n",
      "Epoch: 32, Train Loss: 1.7744349193573, Train Acc 0.3225, Val Loss: 1.8184901816504342, Val Acc: 0.34\n",
      "Epoch: 33, Train Loss: 1.7640516471862793, Train Acc 0.326875, Val Loss: 1.8450519016810827, Val Acc: 0.3225\n",
      "Epoch: 34, Train Loss: 1.7422109460830688, Train Acc 0.351875, Val Loss: 1.826982021331787, Val Acc: 0.3275\n",
      "Epoch: 35, Train Loss: 1.7496183681488038, Train Acc 0.3325, Val Loss: 1.8376151663916451, Val Acc: 0.335\n",
      "Epoch: 36, Train Loss: 1.7517811489105224, Train Acc 0.32625, Val Loss: 1.8264199325016566, Val Acc: 0.37\n",
      "Epoch: 37, Train Loss: 1.7549162673950196, Train Acc 0.33375, Val Loss: 1.821758202144078, Val Acc: 0.34\n",
      "Epoch: 38, Train Loss: 1.7398726272583007, Train Acc 0.3325, Val Loss: 1.825314896447318, Val Acc: 0.345\n",
      "Epoch: 39, Train Loss: 1.7391770124435424, Train Acc 0.33625, Val Loss: 1.858700156211853, Val Acc: 0.315\n",
      "Epoch: 40, Train Loss: 1.7510526990890503, Train Acc 0.35125, Val Loss: 1.853012238230024, Val Acc: 0.3575\n",
      "Epoch: 41, Train Loss: 1.7287484073638917, Train Acc 0.34375, Val Loss: 1.8195528302873885, Val Acc: 0.3375\n",
      "Epoch: 42, Train Loss: 1.7347065210342407, Train Acc 0.34, Val Loss: 1.8168864761080061, Val Acc: 0.3475\n",
      "Epoch: 43, Train Loss: 1.710680651664734, Train Acc 0.369375, Val Loss: 1.846412079674857, Val Acc: 0.355\n",
      "Epoch: 44, Train Loss: 1.7103471994400024, Train Acc 0.3575, Val Loss: 1.8055717604500907, Val Acc: 0.3625\n",
      "Epoch: 45, Train Loss: 1.7262145280838013, Train Acc 0.34875, Val Loss: 1.8234097106116158, Val Acc: 0.32\n",
      "Epoch: 46, Train Loss: 1.7320922374725343, Train Acc 0.345625, Val Loss: 1.8346675634384155, Val Acc: 0.355\n",
      "Epoch: 47, Train Loss: 1.7051528072357178, Train Acc 0.35875, Val Loss: 1.8309372493198939, Val Acc: 0.3475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m eval_loss, eval_acc\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 53\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(pot, train_loader)\n\u001b[1;32m     54\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m evaluate(pot, val_loader)\n\u001b[1;32m     55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device), batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m pot(batch_x)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mPot.forward\u001b[0;34m(self, x, mask, pre_train)\u001b[0m\n\u001b[1;32m     31\u001b[0m         cls_mask \u001b[39m=\u001b[39m cls_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cls_mask, mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x, src_key_padding_mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pre_train:\n\u001b[1;32m     37\u001b[0m     x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m, :] \u001b[39m# grab the class embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:538\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    539\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:546\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    545\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 546\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    547\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    548\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    549\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/functional.py:5163\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5161\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   5162\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m-> 5163\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m dropout(attn_output_weights, p\u001b[39m=\u001b[39;49mdropout_p)\n\u001b[1;32m   5165\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(attn_output_weights, v)\n\u001b[1;32m   5167\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pot = Pot(fea_dim=5, d_model=32, nhead=8, num_layers=1, max_seq_len=64, dim_feedforward=32, dropout=0.1, num_types=9)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=-1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = pot(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=-1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# # Test\n",
    "# test_loss, test_acc = evaluate(pot, test_loader)\n",
    "# print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.prepare_dataset import prepare_dataset_fixedsize\n",
    "\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.132039213180542, Train Acc 0.141875, Val Loss: 2.0961855479649136, Val Acc: 0.145\n",
      "Epoch: 2, Train Loss: 2.0577419900894167, Train Acc 0.200625, Val Loss: 2.0344483171190535, Val Acc: 0.1875\n",
      "Epoch: 3, Train Loss: 2.020531635284424, Train Acc 0.211875, Val Loss: 2.031015532357352, Val Acc: 0.205\n",
      "Epoch: 4, Train Loss: 2.0081984186172486, Train Acc 0.2025, Val Loss: 2.010474579674857, Val Acc: 0.1825\n",
      "Epoch: 5, Train Loss: 1.9823488569259644, Train Acc 0.20875, Val Loss: 1.99062408719744, Val Acc: 0.2225\n",
      "Epoch: 6, Train Loss: 1.965613226890564, Train Acc 0.216875, Val Loss: 1.9332626376833235, Val Acc: 0.2775\n",
      "Epoch: 7, Train Loss: 1.9366360330581665, Train Acc 0.243125, Val Loss: 1.9021392038890295, Val Acc: 0.295\n",
      "Epoch: 8, Train Loss: 1.882493634223938, Train Acc 0.27375, Val Loss: 1.8604224409375871, Val Acc: 0.275\n",
      "Epoch: 9, Train Loss: 1.8495013809204102, Train Acc 0.305625, Val Loss: 1.8233354091644287, Val Acc: 0.3225\n",
      "Epoch: 10, Train Loss: 1.8289145469665526, Train Acc 0.29375, Val Loss: 1.810569473675319, Val Acc: 0.3275\n",
      "Epoch: 11, Train Loss: 1.832412600517273, Train Acc 0.29875, Val Loss: 1.808701685496739, Val Acc: 0.3525\n",
      "Epoch: 12, Train Loss: 1.828912959098816, Train Acc 0.320625, Val Loss: 1.8299809694290161, Val Acc: 0.3175\n",
      "Epoch: 13, Train Loss: 1.808986659049988, Train Acc 0.316875, Val Loss: 1.789123603275844, Val Acc: 0.315\n",
      "Epoch: 14, Train Loss: 1.8025607061386109, Train Acc 0.320625, Val Loss: 1.803123848778861, Val Acc: 0.32\n",
      "Epoch: 15, Train Loss: 1.807176914215088, Train Acc 0.306875, Val Loss: 1.8066481011254447, Val Acc: 0.295\n",
      "Epoch: 16, Train Loss: 1.79416166305542, Train Acc 0.325625, Val Loss: 1.798423205103193, Val Acc: 0.325\n",
      "Epoch: 17, Train Loss: 1.784902753829956, Train Acc 0.31875, Val Loss: 1.7914273568562098, Val Acc: 0.3125\n",
      "Epoch: 18, Train Loss: 1.8111754083633422, Train Acc 0.33625, Val Loss: 1.7992930923189436, Val Acc: 0.3225\n",
      "Epoch: 19, Train Loss: 1.7981256294250487, Train Acc 0.306875, Val Loss: 1.7936293227331979, Val Acc: 0.32\n",
      "Epoch: 20, Train Loss: 1.7876895141601563, Train Acc 0.32875, Val Loss: 1.7909329278128487, Val Acc: 0.3375\n",
      "Epoch: 21, Train Loss: 1.7821267747879028, Train Acc 0.33375, Val Loss: 1.793521523475647, Val Acc: 0.3525\n",
      "Epoch: 22, Train Loss: 1.801377820968628, Train Acc 0.315, Val Loss: 1.7916508061545235, Val Acc: 0.34\n",
      "Epoch: 23, Train Loss: 1.7876618957519532, Train Acc 0.329375, Val Loss: 1.7831587621143885, Val Acc: 0.33\n",
      "Epoch: 24, Train Loss: 1.7660819578170777, Train Acc 0.34625, Val Loss: 1.7997074467795235, Val Acc: 0.31\n",
      "Epoch: 25, Train Loss: 1.7830892276763917, Train Acc 0.325, Val Loss: 1.7879831109728133, Val Acc: 0.305\n",
      "Epoch: 26, Train Loss: 1.781012635231018, Train Acc 0.325625, Val Loss: 1.7865600415638514, Val Acc: 0.315\n",
      "Epoch: 27, Train Loss: 1.768756446838379, Train Acc 0.33875, Val Loss: 1.7854038136346, Val Acc: 0.335\n",
      "Epoch: 28, Train Loss: 1.7679923963546753, Train Acc 0.3425, Val Loss: 1.7874068702970232, Val Acc: 0.34\n",
      "Epoch: 29, Train Loss: 1.7744356822967529, Train Acc 0.321875, Val Loss: 1.7842379467827933, Val Acc: 0.315\n",
      "Epoch: 30, Train Loss: 1.7936260938644408, Train Acc 0.31875, Val Loss: 1.8005082777568273, Val Acc: 0.2775\n",
      "Epoch: 31, Train Loss: 1.781724019050598, Train Acc 0.324375, Val Loss: 1.7865871701921736, Val Acc: 0.325\n",
      "Epoch: 32, Train Loss: 1.7579054975509643, Train Acc 0.3375, Val Loss: 1.7810437168393816, Val Acc: 0.315\n",
      "Epoch: 33, Train Loss: 1.7625316572189331, Train Acc 0.3275, Val Loss: 1.7798452036721366, Val Acc: 0.33\n",
      "Epoch: 34, Train Loss: 1.7610064601898194, Train Acc 0.33125, Val Loss: 1.7753995656967163, Val Acc: 0.3175\n",
      "Epoch: 35, Train Loss: 1.7571851015090942, Train Acc 0.33125, Val Loss: 1.8038281202316284, Val Acc: 0.2975\n",
      "Epoch: 36, Train Loss: 1.7731724405288696, Train Acc 0.325625, Val Loss: 1.7795266253607613, Val Acc: 0.3125\n",
      "Epoch: 37, Train Loss: 1.772287096977234, Train Acc 0.32875, Val Loss: 1.7808362756456648, Val Acc: 0.3075\n",
      "Epoch: 38, Train Loss: 1.7708817434310913, Train Acc 0.33125, Val Loss: 1.7794281925473894, Val Acc: 0.325\n",
      "Epoch: 39, Train Loss: 1.7521746349334717, Train Acc 0.34375, Val Loss: 1.7791780744280135, Val Acc: 0.3125\n",
      "Epoch: 40, Train Loss: 1.7513074922561644, Train Acc 0.3275, Val Loss: 1.7783572673797607, Val Acc: 0.33\n",
      "Epoch: 41, Train Loss: 1.7442545461654664, Train Acc 0.3425, Val Loss: 1.778790099280221, Val Acc: 0.32\n",
      "Epoch: 42, Train Loss: 1.7632269668579101, Train Acc 0.32625, Val Loss: 1.788940123149327, Val Acc: 0.3275\n",
      "Epoch: 43, Train Loss: 1.7450154542922973, Train Acc 0.34375, Val Loss: 1.7773728200367518, Val Acc: 0.33\n",
      "Epoch: 44, Train Loss: 1.7531307697296143, Train Acc 0.33375, Val Loss: 1.7710787057876587, Val Acc: 0.315\n",
      "Epoch: 45, Train Loss: 1.742947793006897, Train Acc 0.335625, Val Loss: 1.78007938180651, Val Acc: 0.32\n",
      "Epoch: 46, Train Loss: 1.7536813926696777, Train Acc 0.3375, Val Loss: 1.770647576877049, Val Acc: 0.3175\n",
      "Epoch: 47, Train Loss: 1.7511376094818116, Train Acc 0.336875, Val Loss: 1.776695932660784, Val Acc: 0.325\n",
      "Epoch: 48, Train Loss: 1.750408525466919, Train Acc 0.346875, Val Loss: 1.7760816642216273, Val Acc: 0.315\n",
      "Epoch: 49, Train Loss: 1.7353503179550172, Train Acc 0.34625, Val Loss: 1.7820784875324793, Val Acc: 0.305\n",
      "Epoch: 50, Train Loss: 1.7428169679641723, Train Acc 0.33375, Val Loss: 1.7734263113566808, Val Acc: 0.3125\n",
      "Epoch: 51, Train Loss: 1.7418315076828004, Train Acc 0.33875, Val Loss: 1.767151951789856, Val Acc: 0.32\n",
      "Epoch: 52, Train Loss: 1.744004759788513, Train Acc 0.34625, Val Loss: 1.7866016796657018, Val Acc: 0.3025\n",
      "Epoch: 53, Train Loss: 1.754635729789734, Train Acc 0.34125, Val Loss: 1.772736941065107, Val Acc: 0.3175\n",
      "Epoch: 54, Train Loss: 1.7492490196228028, Train Acc 0.324375, Val Loss: 1.781165940420968, Val Acc: 0.3\n",
      "Epoch: 55, Train Loss: 1.759817409515381, Train Acc 0.331875, Val Loss: 1.7925216470445906, Val Acc: 0.3075\n",
      "Epoch: 56, Train Loss: 1.750364761352539, Train Acc 0.33375, Val Loss: 1.7853844676698958, Val Acc: 0.3075\n",
      "Epoch: 57, Train Loss: 1.7359113693237305, Train Acc 0.33375, Val Loss: 1.7818742649895805, Val Acc: 0.3225\n",
      "Epoch: 58, Train Loss: 1.7429180097579957, Train Acc 0.341875, Val Loss: 1.777468970843724, Val Acc: 0.3175\n",
      "Epoch: 59, Train Loss: 1.742773027420044, Train Acc 0.339375, Val Loss: 1.802648867879595, Val Acc: 0.33\n",
      "Epoch: 60, Train Loss: 1.7309474325180054, Train Acc 0.33, Val Loss: 1.771316749708993, Val Acc: 0.3175\n",
      "Epoch: 61, Train Loss: 1.7323671054840089, Train Acc 0.358125, Val Loss: 1.7986044543130058, Val Acc: 0.325\n",
      "Epoch: 62, Train Loss: 1.7467075824737548, Train Acc 0.335625, Val Loss: 1.7754297426768713, Val Acc: 0.3275\n",
      "Epoch: 63, Train Loss: 1.7264249229431152, Train Acc 0.334375, Val Loss: 1.7893826791218348, Val Acc: 0.325\n",
      "Epoch: 64, Train Loss: 1.7316192293167114, Train Acc 0.3425, Val Loss: 1.7789982386997767, Val Acc: 0.315\n",
      "Epoch: 65, Train Loss: 1.7319995594024657, Train Acc 0.346875, Val Loss: 1.7707589864730835, Val Acc: 0.3225\n",
      "Epoch: 66, Train Loss: 1.714807243347168, Train Acc 0.35125, Val Loss: 1.7881842681339808, Val Acc: 0.3175\n",
      "Epoch: 67, Train Loss: 1.7402268409729005, Train Acc 0.341875, Val Loss: 1.773109061377389, Val Acc: 0.3175\n",
      "Epoch: 68, Train Loss: 1.7371373987197876, Train Acc 0.336875, Val Loss: 1.789210421698434, Val Acc: 0.3175\n",
      "Epoch: 69, Train Loss: 1.7294826459884645, Train Acc 0.339375, Val Loss: 1.7825466224125452, Val Acc: 0.3175\n",
      "Epoch: 70, Train Loss: 1.718043007850647, Train Acc 0.341875, Val Loss: 1.7691406181880407, Val Acc: 0.315\n",
      "Epoch: 71, Train Loss: 1.719767255783081, Train Acc 0.346875, Val Loss: 1.7781390462602888, Val Acc: 0.3175\n",
      "Epoch: 72, Train Loss: 1.7314263153076173, Train Acc 0.349375, Val Loss: 1.7802710022245134, Val Acc: 0.325\n",
      "Epoch: 73, Train Loss: 1.7217397451400758, Train Acc 0.34375, Val Loss: 1.7867238691874914, Val Acc: 0.33\n",
      "Epoch: 74, Train Loss: 1.7495003652572632, Train Acc 0.3375, Val Loss: 1.7769885744367326, Val Acc: 0.3075\n",
      "Epoch: 75, Train Loss: 1.7245464277267457, Train Acc 0.348125, Val Loss: 1.7707714182989938, Val Acc: 0.3075\n",
      "Epoch: 76, Train Loss: 1.7164743757247924, Train Acc 0.35375, Val Loss: 1.7819706031254359, Val Acc: 0.33\n",
      "Epoch: 77, Train Loss: 1.720469355583191, Train Acc 0.3275, Val Loss: 1.7739087513514928, Val Acc: 0.3375\n",
      "Epoch: 78, Train Loss: 1.717359356880188, Train Acc 0.34375, Val Loss: 1.7902025835854667, Val Acc: 0.3325\n",
      "Epoch: 79, Train Loss: 1.737112364768982, Train Acc 0.334375, Val Loss: 1.7734720706939697, Val Acc: 0.3175\n",
      "Epoch: 80, Train Loss: 1.7200302505493164, Train Acc 0.348125, Val Loss: 1.7852445840835571, Val Acc: 0.32\n",
      "Epoch: 81, Train Loss: 1.7191652870178222, Train Acc 0.34125, Val Loss: 1.7860427924564906, Val Acc: 0.3075\n",
      "Epoch: 82, Train Loss: 1.7071520328521728, Train Acc 0.350625, Val Loss: 1.7742949553898402, Val Acc: 0.32\n",
      "Epoch: 83, Train Loss: 1.7206130123138428, Train Acc 0.34625, Val Loss: 1.775083269391741, Val Acc: 0.3225\n",
      "Epoch: 84, Train Loss: 1.7180055427551268, Train Acc 0.35, Val Loss: 1.8099008458001273, Val Acc: 0.3225\n",
      "Epoch: 85, Train Loss: 1.7375700092315673, Train Acc 0.340625, Val Loss: 1.7764779329299927, Val Acc: 0.3225\n",
      "Epoch: 86, Train Loss: 1.717198419570923, Train Acc 0.34625, Val Loss: 1.7691799913133894, Val Acc: 0.3275\n",
      "Epoch: 87, Train Loss: 1.7215931701660157, Train Acc 0.34875, Val Loss: 1.798240338053022, Val Acc: 0.3225\n",
      "Epoch: 88, Train Loss: 1.7111772441864013, Train Acc 0.351875, Val Loss: 1.7843540736607142, Val Acc: 0.3275\n",
      "Epoch: 89, Train Loss: 1.699610300064087, Train Acc 0.36125, Val Loss: 1.7745226791926794, Val Acc: 0.3225\n",
      "Epoch: 90, Train Loss: 1.711296844482422, Train Acc 0.360625, Val Loss: 1.7935688495635986, Val Acc: 0.315\n",
      "Epoch: 91, Train Loss: 1.7181321907043456, Train Acc 0.349375, Val Loss: 1.7883532047271729, Val Acc: 0.325\n",
      "Epoch: 92, Train Loss: 1.7048240184783936, Train Acc 0.338125, Val Loss: 1.7847092662538802, Val Acc: 0.315\n",
      "Epoch: 93, Train Loss: 1.7144950914382935, Train Acc 0.3375, Val Loss: 1.7776694468089513, Val Acc: 0.33\n",
      "Epoch: 94, Train Loss: 1.7139083290100097, Train Acc 0.333125, Val Loss: 1.7782937117985316, Val Acc: 0.32\n",
      "Epoch: 95, Train Loss: 1.712367377281189, Train Acc 0.351875, Val Loss: 1.7786492449896676, Val Acc: 0.3275\n",
      "Epoch: 96, Train Loss: 1.7030528593063354, Train Acc 0.338125, Val Loss: 1.7818739584514074, Val Acc: 0.3375\n",
      "Epoch: 97, Train Loss: 1.7142343521118164, Train Acc 0.34375, Val Loss: 1.7795121329171317, Val Acc: 0.3275\n",
      "Epoch: 98, Train Loss: 1.7022022581100464, Train Acc 0.345625, Val Loss: 1.7753948824746268, Val Acc: 0.3275\n",
      "Epoch: 99, Train Loss: 1.7099700498580932, Train Acc 0.350625, Val Loss: 1.7852845532553536, Val Acc: 0.33\n",
      "Epoch: 100, Train Loss: 1.708857831954956, Train Acc 0.361875, Val Loss: 1.7836687224251884, Val Acc: 0.3125\n",
      "Epoch: 101, Train Loss: 1.6993593311309814, Train Acc 0.35625, Val Loss: 1.772056221961975, Val Acc: 0.335\n",
      "Epoch: 102, Train Loss: 1.6987484979629517, Train Acc 0.355625, Val Loss: 1.7709111486162459, Val Acc: 0.3275\n",
      "Epoch: 103, Train Loss: 1.7116009044647216, Train Acc 0.365, Val Loss: 1.7941233771187919, Val Acc: 0.3225\n",
      "Epoch: 104, Train Loss: 1.713782353401184, Train Acc 0.35625, Val Loss: 1.7811906848634993, Val Acc: 0.32\n",
      "Epoch: 105, Train Loss: 1.6953510999679566, Train Acc 0.351875, Val Loss: 1.772696512086051, Val Acc: 0.3325\n",
      "Epoch: 106, Train Loss: 1.692947916984558, Train Acc 0.356875, Val Loss: 1.7854871920176916, Val Acc: 0.32\n",
      "Epoch: 107, Train Loss: 1.7046029901504516, Train Acc 0.341875, Val Loss: 1.7834053550447737, Val Acc: 0.325\n",
      "Epoch: 108, Train Loss: 1.7223862600326538, Train Acc 0.3575, Val Loss: 1.7848289183207922, Val Acc: 0.335\n",
      "Epoch: 109, Train Loss: 1.7022518825531006, Train Acc 0.344375, Val Loss: 1.7736821004322596, Val Acc: 0.3375\n",
      "Epoch: 110, Train Loss: 1.698148078918457, Train Acc 0.355625, Val Loss: 1.7718465498515539, Val Acc: 0.335\n",
      "Epoch: 111, Train Loss: 1.696035943031311, Train Acc 0.350625, Val Loss: 1.7788632937840052, Val Acc: 0.33\n",
      "Epoch: 112, Train Loss: 1.704077320098877, Train Acc 0.350625, Val Loss: 1.7802304370062692, Val Acc: 0.3175\n",
      "Epoch: 113, Train Loss: 1.697822380065918, Train Acc 0.36, Val Loss: 1.7851677281515939, Val Acc: 0.33\n",
      "Epoch: 114, Train Loss: 1.6823154592514038, Train Acc 0.355, Val Loss: 1.770226308277675, Val Acc: 0.34\n",
      "Epoch: 115, Train Loss: 1.6941492128372193, Train Acc 0.36375, Val Loss: 1.792544194630214, Val Acc: 0.33\n",
      "Epoch: 116, Train Loss: 1.6896638870239258, Train Acc 0.36625, Val Loss: 1.7683854273387365, Val Acc: 0.3325\n",
      "Epoch: 117, Train Loss: 1.702778263092041, Train Acc 0.338125, Val Loss: 1.781035167830331, Val Acc: 0.33\n",
      "Epoch: 118, Train Loss: 1.7029580926895143, Train Acc 0.348125, Val Loss: 1.7722028493881226, Val Acc: 0.3425\n",
      "Epoch: 119, Train Loss: 1.6981350517272948, Train Acc 0.353125, Val Loss: 1.7784384999956404, Val Acc: 0.3375\n",
      "Epoch: 120, Train Loss: 1.694143853187561, Train Acc 0.363125, Val Loss: 1.8118445021765572, Val Acc: 0.3275\n",
      "Epoch: 121, Train Loss: 1.6751780748367309, Train Acc 0.368125, Val Loss: 1.7974748952048165, Val Acc: 0.34\n",
      "Epoch: 122, Train Loss: 1.6998641061782838, Train Acc 0.35125, Val Loss: 1.7744375807898385, Val Acc: 0.325\n",
      "Epoch: 123, Train Loss: 1.6983863306045532, Train Acc 0.35875, Val Loss: 1.7739711659295219, Val Acc: 0.3325\n",
      "Epoch: 124, Train Loss: 1.6858722925186158, Train Acc 0.37125, Val Loss: 1.7862802233014787, Val Acc: 0.3275\n",
      "Epoch: 125, Train Loss: 1.6966378927230834, Train Acc 0.346875, Val Loss: 1.7751871006829398, Val Acc: 0.3275\n",
      "Epoch: 126, Train Loss: 1.6941716861724854, Train Acc 0.350625, Val Loss: 1.790852699960981, Val Acc: 0.335\n",
      "Epoch: 127, Train Loss: 1.688124279975891, Train Acc 0.35875, Val Loss: 1.7743898800441198, Val Acc: 0.335\n",
      "Epoch: 128, Train Loss: 1.6961776733398437, Train Acc 0.35625, Val Loss: 1.7947311401367188, Val Acc: 0.305\n",
      "Epoch: 129, Train Loss: 1.6817157220840455, Train Acc 0.3525, Val Loss: 1.775599752153669, Val Acc: 0.3375\n",
      "Epoch: 130, Train Loss: 1.681531596183777, Train Acc 0.36375, Val Loss: 1.7731027262551444, Val Acc: 0.34\n",
      "Epoch: 131, Train Loss: 1.6869108915328979, Train Acc 0.35625, Val Loss: 1.775897400719779, Val Acc: 0.3425\n",
      "Epoch: 132, Train Loss: 1.6916159200668335, Train Acc 0.341875, Val Loss: 1.7763984543936593, Val Acc: 0.3275\n",
      "Epoch: 133, Train Loss: 1.6836130237579345, Train Acc 0.355, Val Loss: 1.766573769705636, Val Acc: 0.35\n",
      "Epoch: 134, Train Loss: 1.6749604082107543, Train Acc 0.35375, Val Loss: 1.7758047580718994, Val Acc: 0.3325\n",
      "Epoch: 135, Train Loss: 1.6901776266098023, Train Acc 0.36, Val Loss: 1.7764870268957955, Val Acc: 0.3475\n",
      "Epoch: 136, Train Loss: 1.6837868213653564, Train Acc 0.3575, Val Loss: 1.7959812709263392, Val Acc: 0.33\n",
      "Epoch: 137, Train Loss: 1.6729443979263305, Train Acc 0.3625, Val Loss: 1.775993721825736, Val Acc: 0.32\n",
      "Epoch: 138, Train Loss: 1.6790682983398437, Train Acc 0.36, Val Loss: 1.7777823039463587, Val Acc: 0.335\n",
      "Epoch: 139, Train Loss: 1.6808434104919434, Train Acc 0.3725, Val Loss: 1.784071377345494, Val Acc: 0.325\n",
      "Epoch: 140, Train Loss: 1.6712879180908202, Train Acc 0.366875, Val Loss: 1.7755574328558785, Val Acc: 0.3425\n",
      "Epoch: 141, Train Loss: 1.6698567914962767, Train Acc 0.361875, Val Loss: 1.7708091395241874, Val Acc: 0.345\n",
      "Epoch: 142, Train Loss: 1.6695506286621093, Train Acc 0.368125, Val Loss: 1.7774933235985892, Val Acc: 0.34\n",
      "Epoch: 143, Train Loss: 1.6727938222885133, Train Acc 0.361875, Val Loss: 1.7842454739979334, Val Acc: 0.3075\n",
      "Epoch: 144, Train Loss: 1.670274748802185, Train Acc 0.3525, Val Loss: 1.7918343544006348, Val Acc: 0.3475\n",
      "Epoch: 145, Train Loss: 1.6861704635620116, Train Acc 0.366875, Val Loss: 1.7814949580601283, Val Acc: 0.3375\n",
      "Epoch: 146, Train Loss: 1.6569868755340575, Train Acc 0.369375, Val Loss: 1.7861748422895158, Val Acc: 0.34\n",
      "Epoch: 147, Train Loss: 1.683514952659607, Train Acc 0.376875, Val Loss: 1.7781843968800135, Val Acc: 0.35\n",
      "Epoch: 148, Train Loss: 1.6840918970108032, Train Acc 0.35375, Val Loss: 1.7685422386441911, Val Acc: 0.34\n",
      "Epoch: 149, Train Loss: 1.6758545684814452, Train Acc 0.35875, Val Loss: 1.7861581018992834, Val Acc: 0.335\n",
      "Epoch: 150, Train Loss: 1.6749508237838746, Train Acc 0.356875, Val Loss: 1.7810284069606237, Val Acc: 0.3475\n",
      "Epoch: 151, Train Loss: 1.6692829751968383, Train Acc 0.364375, Val Loss: 1.792388183729989, Val Acc: 0.32\n",
      "Epoch: 152, Train Loss: 1.6973233222961426, Train Acc 0.35, Val Loss: 1.7751044545854842, Val Acc: 0.3375\n",
      "Epoch: 153, Train Loss: 1.6646086740493775, Train Acc 0.361875, Val Loss: 1.7934638602393014, Val Acc: 0.3325\n",
      "Epoch: 154, Train Loss: 1.6789349365234374, Train Acc 0.383125, Val Loss: 1.776517493384225, Val Acc: 0.335\n",
      "Epoch: 155, Train Loss: 1.6501362657546996, Train Acc 0.365, Val Loss: 1.7832070929663522, Val Acc: 0.34\n",
      "Epoch: 156, Train Loss: 1.6833244037628174, Train Acc 0.3625, Val Loss: 1.7948457172938757, Val Acc: 0.33\n",
      "Epoch: 157, Train Loss: 1.6691260814666748, Train Acc 0.36875, Val Loss: 1.784096326146807, Val Acc: 0.3375\n",
      "Epoch: 158, Train Loss: 1.666549129486084, Train Acc 0.360625, Val Loss: 1.78690505027771, Val Acc: 0.34\n",
      "Epoch: 159, Train Loss: 1.6752040910720825, Train Acc 0.360625, Val Loss: 1.7720623186656408, Val Acc: 0.3525\n",
      "Epoch: 160, Train Loss: 1.6736864709854127, Train Acc 0.370625, Val Loss: 1.8282652241843087, Val Acc: 0.34\n",
      "Epoch: 161, Train Loss: 1.670737361907959, Train Acc 0.36, Val Loss: 1.779328737940107, Val Acc: 0.3275\n",
      "Epoch: 162, Train Loss: 1.6651312875747681, Train Acc 0.368125, Val Loss: 1.7835334709712438, Val Acc: 0.3275\n",
      "Epoch: 163, Train Loss: 1.6485204219818115, Train Acc 0.375625, Val Loss: 1.7958533763885498, Val Acc: 0.3325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m eval_loss, eval_acc\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 57\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(conv_model, train_loader)\n\u001b[1;32m     58\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m evaluate(conv_model, val_loader)\n\u001b[1;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     23\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device), batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     27\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mCompareModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     31\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_avgpool(x)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "geom_vector_len = 5  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "num_classes = 9  # Number of output classes\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.001)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 300\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=-1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=-1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(conv_model, train_loader)\n",
    "    val_loss, val_acc = evaluate(conv_model, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(pot.state_dict(), 'pot_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it as feature extractor (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "pot.load_state_dict(torch.load(\"pot_model.pth\"))\n",
    "pot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_hidden = pot(train_tokens, train_mask, pre_train=True).view(train_tokens.size(0), -1)\n",
    "    val_hidden = pot(val_tokens, val_mask, pre_train=True).view(val_tokens.size(0), -1)\n",
    "    test_hidden = pot(test_tokens, test_mask, pre_train=True).view(test_tokens.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_hidden, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_hidden, val_labels))\n",
    "test_loader = DataLoader(TensorDataset(test_hidden, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.9963074207305909, Train Acc 0.50625, Val Loss: 1.30093060284853, Val Acc: 0.62\n",
      "Epoch: 2, Train Loss: 1.2693264412879943, Train Acc 0.5925, Val Loss: 1.0721332759410143, Val Acc: 0.68\n",
      "Epoch: 3, Train Loss: 1.166937198638916, Train Acc 0.605, Val Loss: 1.0162230451405048, Val Acc: 0.7\n",
      "Epoch: 4, Train Loss: 1.1719644212722777, Train Acc 0.59125, Val Loss: 1.1128932654857635, Val Acc: 0.71\n",
      "Epoch: 5, Train Loss: 1.166082215309143, Train Acc 0.6225, Val Loss: 1.0086301210522652, Val Acc: 0.72\n",
      "Epoch: 6, Train Loss: 1.1365370631217957, Train Acc 0.6125, Val Loss: 1.1069583275914192, Val Acc: 0.66\n",
      "Epoch: 7, Train Loss: 1.178542513847351, Train Acc 0.60125, Val Loss: 1.2399723929166795, Val Acc: 0.59\n",
      "Epoch: 8, Train Loss: 1.1612643957138062, Train Acc 0.59125, Val Loss: 1.0868311071395873, Val Acc: 0.69\n",
      "Epoch: 9, Train Loss: 1.1284274244308472, Train Acc 0.60875, Val Loss: 1.0686833444237709, Val Acc: 0.66\n",
      "Epoch: 10, Train Loss: 1.1235071158409118, Train Acc 0.61125, Val Loss: 1.0323871737718582, Val Acc: 0.7\n",
      "Epoch: 11, Train Loss: 1.1324586868286133, Train Acc 0.61625, Val Loss: 1.0419702281057834, Val Acc: 0.69\n",
      "Epoch: 12, Train Loss: 1.1420544934272767, Train Acc 0.60625, Val Loss: 1.0410168534517288, Val Acc: 0.72\n",
      "Epoch: 13, Train Loss: 1.1209018397331239, Train Acc 0.63, Val Loss: 1.0741409802436828, Val Acc: 0.7\n",
      "Epoch: 14, Train Loss: 1.112939703464508, Train Acc 0.61375, Val Loss: 1.0233982527256011, Val Acc: 0.71\n",
      "Epoch: 15, Train Loss: 1.1448683261871337, Train Acc 0.60625, Val Loss: 1.0169035314023496, Val Acc: 0.71\n",
      "Epoch: 16, Train Loss: 1.1269723868370056, Train Acc 0.61375, Val Loss: 1.0167317280173302, Val Acc: 0.69\n",
      "Epoch: 17, Train Loss: 1.1318548393249512, Train Acc 0.60625, Val Loss: 1.1504305997490882, Val Acc: 0.66\n",
      "Epoch: 18, Train Loss: 1.1318707847595215, Train Acc 0.60375, Val Loss: 1.0020979772508145, Val Acc: 0.71\n",
      "Epoch: 19, Train Loss: 1.111134376525879, Train Acc 0.62125, Val Loss: 1.1472789430618286, Val Acc: 0.62\n",
      "Epoch: 20, Train Loss: 1.1136483645439148, Train Acc 0.61, Val Loss: 1.040769821703434, Val Acc: 0.72\n",
      "Test Loss: 0.9882621133327484, Test Acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(train_hidden.size(1), 128, 10, 0.5)\n",
    "\n",
    "if USE_GPU:\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, loader):\n",
    "    model.eval()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(classifier, train_loader)\n",
    "    val_loss, val_acc = evaluate(classifier, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(classifier, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
