{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer building from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Same MLP applied to all token(position) representations\"\"\"\n",
    "    def __init__(self, emb_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0, \"Embedding dimension must be divided by number of heads\"\n",
    "\n",
    "        # Dimensions initialization\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        # all features are divided into multi head, each head have a part of features\n",
    "        self.head_emb_dim = self.emb_dim // self.num_heads\n",
    "\n",
    "        # Transformation matrixs\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_emb_dim)\n",
    "\n",
    "        # Mask scores (where positions are 0) with near negative inf\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply sofxmax to attention scores\n",
    "        attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Get the final output\n",
    "        output = torch.matmul(attn_scores, V)\n",
    "        return output\n",
    "    \n",
    "    def split(self, x):\n",
    "        # Reshape the input emb_dim (to multi-head, each head owns a part of input features) for multi-head attention\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        # transpose to fix batch_size and num_heads, let seq_len, head_emb_dim participate in matrix multiplication\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_emb_dim).transpose(1, 2)\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size, num_heads, seq_len, head_emb_dim = x.size()\n",
    "        # contiguous() ensures the memory layout of the tensor is contiguous\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.emb_dim)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Split input to multi heads\n",
    "        Q = self.split(self.W_q(Q))\n",
    "        K = self.split(self.W_k(K))\n",
    "        V = self.split(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine outputs and apply transformation\n",
    "        output = self.W_o(self.combine(attn_output))\n",
    "        return output\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_atten = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ffn = PositionWiseFFN(emb_dim, ffn_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_atten(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolygonEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads,\n",
    "                num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(emb_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, emb_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # token_mask = (tokens != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.ones((batch_size, 1, 1, 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=3)\n",
    "        \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            x = enc_layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PolygonTransformer(nn.Module):\n",
    "    def __init__(self, num_types, emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = PolygonEncoder(emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(emb_dim, ffn_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(ffn_dim, num_types))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.encoder(x, mask)\n",
    "        x = x[:, 0, :] # grab the class embedding\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid wkt string, skip it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "def count_points(wkt):\n",
    "    try:\n",
    "        num_points = gv.num_points_from_wkt(wkt)\n",
    "        # gv.vectorize_wkt(wkt)\n",
    "        return num_points\n",
    "    except:\n",
    "        print(\"Invalid wkt string, skip it\")\n",
    "        return np.inf\n",
    "\n",
    "filtered_df = df[df['WKT'].apply(lambda x: count_points(x) <= max_seq_len)]\n",
    "df = filtered_df\n",
    "\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(df, val_split_ratio, test_split_ratio):\n",
    "\n",
    "    data, labels = np.array(df['WKT'].tolist()), np.array(df['type'].tolist())\n",
    "\n",
    "    num_val = int(val_split_ratio * len(df))\n",
    "    num_test = int(test_split_ratio * len(df))\n",
    "\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = indices[num_val+num_test:], indices[:num_val], indices[num_val:num_val+num_test]\n",
    "\n",
    "    train_data, train_labels = data[train_indices], labels[train_indices]\n",
    "    val_data, val_labels = data[val_indices], labels[val_indices]\n",
    "    test_data, test_labels = data[test_indices], labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "ori_train_data, ori_train_labels, ori_val_data, ori_val_labels, ori_test_data, ori_test_labels = dataset_split(df, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(wkts, types, max_seq_len): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices < start_points\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_labels, train_mask = prepare_polygon_dataset(ori_train_data, ori_train_labels, max_seq_len)\n",
    "val_tokens, val_labels, val_mask = prepare_polygon_dataset(ori_val_data, ori_val_labels, max_seq_len)\n",
    "test_tokens, test_labels, test_mask = prepare_polygon_dataset(ori_test_data, ori_test_labels, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_split_ratio, test_split_ratio = 0.1, 0.2\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.1, 0.2])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels, train_mask), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens, val_labels, val_mask), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(test_tokens, test_labels, test_mask), batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.591607551574707, Train Acc 0.48125, Val Loss: 1.2651425749063492, Val Acc: 0.54\n",
      "Epoch: 2, Train Loss: 1.3729569435119628, Train Acc 0.53, Val Loss: 1.2503191232681274, Val Acc: 0.54\n",
      "Epoch: 3, Train Loss: 1.3107720279693604, Train Acc 0.53, Val Loss: 1.1492047309875488, Val Acc: 0.54\n",
      "Epoch: 4, Train Loss: 1.1920768117904663, Train Acc 0.58125, Val Loss: 1.1558820307254791, Val Acc: 0.56\n",
      "Epoch: 5, Train Loss: 1.1631019258499145, Train Acc 0.57, Val Loss: 1.0916729718446732, Val Acc: 0.56\n",
      "Epoch: 6, Train Loss: 1.145093524456024, Train Acc 0.58875, Val Loss: 1.0704541057348251, Val Acc: 0.6\n",
      "Epoch: 7, Train Loss: 1.1464222240447999, Train Acc 0.6, Val Loss: 1.0817415863275528, Val Acc: 0.59\n",
      "Epoch: 8, Train Loss: 1.129299645423889, Train Acc 0.59625, Val Loss: 1.0574359893798828, Val Acc: 0.57\n",
      "Epoch: 9, Train Loss: 1.1074336910247802, Train Acc 0.605, Val Loss: 1.0777126401662827, Val Acc: 0.61\n",
      "Epoch: 10, Train Loss: 1.1001583528518677, Train Acc 0.6, Val Loss: 1.0280639827251434, Val Acc: 0.61\n",
      "Epoch: 11, Train Loss: 1.0827924013137817, Train Acc 0.60625, Val Loss: 0.9966135919094086, Val Acc: 0.63\n",
      "Epoch: 12, Train Loss: 1.0710226130485534, Train Acc 0.62, Val Loss: 1.0124687850475311, Val Acc: 0.62\n",
      "Epoch: 13, Train Loss: 1.0688176274299621, Train Acc 0.61625, Val Loss: 1.0124618411064148, Val Acc: 0.64\n",
      "Epoch: 14, Train Loss: 1.071626317501068, Train Acc 0.615, Val Loss: 1.0269559621810913, Val Acc: 0.6\n",
      "Epoch: 15, Train Loss: 1.0705906796455382, Train Acc 0.6225, Val Loss: 1.0097585171461105, Val Acc: 0.61\n",
      "Epoch: 16, Train Loss: 1.0630259251594543, Train Acc 0.62625, Val Loss: 1.0276750773191452, Val Acc: 0.63\n",
      "Epoch: 17, Train Loss: 1.0493853688240051, Train Acc 0.63, Val Loss: 1.0824950635433197, Val Acc: 0.61\n",
      "Epoch: 18, Train Loss: 1.057656271457672, Train Acc 0.62375, Val Loss: 0.9943891167640686, Val Acc: 0.61\n",
      "Epoch: 19, Train Loss: 1.0382200384140015, Train Acc 0.63375, Val Loss: 1.0122914761304855, Val Acc: 0.63\n",
      "Epoch: 20, Train Loss: 1.040091440677643, Train Acc 0.62875, Val Loss: 0.9874487072229385, Val Acc: 0.63\n",
      "Epoch: 21, Train Loss: 1.0374178743362428, Train Acc 0.62875, Val Loss: 0.9961567372083664, Val Acc: 0.65\n",
      "Epoch: 22, Train Loss: 1.0293189358711243, Train Acc 0.63375, Val Loss: 1.0027737319469452, Val Acc: 0.63\n",
      "Epoch: 23, Train Loss: 1.0277523732185363, Train Acc 0.645, Val Loss: 0.9870263338088989, Val Acc: 0.65\n",
      "Epoch: 24, Train Loss: 1.0340946340560913, Train Acc 0.63375, Val Loss: 0.9763946533203125, Val Acc: 0.64\n",
      "Epoch: 25, Train Loss: 1.0272097563743592, Train Acc 0.645, Val Loss: 0.9926439225673676, Val Acc: 0.63\n",
      "Epoch: 26, Train Loss: 1.0182769584655762, Train Acc 0.64375, Val Loss: 1.0848070532083511, Val Acc: 0.58\n",
      "Epoch: 27, Train Loss: 1.0234113669395446, Train Acc 0.64375, Val Loss: 1.0134021490812302, Val Acc: 0.62\n",
      "Epoch: 28, Train Loss: 1.015424153804779, Train Acc 0.64, Val Loss: 0.986802414059639, Val Acc: 0.65\n",
      "Epoch: 29, Train Loss: 1.024402904510498, Train Acc 0.63125, Val Loss: 0.9824708253145218, Val Acc: 0.64\n",
      "Epoch: 30, Train Loss: 1.0106388974189757, Train Acc 0.64625, Val Loss: 1.0513833314180374, Val Acc: 0.64\n",
      "Epoch: 31, Train Loss: 1.0111662721633912, Train Acc 0.64875, Val Loss: 1.0199428796768188, Val Acc: 0.65\n",
      "Epoch: 32, Train Loss: 1.0070394158363343, Train Acc 0.6575, Val Loss: 1.0346052646636963, Val Acc: 0.62\n",
      "Epoch: 33, Train Loss: 1.0154332089424134, Train Acc 0.64625, Val Loss: 1.0209785103797913, Val Acc: 0.63\n",
      "Epoch: 34, Train Loss: 1.0081975626945496, Train Acc 0.65, Val Loss: 1.0012037456035614, Val Acc: 0.65\n",
      "Epoch: 35, Train Loss: 1.0026198792457581, Train Acc 0.64125, Val Loss: 0.9972403049468994, Val Acc: 0.65\n",
      "Epoch: 36, Train Loss: 1.0141914367675782, Train Acc 0.63125, Val Loss: 0.9921434968709946, Val Acc: 0.64\n",
      "Epoch: 37, Train Loss: 1.0140997219085692, Train Acc 0.65375, Val Loss: 0.997676357626915, Val Acc: 0.65\n",
      "Epoch: 38, Train Loss: 1.0186217284202577, Train Acc 0.63625, Val Loss: 1.0215691328048706, Val Acc: 0.61\n",
      "Epoch: 39, Train Loss: 0.9970107555389405, Train Acc 0.65625, Val Loss: 1.0300774425268173, Val Acc: 0.63\n",
      "Epoch: 40, Train Loss: 1.0016853785514832, Train Acc 0.64875, Val Loss: 1.0304452329874039, Val Acc: 0.62\n",
      "Epoch: 41, Train Loss: 0.9959747314453125, Train Acc 0.65, Val Loss: 1.084193542599678, Val Acc: 0.63\n",
      "Epoch: 42, Train Loss: 0.9903687715530396, Train Acc 0.6575, Val Loss: 1.0614320188760757, Val Acc: 0.63\n",
      "Epoch: 43, Train Loss: 0.9907874083518982, Train Acc 0.64625, Val Loss: 1.0528669208288193, Val Acc: 0.62\n",
      "Epoch: 44, Train Loss: 0.9825370597839356, Train Acc 0.65875, Val Loss: 1.0652941763401031, Val Acc: 0.62\n",
      "Epoch: 45, Train Loss: 0.9956056714057923, Train Acc 0.66, Val Loss: 1.036482810974121, Val Acc: 0.67\n",
      "Epoch: 46, Train Loss: 0.9913445496559143, Train Acc 0.655, Val Loss: 1.0057516396045685, Val Acc: 0.68\n",
      "Epoch: 47, Train Loss: 0.9990490579605102, Train Acc 0.6475, Val Loss: 1.0580961853265762, Val Acc: 0.66\n",
      "Epoch: 48, Train Loss: 0.983617103099823, Train Acc 0.6525, Val Loss: 1.0466543138027191, Val Acc: 0.64\n",
      "Epoch: 49, Train Loss: 0.9756866002082825, Train Acc 0.66625, Val Loss: 1.0551193058490753, Val Acc: 0.65\n",
      "Epoch: 50, Train Loss: 0.9723373556137085, Train Acc 0.6625, Val Loss: 1.0470933467149734, Val Acc: 0.66\n",
      "Epoch: 51, Train Loss: 0.9701216864585877, Train Acc 0.66, Val Loss: 1.0243405997753143, Val Acc: 0.62\n",
      "Epoch: 52, Train Loss: 0.9813901424407959, Train Acc 0.65125, Val Loss: 1.0780902653932571, Val Acc: 0.62\n",
      "Epoch: 53, Train Loss: 0.9660846900939941, Train Acc 0.6625, Val Loss: 1.046322450041771, Val Acc: 0.67\n",
      "Epoch: 54, Train Loss: 0.9655221557617187, Train Acc 0.6525, Val Loss: 1.0757809579372406, Val Acc: 0.63\n",
      "Epoch: 55, Train Loss: 0.9558884525299072, Train Acc 0.66375, Val Loss: 1.0529443472623825, Val Acc: 0.63\n",
      "Epoch: 56, Train Loss: 0.9663898658752441, Train Acc 0.66, Val Loss: 1.0469986945390701, Val Acc: 0.62\n",
      "Epoch: 57, Train Loss: 0.9658334684371949, Train Acc 0.65875, Val Loss: 1.091961309313774, Val Acc: 0.64\n",
      "Epoch: 58, Train Loss: 0.9488397574424744, Train Acc 0.67125, Val Loss: 1.0791787505149841, Val Acc: 0.64\n",
      "Epoch: 59, Train Loss: 0.9599464368820191, Train Acc 0.67375, Val Loss: 1.0511790364980698, Val Acc: 0.64\n",
      "Epoch: 60, Train Loss: 0.9518772315979004, Train Acc 0.65875, Val Loss: 1.117739275097847, Val Acc: 0.63\n",
      "Epoch: 61, Train Loss: 0.9543604993820191, Train Acc 0.6675, Val Loss: 1.0757697820663452, Val Acc: 0.66\n",
      "Epoch: 62, Train Loss: 0.9604991126060486, Train Acc 0.655, Val Loss: 1.0968472510576248, Val Acc: 0.63\n",
      "Epoch: 63, Train Loss: 0.9440722370147705, Train Acc 0.66625, Val Loss: 1.0656494945287704, Val Acc: 0.65\n",
      "Epoch: 64, Train Loss: 0.9507207870483398, Train Acc 0.66875, Val Loss: 1.1191112399101257, Val Acc: 0.63\n",
      "Epoch: 65, Train Loss: 0.9516575193405151, Train Acc 0.6725, Val Loss: 1.0703815668821335, Val Acc: 0.66\n",
      "Epoch: 66, Train Loss: 0.9487909317016602, Train Acc 0.6725, Val Loss: 1.1205839216709137, Val Acc: 0.64\n",
      "Epoch: 67, Train Loss: 0.94026620388031, Train Acc 0.66875, Val Loss: 1.088695615530014, Val Acc: 0.66\n",
      "Epoch: 68, Train Loss: 0.9350858688354492, Train Acc 0.66875, Val Loss: 1.0970927625894547, Val Acc: 0.64\n",
      "Epoch: 69, Train Loss: 0.9412618231773376, Train Acc 0.6775, Val Loss: 1.160717487335205, Val Acc: 0.59\n",
      "Epoch: 70, Train Loss: 0.9484878325462341, Train Acc 0.675, Val Loss: 1.0834139287471771, Val Acc: 0.68\n",
      "Epoch: 71, Train Loss: 0.9663291168212891, Train Acc 0.67375, Val Loss: 1.0436501950025558, Val Acc: 0.65\n",
      "Epoch: 72, Train Loss: 0.9402331328392028, Train Acc 0.67375, Val Loss: 1.1207917928695679, Val Acc: 0.65\n",
      "Epoch: 73, Train Loss: 0.9422127985954285, Train Acc 0.6675, Val Loss: 1.085323989391327, Val Acc: 0.67\n",
      "Epoch: 74, Train Loss: 0.925154287815094, Train Acc 0.6775, Val Loss: 1.1023523062467575, Val Acc: 0.64\n",
      "Epoch: 75, Train Loss: 0.9280497622489929, Train Acc 0.675, Val Loss: 1.1037477552890778, Val Acc: 0.66\n",
      "Epoch: 76, Train Loss: 0.9350805187225342, Train Acc 0.67625, Val Loss: 1.0694343894720078, Val Acc: 0.66\n",
      "Epoch: 77, Train Loss: 0.9339766359329223, Train Acc 0.67375, Val Loss: 1.1112312525510788, Val Acc: 0.66\n",
      "Epoch: 78, Train Loss: 0.9359961581230164, Train Acc 0.6725, Val Loss: 1.130456879734993, Val Acc: 0.68\n",
      "Epoch: 79, Train Loss: 0.9464903831481933, Train Acc 0.67, Val Loss: 1.1383262127637863, Val Acc: 0.63\n",
      "Epoch: 80, Train Loss: 0.9329868960380554, Train Acc 0.66625, Val Loss: 1.1031746119260788, Val Acc: 0.63\n",
      "Epoch: 81, Train Loss: 0.9273173403739929, Train Acc 0.66875, Val Loss: 1.1342845410108566, Val Acc: 0.67\n",
      "Epoch: 82, Train Loss: 0.9218477582931519, Train Acc 0.685, Val Loss: 1.1110276281833649, Val Acc: 0.67\n",
      "Epoch: 83, Train Loss: 0.9254383969306946, Train Acc 0.675, Val Loss: 1.1231949180364609, Val Acc: 0.66\n",
      "Epoch: 84, Train Loss: 0.9243722295761109, Train Acc 0.68125, Val Loss: 1.144974246621132, Val Acc: 0.64\n",
      "Epoch: 85, Train Loss: 0.9280933499336242, Train Acc 0.66875, Val Loss: 1.1034084260463715, Val Acc: 0.65\n",
      "Epoch: 86, Train Loss: 0.9385043048858642, Train Acc 0.68, Val Loss: 1.0571909695863724, Val Acc: 0.68\n",
      "Epoch: 87, Train Loss: 0.9218252754211426, Train Acc 0.67375, Val Loss: 1.0820130556821823, Val Acc: 0.67\n",
      "Epoch: 88, Train Loss: 0.9262538266181946, Train Acc 0.68125, Val Loss: 1.1029796600341797, Val Acc: 0.64\n",
      "Epoch: 89, Train Loss: 0.9210998487472534, Train Acc 0.68, Val Loss: 1.096989020705223, Val Acc: 0.67\n",
      "Epoch: 90, Train Loss: 0.9264924573898315, Train Acc 0.67875, Val Loss: 1.1377372294664383, Val Acc: 0.63\n",
      "Epoch: 91, Train Loss: 0.9236571526527405, Train Acc 0.67625, Val Loss: 1.0973841100931168, Val Acc: 0.66\n",
      "Epoch: 92, Train Loss: 0.9223038697242737, Train Acc 0.675, Val Loss: 1.1198783367872238, Val Acc: 0.66\n",
      "Epoch: 93, Train Loss: 0.9246772098541259, Train Acc 0.68, Val Loss: 1.1039453893899918, Val Acc: 0.63\n",
      "Epoch: 94, Train Loss: 0.911519284248352, Train Acc 0.68625, Val Loss: 1.1219165176153183, Val Acc: 0.67\n",
      "Epoch: 95, Train Loss: 0.9074074864387512, Train Acc 0.685, Val Loss: 1.1597533971071243, Val Acc: 0.62\n",
      "Epoch: 96, Train Loss: 0.9159148216247559, Train Acc 0.675, Val Loss: 1.0846144706010818, Val Acc: 0.67\n",
      "Epoch: 97, Train Loss: 0.9204080080986023, Train Acc 0.68125, Val Loss: 1.0730369687080383, Val Acc: 0.66\n",
      "Epoch: 98, Train Loss: 0.9253436887264251, Train Acc 0.68, Val Loss: 1.1403777599334717, Val Acc: 0.66\n",
      "Epoch: 99, Train Loss: 0.9141425776481629, Train Acc 0.68375, Val Loss: 1.0935430377721786, Val Acc: 0.66\n",
      "Epoch: 100, Train Loss: 0.9067480111122131, Train Acc 0.685, Val Loss: 1.1255285739898682, Val Acc: 0.66\n",
      "Epoch: 101, Train Loss: 0.9097454571723937, Train Acc 0.67875, Val Loss: 1.1452516615390778, Val Acc: 0.66\n",
      "Epoch: 102, Train Loss: 0.9254889607429504, Train Acc 0.675, Val Loss: 1.114227831363678, Val Acc: 0.62\n",
      "Epoch: 103, Train Loss: 0.9323570799827575, Train Acc 0.67125, Val Loss: 1.096923902630806, Val Acc: 0.66\n",
      "Epoch: 104, Train Loss: 0.927489013671875, Train Acc 0.67375, Val Loss: 1.1032663136720657, Val Acc: 0.67\n",
      "Epoch: 105, Train Loss: 0.9124805641174316, Train Acc 0.675, Val Loss: 1.0971209108829498, Val Acc: 0.64\n",
      "Epoch: 106, Train Loss: 0.9025732755661011, Train Acc 0.685, Val Loss: 1.110841616988182, Val Acc: 0.67\n",
      "Epoch: 107, Train Loss: 0.8992485499382019, Train Acc 0.685, Val Loss: 1.1015753895044327, Val Acc: 0.66\n",
      "Epoch: 108, Train Loss: 0.9131795120239258, Train Acc 0.68375, Val Loss: 1.0735645145177841, Val Acc: 0.67\n",
      "Epoch: 109, Train Loss: 0.908436450958252, Train Acc 0.68625, Val Loss: 1.1074797362089157, Val Acc: 0.66\n",
      "Epoch: 110, Train Loss: 0.9096065163612366, Train Acc 0.68625, Val Loss: 1.1757533252239227, Val Acc: 0.64\n",
      "Epoch: 111, Train Loss: 0.9229400038719178, Train Acc 0.67375, Val Loss: 1.1081865727901459, Val Acc: 0.65\n",
      "Epoch: 112, Train Loss: 0.9089280235767364, Train Acc 0.68375, Val Loss: 1.1034547090530396, Val Acc: 0.66\n",
      "Epoch: 113, Train Loss: 0.900214467048645, Train Acc 0.67875, Val Loss: 1.1137769669294357, Val Acc: 0.67\n",
      "Epoch: 114, Train Loss: 0.8964004802703858, Train Acc 0.6825, Val Loss: 1.10965096950531, Val Acc: 0.64\n",
      "Epoch: 115, Train Loss: 0.9072817611694336, Train Acc 0.6725, Val Loss: 1.1407893896102905, Val Acc: 0.66\n",
      "Epoch: 116, Train Loss: 0.8963812327384949, Train Acc 0.68875, Val Loss: 1.1215483099222183, Val Acc: 0.66\n",
      "Epoch: 117, Train Loss: 0.8940100836753845, Train Acc 0.68375, Val Loss: 1.0967201441526413, Val Acc: 0.68\n",
      "Epoch: 118, Train Loss: 0.89937175989151, Train Acc 0.68875, Val Loss: 1.1334800720214844, Val Acc: 0.66\n",
      "Epoch: 119, Train Loss: 0.9089503908157348, Train Acc 0.6775, Val Loss: 1.1513846069574356, Val Acc: 0.64\n",
      "Epoch: 120, Train Loss: 0.9192928433418274, Train Acc 0.6725, Val Loss: 1.0998072028160095, Val Acc: 0.65\n",
      "Epoch: 121, Train Loss: 0.9224147653579712, Train Acc 0.6825, Val Loss: 1.1446053832769394, Val Acc: 0.65\n",
      "Epoch: 122, Train Loss: 0.9052338171005249, Train Acc 0.68625, Val Loss: 1.1466688960790634, Val Acc: 0.68\n",
      "Epoch: 123, Train Loss: 0.8926766538619995, Train Acc 0.6825, Val Loss: 1.095668986439705, Val Acc: 0.66\n",
      "Epoch: 124, Train Loss: 0.8952350926399231, Train Acc 0.6875, Val Loss: 1.1262233555316925, Val Acc: 0.63\n",
      "Epoch: 125, Train Loss: 0.9024709248542786, Train Acc 0.6875, Val Loss: 1.1013108044862747, Val Acc: 0.67\n",
      "Epoch: 126, Train Loss: 0.8975685143470764, Train Acc 0.68625, Val Loss: 1.1118896305561066, Val Acc: 0.68\n",
      "Epoch: 127, Train Loss: 0.8967533040046692, Train Acc 0.685, Val Loss: 1.1276308447122574, Val Acc: 0.66\n",
      "Epoch: 128, Train Loss: 0.8981343984603882, Train Acc 0.685, Val Loss: 1.1304228454828262, Val Acc: 0.67\n",
      "Epoch: 129, Train Loss: 0.9003040385246277, Train Acc 0.67375, Val Loss: 1.0857432037591934, Val Acc: 0.65\n",
      "Epoch: 130, Train Loss: 0.9226459884643554, Train Acc 0.675, Val Loss: 1.1122316122055054, Val Acc: 0.67\n",
      "Epoch: 131, Train Loss: 0.9033266639709473, Train Acc 0.67875, Val Loss: 1.1352452784776688, Val Acc: 0.67\n",
      "Epoch: 132, Train Loss: 0.8897005724906921, Train Acc 0.68625, Val Loss: 1.1198490858078003, Val Acc: 0.66\n",
      "Epoch: 133, Train Loss: 0.8949249720573426, Train Acc 0.69, Val Loss: 1.1501203328371048, Val Acc: 0.66\n",
      "Epoch: 134, Train Loss: 0.8873956263065338, Train Acc 0.68875, Val Loss: 1.144038513302803, Val Acc: 0.66\n",
      "Epoch: 135, Train Loss: 0.9025660109519958, Train Acc 0.685, Val Loss: 1.1329347640275955, Val Acc: 0.66\n",
      "Epoch: 136, Train Loss: 0.8981331706047058, Train Acc 0.685, Val Loss: 1.1382744014263153, Val Acc: 0.67\n",
      "Epoch: 137, Train Loss: 0.8950893092155456, Train Acc 0.69, Val Loss: 1.1735678762197495, Val Acc: 0.64\n",
      "Epoch: 138, Train Loss: 0.8999221038818359, Train Acc 0.67625, Val Loss: 1.1054560393095016, Val Acc: 0.66\n",
      "Epoch: 139, Train Loss: 0.9040899014472962, Train Acc 0.6775, Val Loss: 1.0922306329011917, Val Acc: 0.66\n",
      "Epoch: 140, Train Loss: 0.9052302205562591, Train Acc 0.67875, Val Loss: 1.1441366970539093, Val Acc: 0.66\n",
      "Epoch: 141, Train Loss: 0.8905555391311646, Train Acc 0.69, Val Loss: 1.1125903129577637, Val Acc: 0.65\n",
      "Epoch: 142, Train Loss: 0.8891003251075744, Train Acc 0.68625, Val Loss: 1.1430731266736984, Val Acc: 0.66\n",
      "Epoch: 143, Train Loss: 0.8964389824867248, Train Acc 0.6825, Val Loss: 1.1445547193288803, Val Acc: 0.65\n",
      "Epoch: 144, Train Loss: 0.8930924677848816, Train Acc 0.68125, Val Loss: 1.0947906821966171, Val Acc: 0.67\n",
      "Epoch: 145, Train Loss: 0.9010167121887207, Train Acc 0.6875, Val Loss: 1.1033053994178772, Val Acc: 0.67\n",
      "Epoch: 146, Train Loss: 0.8874643230438233, Train Acc 0.68, Val Loss: 1.1610124856233597, Val Acc: 0.65\n",
      "Epoch: 147, Train Loss: 0.8936515331268311, Train Acc 0.685, Val Loss: 1.151473268866539, Val Acc: 0.66\n",
      "Epoch: 148, Train Loss: 0.8957795190811157, Train Acc 0.68375, Val Loss: 1.1737866699695587, Val Acc: 0.62\n",
      "Epoch: 149, Train Loss: 0.8921030831336975, Train Acc 0.685, Val Loss: 1.132276326417923, Val Acc: 0.64\n",
      "Epoch: 150, Train Loss: 0.8889117813110352, Train Acc 0.69125, Val Loss: 1.146971955895424, Val Acc: 0.65\n",
      "Epoch: 151, Train Loss: 0.898442075252533, Train Acc 0.685, Val Loss: 1.1315613090991974, Val Acc: 0.64\n",
      "Epoch: 152, Train Loss: 0.9047465968132019, Train Acc 0.68625, Val Loss: 1.154469519853592, Val Acc: 0.65\n",
      "Epoch: 153, Train Loss: 0.894629385471344, Train Acc 0.68125, Val Loss: 1.0752501487731934, Val Acc: 0.67\n",
      "Epoch: 154, Train Loss: 0.8994950699806213, Train Acc 0.69125, Val Loss: 1.127267599105835, Val Acc: 0.69\n",
      "Epoch: 155, Train Loss: 0.8893530678749084, Train Acc 0.67875, Val Loss: 1.1286493837833405, Val Acc: 0.65\n",
      "Epoch: 156, Train Loss: 0.8841595458984375, Train Acc 0.6925, Val Loss: 1.1654280871152878, Val Acc: 0.66\n",
      "Epoch: 157, Train Loss: 0.8800708198547363, Train Acc 0.69625, Val Loss: 1.1923793852329254, Val Acc: 0.67\n",
      "Epoch: 158, Train Loss: 0.8986811304092407, Train Acc 0.67875, Val Loss: 1.0786915123462677, Val Acc: 0.63\n",
      "Epoch: 159, Train Loss: 0.8817218852043152, Train Acc 0.69125, Val Loss: 1.131124347448349, Val Acc: 0.65\n",
      "Epoch: 160, Train Loss: 0.8860246467590333, Train Acc 0.69, Val Loss: 1.1562680453062057, Val Acc: 0.67\n",
      "Epoch: 161, Train Loss: 0.880903799533844, Train Acc 0.6875, Val Loss: 1.1043371111154556, Val Acc: 0.66\n",
      "Epoch: 162, Train Loss: 0.9251158666610718, Train Acc 0.67375, Val Loss: 1.1136511415243149, Val Acc: 0.65\n",
      "Epoch: 163, Train Loss: 0.8863299679756165, Train Acc 0.6875, Val Loss: 1.1384538114070892, Val Acc: 0.64\n",
      "Epoch: 164, Train Loss: 0.8867188882827759, Train Acc 0.6925, Val Loss: 1.1588298976421356, Val Acc: 0.65\n",
      "Epoch: 165, Train Loss: 0.8793514895439148, Train Acc 0.69125, Val Loss: 1.1814431548118591, Val Acc: 0.66\n",
      "Epoch: 166, Train Loss: 0.8816030168533325, Train Acc 0.6875, Val Loss: 1.1540803611278534, Val Acc: 0.65\n",
      "Epoch: 167, Train Loss: 0.8740336322784423, Train Acc 0.69, Val Loss: 1.1794173866510391, Val Acc: 0.66\n",
      "Epoch: 168, Train Loss: 0.8729607939720154, Train Acc 0.695, Val Loss: 1.1400629431009293, Val Acc: 0.67\n",
      "Epoch: 169, Train Loss: 0.8768444561958313, Train Acc 0.68875, Val Loss: 1.151938021183014, Val Acc: 0.67\n",
      "Epoch: 170, Train Loss: 0.8714609384536743, Train Acc 0.69375, Val Loss: 1.155200555920601, Val Acc: 0.67\n",
      "Epoch: 171, Train Loss: 0.8876597714424134, Train Acc 0.68625, Val Loss: 1.1668423265218735, Val Acc: 0.66\n",
      "Epoch: 172, Train Loss: 0.9185741496086121, Train Acc 0.6775, Val Loss: 1.1783702373504639, Val Acc: 0.66\n",
      "Epoch: 173, Train Loss: 0.8933449816703797, Train Acc 0.68625, Val Loss: 1.1331334859132767, Val Acc: 0.67\n",
      "Epoch: 174, Train Loss: 0.8856861352920532, Train Acc 0.6825, Val Loss: 1.1347038596868515, Val Acc: 0.67\n",
      "Epoch: 175, Train Loss: 0.8704920649528504, Train Acc 0.695, Val Loss: 1.1259184181690216, Val Acc: 0.66\n",
      "Epoch: 176, Train Loss: 0.8895631909370423, Train Acc 0.6825, Val Loss: 1.1455936282873154, Val Acc: 0.65\n",
      "Epoch: 177, Train Loss: 0.8770425367355347, Train Acc 0.695, Val Loss: 1.1569727063179016, Val Acc: 0.66\n",
      "Epoch: 178, Train Loss: 0.8755656051635742, Train Acc 0.6975, Val Loss: 1.156637191772461, Val Acc: 0.68\n",
      "Epoch: 179, Train Loss: 0.8806062626838684, Train Acc 0.6875, Val Loss: 1.1363257467746735, Val Acc: 0.68\n",
      "Epoch: 180, Train Loss: 0.8723069715499878, Train Acc 0.68875, Val Loss: 1.151425912976265, Val Acc: 0.65\n",
      "Epoch: 181, Train Loss: 0.884292471408844, Train Acc 0.685, Val Loss: 1.1065239757299423, Val Acc: 0.67\n",
      "Epoch: 182, Train Loss: 0.8847607803344727, Train Acc 0.69125, Val Loss: 1.1871200054883957, Val Acc: 0.68\n",
      "Epoch: 183, Train Loss: 0.8726559233665466, Train Acc 0.6925, Val Loss: 1.2140019983053207, Val Acc: 0.64\n",
      "Epoch: 184, Train Loss: 0.8773383510112762, Train Acc 0.6925, Val Loss: 1.182091623544693, Val Acc: 0.67\n",
      "Epoch: 185, Train Loss: 0.8790646314620971, Train Acc 0.6925, Val Loss: 1.1857830286026, Val Acc: 0.66\n",
      "Epoch: 186, Train Loss: 0.888989109992981, Train Acc 0.6925, Val Loss: 1.1448529660701752, Val Acc: 0.67\n",
      "Epoch: 187, Train Loss: 0.8877072620391846, Train Acc 0.6875, Val Loss: 1.209654986858368, Val Acc: 0.65\n",
      "Epoch: 188, Train Loss: 0.8670580434799194, Train Acc 0.69625, Val Loss: 1.1476726233959198, Val Acc: 0.67\n",
      "Epoch: 189, Train Loss: 0.8711583399772644, Train Acc 0.6975, Val Loss: 1.1825380474328995, Val Acc: 0.67\n",
      "Epoch: 190, Train Loss: 0.8673388981819152, Train Acc 0.6975, Val Loss: 1.151038184762001, Val Acc: 0.67\n",
      "Epoch: 191, Train Loss: 0.8679164481163025, Train Acc 0.6975, Val Loss: 1.1874489039182663, Val Acc: 0.66\n",
      "Epoch: 192, Train Loss: 0.869136381149292, Train Acc 0.69, Val Loss: 1.1912668347358704, Val Acc: 0.66\n",
      "Epoch: 193, Train Loss: 0.8761361265182495, Train Acc 0.69375, Val Loss: 1.1357063949108124, Val Acc: 0.65\n",
      "Epoch: 194, Train Loss: 0.8866913986206054, Train Acc 0.69375, Val Loss: 1.1230600774288177, Val Acc: 0.65\n",
      "Epoch: 195, Train Loss: 0.881588591337204, Train Acc 0.68875, Val Loss: 1.1893964558839798, Val Acc: 0.66\n",
      "Epoch: 196, Train Loss: 0.8812373733520508, Train Acc 0.69125, Val Loss: 1.2044845670461655, Val Acc: 0.67\n",
      "Epoch: 197, Train Loss: 0.8758800721168518, Train Acc 0.69375, Val Loss: 1.1595681011676788, Val Acc: 0.67\n",
      "Epoch: 198, Train Loss: 0.8640926694869995, Train Acc 0.7, Val Loss: 1.2111582159996033, Val Acc: 0.66\n",
      "Epoch: 199, Train Loss: 0.8645809364318847, Train Acc 0.695, Val Loss: 1.2004941403865814, Val Acc: 0.67\n",
      "Epoch: 200, Train Loss: 0.8739103937149048, Train Acc 0.695, Val Loss: 1.1991897374391556, Val Acc: 0.66\n",
      "Test Loss: 1.7275504171848297, Test Acc: 0.68\n"
     ]
    }
   ],
   "source": [
    "pot = PolygonTransformer(num_types=10,\n",
    "                        emb_dim=7,\n",
    "                        num_heads=1,\n",
    "                        num_layers=1,\n",
    "                        ffn_dim=64, \n",
    "                        max_seq_len=max_seq_len,\n",
    "                        dropout=0.0)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y, batch_mask in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "            outputs = pot(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(pot, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conv\n",
    "##### refer to https://arxiv.org/pdf/1806.03857.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(wkts, types):\n",
    "    train_geoms = [gv.vectorize_wkt(wkt) for wkt in wkts]\n",
    "    \n",
    "    zipped = zip(train_geoms, types)\n",
    "    train_input_sorted = {}\n",
    "    train_labels_sorted = {}\n",
    "\n",
    "    for geom, label in sorted(zipped, key=lambda x: len(x[0]), reverse=True):\n",
    "        seq_len = geom.shape[0]\n",
    "        if seq_len in train_input_sorted:\n",
    "            train_input_sorted[seq_len].append(geom)\n",
    "            train_labels_sorted[seq_len].append(label)\n",
    "        else:\n",
    "            train_input_sorted[seq_len] = [geom]\n",
    "            train_labels_sorted[seq_len] = [label]\n",
    "    \n",
    "    return train_input_sorted, train_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_sorted, train_labels_sorted = prepare_dataset(ori_train_data, ori_train_labels)\n",
    "val_input_sorted, val_labels_sorted = prepare_dataset(ori_val_data, ori_val_labels)\n",
    "test_input_sorted, test_labels_sorted = prepare_dataset(ori_test_data, ori_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.626598565017476, Train Acc 0.59625, Val Loss: 2.012591189629323, Val Acc: 0.54\n",
      "Epoch: 2, Train Loss: 3.30355862308951, Train Acc 0.525, Val Loss: 1.7245333694122933, Val Acc: 0.54\n",
      "Epoch: 3, Train Loss: 3.087922242634437, Train Acc 0.515, Val Loss: 1.5360147598627452, Val Acc: 0.54\n",
      "Epoch: 4, Train Loss: 3.0041527840144493, Train Acc 0.52, Val Loss: 1.560276061296463, Val Acc: 0.54\n",
      "Epoch: 5, Train Loss: 2.9932270505849052, Train Acc 0.45625, Val Loss: 1.4831654413326367, Val Acc: 0.54\n",
      "Epoch: 6, Train Loss: 3.0723710594808353, Train Acc 0.31125, Val Loss: 1.5294046720137466, Val Acc: 0.54\n",
      "Epoch: 7, Train Loss: 3.1170386864858517, Train Acc 0.2625, Val Loss: 1.3922157557429493, Val Acc: 0.54\n",
      "Epoch: 8, Train Loss: 2.8722304820137867, Train Acc 0.4325, Val Loss: 1.3484793603420258, Val Acc: 0.54\n",
      "Epoch: 9, Train Loss: 2.7436239533126354, Train Acc 0.4625, Val Loss: 1.5909577523534362, Val Acc: 0.54\n",
      "Epoch: 10, Train Loss: 2.7839523729156044, Train Acc 0.4625, Val Loss: 1.486697555192419, Val Acc: 0.54\n",
      "Epoch: 11, Train Loss: 2.909132189610425, Train Acc 0.435, Val Loss: 1.2462995592001322, Val Acc: 0.57\n",
      "Epoch: 12, Train Loss: 2.668245099046651, Train Acc 0.48125, Val Loss: 1.2294595765100944, Val Acc: 0.59\n",
      "Epoch: 13, Train Loss: 2.516393326880301, Train Acc 0.5525, Val Loss: 1.1504864463129558, Val Acc: 0.62\n",
      "Epoch: 14, Train Loss: 2.4062341142664936, Train Acc 0.555, Val Loss: 1.1808744193734348, Val Acc: 0.59\n",
      "Epoch: 15, Train Loss: 2.604665701003636, Train Acc 0.555, Val Loss: 1.190511843642673, Val Acc: 0.6\n",
      "Epoch: 16, Train Loss: 2.4509756959536495, Train Acc 0.5175, Val Loss: 1.2251706630797, Val Acc: 0.57\n",
      "Epoch: 17, Train Loss: 2.4762285360518623, Train Acc 0.5725, Val Loss: 1.156696612770493, Val Acc: 0.6\n",
      "Epoch: 18, Train Loss: 2.426736544598551, Train Acc 0.56875, Val Loss: 1.1671810013216894, Val Acc: 0.59\n",
      "Epoch: 19, Train Loss: 2.520086375229499, Train Acc 0.5725, Val Loss: 1.157495392738162, Val Acc: 0.59\n",
      "Epoch: 20, Train Loss: 2.472528008853688, Train Acc 0.57, Val Loss: 1.1723093092441559, Val Acc: 0.64\n",
      "Epoch: 21, Train Loss: 2.3118344742585633, Train Acc 0.57, Val Loss: 1.129026099636748, Val Acc: 0.6\n",
      "Epoch: 22, Train Loss: 2.374385261798606, Train Acc 0.5925, Val Loss: 1.1365758909566983, Val Acc: 0.61\n",
      "Epoch: 23, Train Loss: 2.2420295292840287, Train Acc 0.5925, Val Loss: 1.113958030133634, Val Acc: 0.62\n",
      "Epoch: 24, Train Loss: 2.2824124772759045, Train Acc 0.59625, Val Loss: 1.1082987140964817, Val Acc: 0.62\n",
      "Epoch: 25, Train Loss: 2.4209476595415786, Train Acc 0.59625, Val Loss: 1.1777902438833907, Val Acc: 0.56\n",
      "Epoch: 26, Train Loss: 2.3294174307409454, Train Acc 0.58, Val Loss: 1.3298089657280896, Val Acc: 0.54\n",
      "Epoch: 27, Train Loss: 2.6666079049601272, Train Acc 0.51125, Val Loss: 1.2816480293467238, Val Acc: 0.55\n",
      "Epoch: 28, Train Loss: 2.4912553767071053, Train Acc 0.56125, Val Loss: 1.3095739501956347, Val Acc: 0.55\n",
      "Epoch: 29, Train Loss: 2.3824235466034973, Train Acc 0.58, Val Loss: 1.1681045253534574, Val Acc: 0.6\n",
      "Epoch: 30, Train Loss: 2.335898752159932, Train Acc 0.5925, Val Loss: 1.1642068048586716, Val Acc: 0.6\n",
      "Epoch: 31, Train Loss: 2.3625558965346394, Train Acc 0.5925, Val Loss: 1.1735343457879246, Val Acc: 0.6\n",
      "Epoch: 32, Train Loss: 2.258009274654529, Train Acc 0.5925, Val Loss: 1.125457824081988, Val Acc: 0.59\n",
      "Epoch: 33, Train Loss: 2.3919517906273113, Train Acc 0.595, Val Loss: 1.2325576624354802, Val Acc: 0.56\n",
      "Epoch: 34, Train Loss: 2.392585441470146, Train Acc 0.5925, Val Loss: 1.2118908940134823, Val Acc: 0.6\n",
      "Epoch: 35, Train Loss: 2.3789514750242233, Train Acc 0.5925, Val Loss: 1.1512556450592506, Val Acc: 0.59\n",
      "Epoch: 36, Train Loss: 2.3462553366142163, Train Acc 0.595, Val Loss: 1.1652810162789113, Val Acc: 0.57\n",
      "Epoch: 37, Train Loss: 2.411952840931275, Train Acc 0.59375, Val Loss: 1.1812938475931012, Val Acc: 0.57\n",
      "Epoch: 38, Train Loss: 2.2645699557136085, Train Acc 0.5925, Val Loss: 1.1684349179267883, Val Acc: 0.6\n",
      "Epoch: 39, Train Loss: 2.254560443408349, Train Acc 0.5925, Val Loss: 1.1592818397927929, Val Acc: 0.6\n",
      "Epoch: 40, Train Loss: 2.2344698143355988, Train Acc 0.595, Val Loss: 1.1388887118648838, Val Acc: 0.58\n",
      "Epoch: 41, Train Loss: 2.240072287619114, Train Acc 0.595, Val Loss: 1.1536883647377427, Val Acc: 0.56\n",
      "Epoch: 42, Train Loss: 2.2741422285051907, Train Acc 0.595, Val Loss: 1.1388117840161194, Val Acc: 0.56\n",
      "Epoch: 43, Train Loss: 2.229955402805525, Train Acc 0.60125, Val Loss: 1.1340498767189078, Val Acc: 0.58\n",
      "Epoch: 44, Train Loss: 2.3198331901255775, Train Acc 0.60125, Val Loss: 1.21864979251011, Val Acc: 0.57\n",
      "Epoch: 45, Train Loss: 2.1925460604183815, Train Acc 0.595, Val Loss: 1.2353126535544525, Val Acc: 0.57\n",
      "Epoch: 46, Train Loss: 2.370175708742703, Train Acc 0.5925, Val Loss: 1.1706249524612684, Val Acc: 0.6\n",
      "Epoch: 47, Train Loss: 2.241415281506146, Train Acc 0.59625, Val Loss: 1.1202406516751728, Val Acc: 0.59\n",
      "Epoch: 48, Train Loss: 2.345843402778401, Train Acc 0.595, Val Loss: 1.1695273317195274, Val Acc: 0.56\n",
      "Epoch: 49, Train Loss: 2.203638607088257, Train Acc 0.595, Val Loss: 1.1379173281225, Val Acc: 0.57\n",
      "Epoch: 50, Train Loss: 2.2076713740825653, Train Acc 0.59875, Val Loss: 1.1339134292022601, Val Acc: 0.59\n",
      "Epoch: 51, Train Loss: 2.2838850599877976, Train Acc 0.60125, Val Loss: 1.155861884355545, Val Acc: 0.56\n",
      "Epoch: 52, Train Loss: 2.234056670437841, Train Acc 0.6, Val Loss: 1.130275669130119, Val Acc: 0.59\n",
      "Epoch: 53, Train Loss: 2.198269596871208, Train Acc 0.60125, Val Loss: 1.1384678362189113, Val Acc: 0.56\n",
      "Epoch: 54, Train Loss: 2.18719783512985, Train Acc 0.60125, Val Loss: 1.1290307367170178, Val Acc: 0.58\n",
      "Epoch: 55, Train Loss: 2.26968697195544, Train Acc 0.60125, Val Loss: 1.1761249903891537, Val Acc: 0.56\n",
      "Epoch: 56, Train Loss: 2.273186707321335, Train Acc 0.595, Val Loss: 1.1445434939216923, Val Acc: 0.56\n",
      "Epoch: 57, Train Loss: 2.197601508568315, Train Acc 0.6, Val Loss: 1.139661720878369, Val Acc: 0.62\n",
      "Epoch: 58, Train Loss: 2.277900508221458, Train Acc 0.5975, Val Loss: 1.1262229809889923, Val Acc: 0.62\n",
      "Epoch: 59, Train Loss: 2.1985264122486115, Train Acc 0.6025, Val Loss: 1.1538957989699132, Val Acc: 0.6\n",
      "Epoch: 60, Train Loss: 2.2010206203250324, Train Acc 0.595, Val Loss: 1.1377519510082297, Val Acc: 0.6\n",
      "Epoch: 61, Train Loss: 2.1920738864470932, Train Acc 0.595, Val Loss: 1.12822663381293, Val Acc: 0.59\n",
      "Epoch: 62, Train Loss: 2.292268168400316, Train Acc 0.60125, Val Loss: 1.1537486475867194, Val Acc: 0.56\n",
      "Epoch: 63, Train Loss: 2.244647827218561, Train Acc 0.6025, Val Loss: 1.1440718504222664, Val Acc: 0.56\n",
      "Epoch: 64, Train Loss: 2.171851321178324, Train Acc 0.60125, Val Loss: 1.1316778406098082, Val Acc: 0.59\n",
      "Epoch: 65, Train Loss: 2.2688158434980057, Train Acc 0.60125, Val Loss: 1.132809957942447, Val Acc: 0.59\n",
      "Epoch: 66, Train Loss: 2.240652107140597, Train Acc 0.60125, Val Loss: 1.1371415660188005, Val Acc: 0.58\n",
      "Epoch: 67, Train Loss: 2.315420980839168, Train Acc 0.6025, Val Loss: 1.1266348394187722, Val Acc: 0.62\n",
      "Epoch: 68, Train Loss: 2.244277588584844, Train Acc 0.59625, Val Loss: 1.13417578307358, Val Acc: 0.59\n",
      "Epoch: 69, Train Loss: 2.2926358966266407, Train Acc 0.59625, Val Loss: 1.118466560099576, Val Acc: 0.6\n",
      "Epoch: 70, Train Loss: 2.273462136002148, Train Acc 0.6, Val Loss: 1.1457477928818882, Val Acc: 0.6\n",
      "Epoch: 71, Train Loss: 2.147315276457983, Train Acc 0.595, Val Loss: 1.1411033965445854, Val Acc: 0.6\n",
      "Epoch: 72, Train Loss: 2.2937557732357696, Train Acc 0.595, Val Loss: 1.1642258018255234, Val Acc: 0.56\n",
      "Epoch: 73, Train Loss: 2.151625272105722, Train Acc 0.595, Val Loss: 1.1327702705119107, Val Acc: 0.59\n",
      "Epoch: 74, Train Loss: 2.2576417589888855, Train Acc 0.60125, Val Loss: 1.151428991878355, Val Acc: 0.56\n",
      "Epoch: 75, Train Loss: 2.2432656121604584, Train Acc 0.6025, Val Loss: 1.1430357666434467, Val Acc: 0.57\n",
      "Epoch: 76, Train Loss: 2.1814820529783474, Train Acc 0.595, Val Loss: 1.1351794737416345, Val Acc: 0.59\n",
      "Epoch: 77, Train Loss: 2.2747655502137016, Train Acc 0.60125, Val Loss: 1.1581046959838353, Val Acc: 0.56\n",
      "Epoch: 78, Train Loss: 2.185489391141078, Train Acc 0.60125, Val Loss: 1.1300942414515727, Val Acc: 0.58\n",
      "Epoch: 79, Train Loss: 2.233240833177286, Train Acc 0.6, Val Loss: 1.132858796699627, Val Acc: 0.6\n",
      "Epoch: 80, Train Loss: 2.2779701194342445, Train Acc 0.6, Val Loss: 1.1382160082056716, Val Acc: 0.6\n",
      "Epoch: 81, Train Loss: 2.2969091762514675, Train Acc 0.6, Val Loss: 1.1555422101471875, Val Acc: 0.56\n",
      "Epoch: 82, Train Loss: 2.3877160154721317, Train Acc 0.6025, Val Loss: 1.1306762824187409, Val Acc: 0.6\n",
      "Epoch: 83, Train Loss: 2.309677013579537, Train Acc 0.595, Val Loss: 1.1413060825418782, Val Acc: 0.61\n",
      "Epoch: 84, Train Loss: 2.278064388562651, Train Acc 0.59875, Val Loss: 1.1523988536886267, Val Acc: 0.56\n",
      "Epoch: 85, Train Loss: 2.2376024201512337, Train Acc 0.60125, Val Loss: 1.1851190892425743, Val Acc: 0.57\n",
      "Epoch: 86, Train Loss: 2.2104643237064865, Train Acc 0.60125, Val Loss: 1.1303703776887946, Val Acc: 0.58\n",
      "Epoch: 87, Train Loss: 2.181286366546855, Train Acc 0.6, Val Loss: 1.1291204860081543, Val Acc: 0.6\n",
      "Epoch: 88, Train Loss: 2.3267542632187115, Train Acc 0.59875, Val Loss: 1.1728686223158966, Val Acc: 0.57\n",
      "Epoch: 89, Train Loss: 2.235611815662945, Train Acc 0.595, Val Loss: 1.185317697557243, Val Acc: 0.58\n",
      "Epoch: 90, Train Loss: 2.252790796844398, Train Acc 0.59625, Val Loss: 1.141443315389994, Val Acc: 0.6\n",
      "Epoch: 91, Train Loss: 2.1633065794320667, Train Acc 0.59625, Val Loss: 1.1381863403964687, Val Acc: 0.61\n",
      "Epoch: 92, Train Loss: 2.186924785375595, Train Acc 0.595, Val Loss: 1.1402003297934662, Val Acc: 0.59\n",
      "Epoch: 93, Train Loss: 2.177615471622523, Train Acc 0.60125, Val Loss: 1.1421721750820004, Val Acc: 0.56\n",
      "Epoch: 94, Train Loss: 2.3117606613565895, Train Acc 0.60125, Val Loss: 1.1664894360142786, Val Acc: 0.56\n",
      "Epoch: 95, Train Loss: 2.18637326228268, Train Acc 0.60125, Val Loss: 1.1406913206383988, Val Acc: 0.58\n",
      "Epoch: 96, Train Loss: 2.1740821560516075, Train Acc 0.60125, Val Loss: 1.1302336583266388, Val Acc: 0.58\n",
      "Epoch: 97, Train Loss: 2.255770296734922, Train Acc 0.6, Val Loss: 1.1415381355060112, Val Acc: 0.59\n",
      "Epoch: 98, Train Loss: 2.1899246950359905, Train Acc 0.60125, Val Loss: 1.156156015959946, Val Acc: 0.56\n",
      "Epoch: 99, Train Loss: 2.1748868406695476, Train Acc 0.60125, Val Loss: 1.143077719453219, Val Acc: 0.58\n",
      "Epoch: 100, Train Loss: 2.2204604937749752, Train Acc 0.60125, Val Loss: 1.1478779895885571, Val Acc: 0.58\n",
      "Epoch: 101, Train Loss: 2.28047021083972, Train Acc 0.6025, Val Loss: 1.1449200865384694, Val Acc: 0.56\n",
      "Epoch: 102, Train Loss: 2.2191701715483383, Train Acc 0.60125, Val Loss: 1.1449097676857098, Val Acc: 0.56\n",
      "Epoch: 103, Train Loss: 2.2540166619069435, Train Acc 0.59875, Val Loss: 1.137954747354662, Val Acc: 0.58\n",
      "Epoch: 104, Train Loss: 2.2967926246278427, Train Acc 0.6025, Val Loss: 1.1192138195037842, Val Acc: 0.62\n",
      "Epoch: 105, Train Loss: 2.1787749116911606, Train Acc 0.6025, Val Loss: 1.1591587590204704, Val Acc: 0.61\n",
      "Epoch: 106, Train Loss: 2.2439804848502662, Train Acc 0.59875, Val Loss: 1.1538417202395361, Val Acc: 0.59\n",
      "Epoch: 107, Train Loss: 2.305872247499578, Train Acc 0.6, Val Loss: 1.1923519695127331, Val Acc: 0.57\n",
      "Epoch: 108, Train Loss: 2.3823052793741226, Train Acc 0.60125, Val Loss: 1.165494226926082, Val Acc: 0.56\n",
      "Epoch: 109, Train Loss: 2.2145576639210476, Train Acc 0.6, Val Loss: 1.1446090002317686, Val Acc: 0.62\n",
      "Epoch: 110, Train Loss: 2.2461433607865784, Train Acc 0.59375, Val Loss: 1.1238170519873902, Val Acc: 0.58\n",
      "Epoch: 111, Train Loss: 2.215372057083775, Train Acc 0.6025, Val Loss: 1.1225823888907562, Val Acc: 0.58\n",
      "Epoch: 112, Train Loss: 2.1681961706456017, Train Acc 0.6025, Val Loss: 1.1296076794733871, Val Acc: 0.58\n",
      "Epoch: 113, Train Loss: 2.218838006258011, Train Acc 0.6025, Val Loss: 1.1346768897932928, Val Acc: 0.58\n",
      "Epoch: 114, Train Loss: 2.145111811511657, Train Acc 0.6025, Val Loss: 1.140748404167794, Val Acc: 0.59\n",
      "Epoch: 115, Train Loss: 2.276729331297033, Train Acc 0.6, Val Loss: 1.1672633572204694, Val Acc: 0.56\n",
      "Epoch: 116, Train Loss: 2.264116171528311, Train Acc 0.6, Val Loss: 1.1348944266100187, Val Acc: 0.58\n",
      "Epoch: 117, Train Loss: 2.2546164340832653, Train Acc 0.6025, Val Loss: 1.1344262679686417, Val Acc: 0.59\n",
      "Epoch: 118, Train Loss: 2.2536192361046288, Train Acc 0.60125, Val Loss: 1.1360872138190914, Val Acc: 0.58\n",
      "Epoch: 119, Train Loss: 2.1687196011052414, Train Acc 0.6025, Val Loss: 1.1384063242255031, Val Acc: 0.59\n",
      "Epoch: 120, Train Loss: 2.2838662100188873, Train Acc 0.60125, Val Loss: 1.160447877001118, Val Acc: 0.56\n",
      "Epoch: 121, Train Loss: 2.224020645898931, Train Acc 0.6025, Val Loss: 1.12453500966768, Val Acc: 0.58\n",
      "Epoch: 122, Train Loss: 2.173069706734489, Train Acc 0.6025, Val Loss: 1.110067447697794, Val Acc: 0.6\n",
      "Epoch: 123, Train Loss: 2.15594392883427, Train Acc 0.6025, Val Loss: 1.1276132460381534, Val Acc: 0.6\n",
      "Epoch: 124, Train Loss: 2.238076984005816, Train Acc 0.6, Val Loss: 1.1384165536712956, Val Acc: 0.59\n",
      "Epoch: 125, Train Loss: 2.1563783831456127, Train Acc 0.6, Val Loss: 1.1407070558618855, Val Acc: 0.59\n",
      "Epoch: 126, Train Loss: 2.320006303050939, Train Acc 0.60125, Val Loss: 1.2314258966896985, Val Acc: 0.57\n",
      "Epoch: 127, Train Loss: 2.1473289289895225, Train Acc 0.6025, Val Loss: 1.1139177224926047, Val Acc: 0.6\n",
      "Epoch: 128, Train Loss: 2.237972338409985, Train Acc 0.6025, Val Loss: 1.1254131359023016, Val Acc: 0.59\n",
      "Epoch: 129, Train Loss: 2.19643270881737, Train Acc 0.6025, Val Loss: 1.123354729365658, Val Acc: 0.62\n",
      "Epoch: 130, Train Loss: 2.262968923239147, Train Acc 0.6025, Val Loss: 1.1348065830565788, Val Acc: 0.59\n",
      "Epoch: 131, Train Loss: 2.328226649585892, Train Acc 0.6025, Val Loss: 1.1491815753885217, Val Acc: 0.56\n",
      "Epoch: 132, Train Loss: 2.2932313742006527, Train Acc 0.60125, Val Loss: 1.205958585481386, Val Acc: 0.57\n",
      "Epoch: 133, Train Loss: 2.152367985862143, Train Acc 0.59625, Val Loss: 1.1397912055253983, Val Acc: 0.62\n",
      "Epoch: 134, Train Loss: 2.2804681492202423, Train Acc 0.6, Val Loss: 1.1133795694724933, Val Acc: 0.6\n",
      "Epoch: 135, Train Loss: 2.1924490911119126, Train Acc 0.6025, Val Loss: 1.133862233242473, Val Acc: 0.59\n",
      "Epoch: 136, Train Loss: 2.200677006998483, Train Acc 0.60125, Val Loss: 1.1499628086347837, Val Acc: 0.56\n",
      "Epoch: 137, Train Loss: 2.3066876705955055, Train Acc 0.6, Val Loss: 1.1933783945199605, Val Acc: 0.57\n",
      "Epoch: 138, Train Loss: 2.2412143950076664, Train Acc 0.6025, Val Loss: 1.125431183222178, Val Acc: 0.61\n",
      "Epoch: 139, Train Loss: 2.2453531596590492, Train Acc 0.59875, Val Loss: 1.140250792374482, Val Acc: 0.58\n",
      "Epoch: 140, Train Loss: 2.2681454174658833, Train Acc 0.6025, Val Loss: 1.128495902628512, Val Acc: 0.62\n",
      "Epoch: 141, Train Loss: 2.1477830322349774, Train Acc 0.6025, Val Loss: 1.1415957478252616, Val Acc: 0.61\n",
      "Epoch: 142, Train Loss: 2.2067647624541733, Train Acc 0.6025, Val Loss: 1.153753455828976, Val Acc: 0.56\n",
      "Epoch: 143, Train Loss: 2.1459331797326313, Train Acc 0.60125, Val Loss: 1.1460753594701354, Val Acc: 0.58\n",
      "Epoch: 144, Train Loss: 2.223447249216192, Train Acc 0.6025, Val Loss: 1.1480758117662895, Val Acc: 0.59\n",
      "Epoch: 145, Train Loss: 2.2395712838453403, Train Acc 0.6, Val Loss: 1.1499803533425201, Val Acc: 0.58\n",
      "Epoch: 146, Train Loss: 2.2731160749407375, Train Acc 0.6025, Val Loss: 1.155047656716527, Val Acc: 0.59\n",
      "Epoch: 147, Train Loss: 2.1601884330020233, Train Acc 0.6025, Val Loss: 1.126462393918553, Val Acc: 0.58\n",
      "Epoch: 148, Train Loss: 2.145214077742661, Train Acc 0.6025, Val Loss: 1.1322179142687772, Val Acc: 0.6\n",
      "Epoch: 149, Train Loss: 2.1305501268190494, Train Acc 0.6, Val Loss: 1.1410526359403454, Val Acc: 0.59\n",
      "Epoch: 150, Train Loss: 2.2274027159985375, Train Acc 0.6025, Val Loss: 1.1657395233979095, Val Acc: 0.56\n",
      "Epoch: 151, Train Loss: 2.2936385168748745, Train Acc 0.6025, Val Loss: 1.1643786921694472, Val Acc: 0.56\n",
      "Epoch: 152, Train Loss: 2.2355591593419804, Train Acc 0.59625, Val Loss: 1.1301105046594464, Val Acc: 0.62\n",
      "Epoch: 153, Train Loss: 2.204654045841273, Train Acc 0.59625, Val Loss: 1.1297312177516319, Val Acc: 0.61\n",
      "Epoch: 154, Train Loss: 2.213243526132668, Train Acc 0.6025, Val Loss: 1.1427500533896524, Val Acc: 0.57\n",
      "Epoch: 155, Train Loss: 2.199846875141649, Train Acc 0.6025, Val Loss: 1.1488510611894969, Val Acc: 0.56\n",
      "Epoch: 156, Train Loss: 2.16819322416011, Train Acc 0.6025, Val Loss: 1.1139908875968005, Val Acc: 0.6\n",
      "Epoch: 157, Train Loss: 2.1768976546385708, Train Acc 0.6025, Val Loss: 1.122117804111661, Val Acc: 0.59\n",
      "Epoch: 158, Train Loss: 2.1705907925086865, Train Acc 0.6025, Val Loss: 1.3899499959236867, Val Acc: 0.54\n",
      "Epoch: 159, Train Loss: 2.157510638675269, Train Acc 0.605, Val Loss: 1.1340181416756399, Val Acc: 0.61\n",
      "Epoch: 160, Train Loss: 2.1457066877799877, Train Acc 0.6025, Val Loss: 1.124736105268066, Val Acc: 0.6\n",
      "Epoch: 161, Train Loss: 2.1892279092879856, Train Acc 0.6, Val Loss: 1.136628784037925, Val Acc: 0.58\n",
      "Epoch: 162, Train Loss: 2.196631822077667, Train Acc 0.6025, Val Loss: 1.135347698588629, Val Acc: 0.61\n",
      "Epoch: 163, Train Loss: 2.1802536735639855, Train Acc 0.6, Val Loss: 1.1510428089547802, Val Acc: 0.56\n",
      "Epoch: 164, Train Loss: 2.168105279259822, Train Acc 0.6025, Val Loss: 1.1493219306340088, Val Acc: 0.6\n",
      "Epoch: 165, Train Loss: 2.16655973564176, Train Acc 0.6, Val Loss: 1.1510581635945552, Val Acc: 0.58\n",
      "Epoch: 166, Train Loss: 2.130241238457315, Train Acc 0.6025, Val Loss: 1.1419382300731298, Val Acc: 0.58\n",
      "Epoch: 167, Train Loss: 2.2312114015221596, Train Acc 0.6, Val Loss: 1.1410264256032738, Val Acc: 0.59\n",
      "Epoch: 168, Train Loss: 2.2894326334490493, Train Acc 0.6025, Val Loss: 1.1823078555029791, Val Acc: 0.57\n",
      "Epoch: 169, Train Loss: 2.331876516342163, Train Acc 0.6025, Val Loss: 1.1626554872538593, Val Acc: 0.59\n",
      "Epoch: 170, Train Loss: 2.2534555402748726, Train Acc 0.6025, Val Loss: 1.183387881195223, Val Acc: 0.6\n",
      "Epoch: 171, Train Loss: 2.2581454607493736, Train Acc 0.59625, Val Loss: 1.1324592409907162, Val Acc: 0.6\n",
      "Epoch: 172, Train Loss: 2.140715514035786, Train Acc 0.60125, Val Loss: 1.1378441682538472, Val Acc: 0.6\n",
      "Epoch: 173, Train Loss: 2.2670329677708008, Train Acc 0.60125, Val Loss: 1.1453528657958314, Val Acc: 0.58\n",
      "Epoch: 174, Train Loss: 2.2371737045400284, Train Acc 0.61375, Val Loss: 1.1387800292388812, Val Acc: 0.59\n",
      "Epoch: 175, Train Loss: 2.21133388403584, Train Acc 0.61375, Val Loss: 1.1235391553994771, Val Acc: 0.61\n",
      "Epoch: 176, Train Loss: 2.1450442159000565, Train Acc 0.605, Val Loss: 1.1385329728190963, Val Acc: 0.6\n",
      "Epoch: 177, Train Loss: 2.174729923991596, Train Acc 0.6025, Val Loss: 1.1492581186262336, Val Acc: 0.58\n",
      "Epoch: 178, Train Loss: 2.142744169515722, Train Acc 0.605, Val Loss: 1.154743277946034, Val Acc: 0.58\n",
      "Epoch: 179, Train Loss: 2.2015210451448666, Train Acc 0.6025, Val Loss: 1.1717807872069848, Val Acc: 0.57\n",
      "Epoch: 180, Train Loss: 2.2691714500679687, Train Acc 0.6025, Val Loss: 1.131384472991969, Val Acc: 0.6\n",
      "Epoch: 181, Train Loss: 2.2230792282258762, Train Acc 0.60375, Val Loss: 1.15259426429465, Val Acc: 0.56\n",
      "Epoch: 182, Train Loss: 2.2710282338016174, Train Acc 0.6025, Val Loss: 1.12810019544653, Val Acc: 0.61\n",
      "Epoch: 183, Train Loss: 2.174035948865554, Train Acc 0.6025, Val Loss: 1.1359976691168707, Val Acc: 0.6\n",
      "Epoch: 184, Train Loss: 2.259244235122905, Train Acc 0.6025, Val Loss: 1.1535219147398665, Val Acc: 0.59\n",
      "Epoch: 185, Train Loss: 2.2401113063097, Train Acc 0.6025, Val Loss: 1.1505918381987392, Val Acc: 0.58\n",
      "Epoch: 186, Train Loss: 2.2194398878251804, Train Acc 0.615, Val Loss: 1.144312350331126, Val Acc: 0.6\n",
      "Epoch: 187, Train Loss: 2.1568029181045643, Train Acc 0.61375, Val Loss: 1.1060667444725294, Val Acc: 0.6\n",
      "Epoch: 188, Train Loss: 2.1433440306607414, Train Acc 0.61375, Val Loss: 1.1178279088155643, Val Acc: 0.61\n",
      "Epoch: 189, Train Loss: 2.149993386338739, Train Acc 0.61375, Val Loss: 1.1247908150827564, Val Acc: 0.59\n",
      "Epoch: 190, Train Loss: 2.1333702118957745, Train Acc 0.61375, Val Loss: 1.1288235775522284, Val Acc: 0.6\n",
      "Epoch: 191, Train Loss: 2.1136304446879555, Train Acc 0.61375, Val Loss: 1.1412011103050128, Val Acc: 0.6\n",
      "Epoch: 192, Train Loss: 2.2007921723758472, Train Acc 0.6125, Val Loss: 1.1733250791156613, Val Acc: 0.56\n",
      "Epoch: 193, Train Loss: 2.1401221747784054, Train Acc 0.61375, Val Loss: 1.1515203094160236, Val Acc: 0.6\n",
      "Epoch: 194, Train Loss: 2.1319167390465736, Train Acc 0.61375, Val Loss: 1.1258295561010774, Val Acc: 0.6\n",
      "Epoch: 195, Train Loss: 2.328448672504986, Train Acc 0.61375, Val Loss: 1.17017614922008, Val Acc: 0.56\n",
      "Epoch: 196, Train Loss: 2.196562647819519, Train Acc 0.6025, Val Loss: 1.1249907282558647, Val Acc: 0.61\n",
      "Epoch: 197, Train Loss: 2.2599918211207672, Train Acc 0.61375, Val Loss: 1.1132939800217345, Val Acc: 0.6\n",
      "Epoch: 198, Train Loss: 2.2722648986998726, Train Acc 0.61375, Val Loss: 1.1483223341606759, Val Acc: 0.6\n",
      "Epoch: 199, Train Loss: 2.185009796829785, Train Acc 0.61375, Val Loss: 1.1149121031567857, Val Acc: 0.61\n",
      "Epoch: 200, Train Loss: 2.172952515675741, Train Acc 0.61375, Val Loss: 1.131468209463197, Val Acc: 0.62\n",
      "Test Loss: 1.2551521560863446, Test Acc: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "sequence_length = 10\n",
    "geom_vector_len = 7  # Assuming geom_vector_len is known\n",
    "dense_size = 32  # Size of the dense layer\n",
    "dropout = 0.0  # Dropout rate\n",
    "num_classes = 10  # Number of output classes\n",
    "batch_size = 32\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes).to(device)\n",
    "if USE_GPU:\n",
    "    conv_model = conv_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.004)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "def train(model, inputs_sorted, labels_sorted):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    total_batch_train = 0\n",
    "    for seq_len in train_input_sorted:\n",
    "        inputs = torch.tensor(inputs_sorted[seq_len], dtype=torch.float32)\n",
    "        labels = torch.tensor(labels_sorted[seq_len], dtype=torch.long)\n",
    "        dataset = TensorDataset(inputs, labels)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = conv_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            total_batch_train += 1\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    train_loss /= total_batch_train\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def evaluate(model, inputs_sorted, labels_sorted):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_batch_eval = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_len in inputs_sorted:\n",
    "            inputs = torch.tensor(inputs_sorted[seq_len], dtype=torch.float32)\n",
    "            labels = torch.tensor(labels_sorted[seq_len], dtype=torch.long)\n",
    "            dataset = TensorDataset(inputs, labels)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            for batch_x, batch_y in loader:\n",
    "                if USE_GPU:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                eval_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += batch_y.size(0)\n",
    "                total_batch_eval += 1\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= total_batch_eval\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(conv_model, train_input_sorted, train_labels_sorted)\n",
    "    val_loss, val_acc = evaluate(conv_model, val_input_sorted, val_labels_sorted)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(conv_model, test_input_sorted, test_labels_sorted)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the Conv model: 16266\n",
      "Total number of parameters in the Transformer model: 11376\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters\n",
    "total_params_conv_model = sum(p.numel() for p in conv_model.parameters())\n",
    "print(f\"Total number of parameters in the Conv model: {total_params_conv_model}\")\n",
    "\n",
    "total_params_pot_model = sum(p.numel() for p in pot.parameters())\n",
    "print(f\"Total number of parameters in the Transformer model: {total_params_pot_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
