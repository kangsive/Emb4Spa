{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Same MLP applied to all token(position) representations\"\"\"\n",
    "    def __init__(self, emb_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0, \"Embedding dimension must be divided by number of heads\"\n",
    "\n",
    "        # Dimensions initialization\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        # all features are divided into multi head, each head have a part of features\n",
    "        self.head_emb_dim = self.emb_dim // self.num_heads\n",
    "\n",
    "        # Transformation matrixs\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_emb_dim)\n",
    "\n",
    "        # Mask scores (where positions are 0) with near negative inf\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply sofxmax to attention scores\n",
    "        attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Get the final output\n",
    "        output = torch.matmul(attn_scores, V)\n",
    "        return output\n",
    "    \n",
    "    def split(self, x):\n",
    "        # Reshape the input emb_dim (to multi-head, each head owns a part of input features) for multi-head attention\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        # transpose to fix batch_size and num_heads, let seq_len, head_emb_dim participate in matrix multiplication\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_emb_dim).transpose(1, 2)\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size, num_heads, seq_len, head_emb_dim = x.size()\n",
    "        # contiguous() ensures the memory layout of the tensor is contiguous\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.emb_dim)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Split input to multi heads\n",
    "        Q = self.split(self.W_q(Q))\n",
    "        K = self.split(self.W_k(K))\n",
    "        V = self.split(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine outputs and apply transformation\n",
    "        output = self.W_o(self.combine(attn_output))\n",
    "        return output\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_atten = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ffn = PositionWiseFFN(emb_dim, ffn_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_atten(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolygonEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads,\n",
    "                num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(emb_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, emb_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # token_mask = (tokens != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        cls_mask = torch.ones((batch_size, 1, 1, 1), dtype=torch.bool)\n",
    "        mask = torch.cat((cls_mask, mask), dim=3)\n",
    "        \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            x = enc_layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PolygonTransformer(nn.Module):\n",
    "    def __init__(self, num_types, emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = PolygonEncoder(emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(emb_dim, ffn_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(ffn_dim, num_types))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.encoder(x, mask)\n",
    "        x = x[:, 0, :] # grab the class embedding\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# wkts = df['WKT'][:1000].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(df, dataset_size, max_seq_len): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    wkts = df['WKT'][:dataset_size]\n",
    "    types = df['type'][:dataset_size]\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices < start_points\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "max_seq_len = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels, mask = prepare_polygon_dataset(df, dataset_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(tokens, mask, labels)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.2751638889312744\n",
      "Epoch: 2, Loss: 2.267160654067993\n",
      "Epoch: 3, Loss: 2.076948404312134\n",
      "Epoch: 4, Loss: 2.061600923538208\n",
      "Epoch: 5, Loss: 2.050260066986084\n",
      "Epoch: 6, Loss: 1.950992465019226\n",
      "Epoch: 7, Loss: 1.757738709449768\n",
      "Epoch: 8, Loss: 1.6771175861358643\n",
      "Epoch: 9, Loss: 1.7088524103164673\n",
      "Epoch: 10, Loss: 1.7894134521484375\n",
      "Epoch: 11, Loss: 1.5296170711517334\n",
      "Epoch: 12, Loss: 1.529089093208313\n",
      "Epoch: 13, Loss: 1.5843652486801147\n",
      "Epoch: 14, Loss: 1.6824382543563843\n",
      "Epoch: 15, Loss: 1.491610050201416\n",
      "Epoch: 16, Loss: 1.16709566116333\n",
      "Epoch: 17, Loss: 1.4058548212051392\n",
      "Epoch: 18, Loss: 1.1387109756469727\n",
      "Epoch: 19, Loss: 1.306123971939087\n",
      "Epoch: 20, Loss: 1.4469846487045288\n"
     ]
    }
   ],
   "source": [
    "pot = PolygonTransformer(num_types=10,\n",
    "                        emb_dim=7,\n",
    "                        num_heads=1,\n",
    "                        num_layers=6,\n",
    "                        ffn_dim=64, \n",
    "                        max_seq_len=64,\n",
    "                        dropout=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "pot.train()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batch_x, batch_mask, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = pot(batch_x, batch_mask)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([894, 64, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.3755531311035156\n",
      "Epoch: 2, Loss: 2.3280155658721924\n",
      "Epoch: 3, Loss: 2.3263702392578125\n",
      "Epoch: 4, Loss: 2.328519105911255\n",
      "Epoch: 5, Loss: 2.367708683013916\n",
      "Epoch: 6, Loss: 2.300119161605835\n",
      "Epoch: 7, Loss: 2.3383395671844482\n",
      "Epoch: 8, Loss: 2.328643560409546\n",
      "Epoch: 9, Loss: 2.29815411567688\n",
      "Epoch: 10, Loss: 2.3713440895080566\n",
      "Epoch: 11, Loss: 2.336233615875244\n",
      "Epoch: 12, Loss: 2.325115203857422\n",
      "Epoch: 13, Loss: 2.3378660678863525\n",
      "Epoch: 14, Loss: 2.3752248287200928\n",
      "Epoch: 15, Loss: 2.3822243213653564\n",
      "Epoch: 16, Loss: 2.359609842300415\n",
      "Epoch: 17, Loss: 2.302586555480957\n",
      "Epoch: 18, Loss: 2.3237926959991455\n",
      "Epoch: 19, Loss: 2.3504207134246826\n",
      "Epoch: 20, Loss: 2.333038568496704\n",
      "Epoch: 21, Loss: 2.3355369567871094\n",
      "Epoch: 22, Loss: 2.33678936958313\n",
      "Epoch: 23, Loss: 2.361912965774536\n",
      "Epoch: 24, Loss: 2.3079512119293213\n",
      "Epoch: 25, Loss: 2.3314294815063477\n",
      "Epoch: 26, Loss: 2.2821238040924072\n",
      "Epoch: 27, Loss: 2.350801944732666\n",
      "Epoch: 28, Loss: 2.3137223720550537\n",
      "Epoch: 29, Loss: 2.3736181259155273\n",
      "Epoch: 30, Loss: 2.354924201965332\n",
      "Epoch: 31, Loss: 2.379969358444214\n",
      "Epoch: 32, Loss: 2.332777500152588\n",
      "Epoch: 33, Loss: 2.339202642440796\n",
      "Epoch: 34, Loss: 2.3735156059265137\n",
      "Epoch: 35, Loss: 2.324387550354004\n",
      "Epoch: 36, Loss: 2.3194849491119385\n",
      "Epoch: 37, Loss: 2.344991683959961\n",
      "Epoch: 38, Loss: 2.334901809692383\n",
      "Epoch: 39, Loss: 2.392930030822754\n",
      "Epoch: 40, Loss: 2.2791903018951416\n",
      "Epoch: 41, Loss: 2.338121175765991\n",
      "Epoch: 42, Loss: 2.3775863647460938\n",
      "Epoch: 43, Loss: 2.33426833152771\n",
      "Epoch: 44, Loss: 2.3277482986450195\n",
      "Epoch: 45, Loss: 2.3215548992156982\n",
      "Epoch: 46, Loss: 2.3455724716186523\n",
      "Epoch: 47, Loss: 2.3408260345458984\n",
      "Epoch: 48, Loss: 2.378897190093994\n",
      "Epoch: 49, Loss: 2.3373663425445557\n",
      "Epoch: 50, Loss: 2.3568310737609863\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "geom_vector_len = 7  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "output_size = 10  # Number of output classes\n",
    "conv_model = CompareModel(geom_vector_len, dense_size, dropout, output_size)\n",
    "\n",
    "# Print the model architecture\n",
    "# print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "conv_model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch_x, batch_mask, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = conv_model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
