{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Same MLP applied to all token(position) representations\"\"\"\n",
    "    def __init__(self, emb_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0, \"Embedding dimension must be divided by number of heads\"\n",
    "\n",
    "        # Dimensions initialization\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        # all features are divided into multi head, each head have a part of features\n",
    "        self.head_emb_dim = self.emb_dim // self.num_heads\n",
    "\n",
    "        # Transformation matrixs\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_emb_dim)\n",
    "\n",
    "        # Mask scores (where positions are 0) with near negative inf\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply sofxmax to attention scores\n",
    "        attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Get the final output\n",
    "        output = torch.matmul(attn_scores, V)\n",
    "        return output\n",
    "    \n",
    "    def split(self, x):\n",
    "        # Reshape the input emb_dim (to multi-head, each head owns a part of input features) for multi-head attention\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        # transpose to fix batch_size and num_heads, let seq_len, head_emb_dim participate in matrix multiplication\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_emb_dim).transpose(1, 2)\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size, num_heads, seq_len, head_emb_dim = x.size()\n",
    "        # contiguous() ensures the memory layout of the tensor is contiguous\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.emb_dim)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Split input to multi heads\n",
    "        Q = self.split(self.W_q(Q))\n",
    "        K = self.split(self.W_k(K))\n",
    "        V = self.split(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine outputs and apply transformation\n",
    "        output = self.W_o(self.combine(attn_output))\n",
    "        return output\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_atten = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ffn = PositionWiseFFN(emb_dim, ffn_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_atten(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolygonEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads,\n",
    "                num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(emb_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, emb_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # token_mask = (tokens != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        cls_mask = torch.ones((batch_size, 1, 1, 1), dtype=torch.bool)\n",
    "        mask = torch.cat((cls_mask, mask), dim=3)\n",
    "        \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            x = enc_layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PolygonTransformer(nn.Module):\n",
    "    def __init__(self, num_types, emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = PolygonEncoder(emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(emb_dim, ffn_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(ffn_dim, num_types))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.encoder(x, mask)\n",
    "        x = x[:, 0, :] # grab the class embedding\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(df, val_split_ratio, test_split_ratio):\n",
    "\n",
    "    data, labels = np.array(df['WKT'].tolist()), np.array(df['type'].tolist())\n",
    "\n",
    "    num_val = int(val_split_ratio * len(df))\n",
    "    num_test = int(test_split_ratio * len(df))\n",
    "\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = indices[num_val+num_test:], indices[:num_val], indices[num_val:num_val+num_test]\n",
    "\n",
    "    train_data, train_labels = data[train_indices], labels[train_indices]\n",
    "    val_data, val_labels = data[val_indices], labels[val_indices]\n",
    "    test_data, test_labels = data[test_indices], labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = dataset_split(df, 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(wkts, types, max_seq_len): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices < start_points\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_labels, train_mask = prepare_polygon_dataset(train_data, train_labels, max_seq_len)\n",
    "val_tokens, val_labels, val_mask = prepare_polygon_dataset(val_data, val_labels, max_seq_len)\n",
    "test_tokens, test_labels, test_mask = prepare_polygon_dataset(test_data, test_labels, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_split_ratio, test_split_ratio = 0.1, 0.2\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.1, 0.2])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels, train_mask), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens, val_labels, val_mask), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(test_tokens, test_labels, test_mask), batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.062318855694362095, Val Loss: 1.8923246065775554, Val Acc: 0.21505376344086022\n",
      "Epoch: 2, Train Loss: 0.05251908540725708, Val Loss: 1.570486068725586, Val Acc: 0.21505376344086022\n",
      "Epoch: 3, Train Loss: 0.04657983609608241, Val Loss: 1.3656121492385864, Val Acc: 0.21505376344086022\n",
      "Epoch: 4, Train Loss: 0.042496724469321116, Val Loss: 1.300952931245168, Val Acc: 0.21505376344086022\n",
      "Epoch: 5, Train Loss: 0.041170631476811, Val Loss: 1.2913121183713276, Val Acc: 0.21505376344086022\n",
      "Epoch: 6, Train Loss: 0.040675604002816335, Val Loss: 1.272122323513031, Val Acc: 0.21505376344086022\n",
      "Epoch: 7, Train Loss: 0.039913969039916994, Val Loss: 1.2713905175526936, Val Acc: 0.21505376344086022\n",
      "Epoch: 8, Train Loss: 0.040540697063718524, Val Loss: 1.2641054789225261, Val Acc: 0.21505376344086022\n",
      "Epoch: 9, Train Loss: 0.0397218839611326, Val Loss: 1.2699339389801025, Val Acc: 0.21505376344086022\n",
      "Epoch: 10, Train Loss: 0.04048592959131513, Val Loss: 1.2603050072987874, Val Acc: 0.21505376344086022\n",
      "Epoch: 11, Train Loss: 0.03941447521959032, Val Loss: 1.2687506874402363, Val Acc: 0.21505376344086022\n",
      "Epoch: 12, Train Loss: 0.039861273424965996, Val Loss: 1.2711045344670613, Val Acc: 0.21505376344086022\n",
      "Epoch: 13, Train Loss: 0.03916690766811371, Val Loss: 1.2602307796478271, Val Acc: 0.21505376344086022\n",
      "Epoch: 14, Train Loss: 0.039559260266167774, Val Loss: 1.2538280089696248, Val Acc: 0.21505376344086022\n",
      "Epoch: 15, Train Loss: 0.03922817485673087, Val Loss: 1.2630178729693096, Val Acc: 0.21505376344086022\n",
      "Epoch: 16, Train Loss: 0.04132536845547812, Val Loss: 1.2566332817077637, Val Acc: 0.21505376344086022\n",
      "Epoch: 17, Train Loss: 0.03962472643171038, Val Loss: 1.2702110409736633, Val Acc: 0.21505376344086022\n",
      "Epoch: 18, Train Loss: 0.03976830073765346, Val Loss: 1.2509085536003113, Val Acc: 0.21505376344086022\n",
      "Epoch: 19, Train Loss: 0.03960217544010707, Val Loss: 1.2592398921648662, Val Acc: 0.21505376344086022\n",
      "Epoch: 20, Train Loss: 0.039632431438991, Val Loss: 1.2541149457295735, Val Acc: 0.21505376344086022\n",
      "Epoch: 21, Train Loss: 0.039800316265651156, Val Loss: 1.2419209877649944, Val Acc: 0.21505376344086022\n",
      "Epoch: 22, Train Loss: 0.03924753155027117, Val Loss: 1.2462359070777893, Val Acc: 0.21505376344086022\n",
      "Epoch: 23, Train Loss: 0.03877352561269488, Val Loss: 1.229565938313802, Val Acc: 0.21505376344086022\n",
      "Epoch: 24, Train Loss: 0.03926420841898237, Val Loss: 1.2046114603678386, Val Acc: 0.21505376344086022\n",
      "Epoch: 25, Train Loss: 0.03685996881553105, Val Loss: 1.1826861103375752, Val Acc: 0.21505376344086022\n",
      "Epoch: 26, Train Loss: 0.03852508459772382, Val Loss: 1.2297630707422893, Val Acc: 0.21505376344086022\n",
      "Epoch: 27, Train Loss: 0.037996311272893636, Val Loss: 1.1709298690160115, Val Acc: 0.21505376344086022\n",
      "Epoch: 28, Train Loss: 0.03690470593316215, Val Loss: 1.1659664312998455, Val Acc: 0.22580645161290322\n",
      "Epoch: 29, Train Loss: 0.037707490835871015, Val Loss: 1.173977752526601, Val Acc: 0.21505376344086022\n",
      "Epoch: 30, Train Loss: 0.03765999223504748, Val Loss: 1.2125403086344402, Val Acc: 0.23655913978494625\n",
      "Epoch: 31, Train Loss: 0.03743285059928894, Val Loss: 1.2127468784650166, Val Acc: 0.21505376344086022\n",
      "Epoch: 32, Train Loss: 0.03731989502906799, Val Loss: 1.1690463821093242, Val Acc: 0.23655913978494625\n",
      "Epoch: 33, Train Loss: 0.03802759689944131, Val Loss: 1.193380614121755, Val Acc: 0.21505376344086022\n",
      "Epoch: 34, Train Loss: 0.03663059268678938, Val Loss: 1.1695487300554912, Val Acc: 0.21505376344086022\n",
      "Epoch: 35, Train Loss: 0.037468191640717644, Val Loss: 1.171062171459198, Val Acc: 0.21505376344086022\n",
      "Epoch: 36, Train Loss: 0.03683480645929064, Val Loss: 1.2066332300504048, Val Acc: 0.23655913978494625\n",
      "Epoch: 37, Train Loss: 0.036956418837819784, Val Loss: 1.1484145124753316, Val Acc: 0.21505376344086022\n",
      "Epoch: 38, Train Loss: 0.036819346972874235, Val Loss: 1.1710236469904582, Val Acc: 0.21505376344086022\n",
      "Epoch: 39, Train Loss: 0.03636790224484035, Val Loss: 1.1983117858568828, Val Acc: 0.21505376344086022\n",
      "Epoch: 40, Train Loss: 0.03670763645853315, Val Loss: 1.161864976088206, Val Acc: 0.21505376344086022\n",
      "Epoch: 41, Train Loss: 0.0369525465794972, Val Loss: 1.1717665791511536, Val Acc: 0.21505376344086022\n",
      "Epoch: 42, Train Loss: 0.03704436463969094, Val Loss: 1.188057541847229, Val Acc: 0.21505376344086022\n",
      "Epoch: 43, Train Loss: 0.03627889284065792, Val Loss: 1.18293563524882, Val Acc: 0.21505376344086022\n",
      "Epoch: 44, Train Loss: 0.03762724203722818, Val Loss: 1.1605721513430278, Val Acc: 0.21505376344086022\n",
      "Epoch: 45, Train Loss: 0.037123055287769866, Val Loss: 1.157274107138316, Val Acc: 0.21505376344086022\n",
      "Epoch: 46, Train Loss: 0.03653340458869934, Val Loss: 1.1737253069877625, Val Acc: 0.21505376344086022\n",
      "Epoch: 47, Train Loss: 0.035817870582853045, Val Loss: 1.1698442498842876, Val Acc: 0.21505376344086022\n",
      "Epoch: 48, Train Loss: 0.03614493489265442, Val Loss: 1.1988407969474792, Val Acc: 0.21505376344086022\n",
      "Epoch: 49, Train Loss: 0.03811803051403591, Val Loss: 1.1544472376505535, Val Acc: 0.21505376344086022\n",
      "Epoch: 50, Train Loss: 0.03646649880068643, Val Loss: 1.1733558177947998, Val Acc: 0.21505376344086022\n",
      "Test Loss: 1.1606313188870747, Test Acc: 0.25136612021857924\n"
     ]
    }
   ],
   "source": [
    "pot = PolygonTransformer(num_types=10,\n",
    "                        emb_dim=7,\n",
    "                        num_heads=1,\n",
    "                        num_layers=6,\n",
    "                        ffn_dim=64, \n",
    "                        max_seq_len=64,\n",
    "                        dropout=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "for epoch in range(50):\n",
    "    pot.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y, batch_mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    pot.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in val_loader:\n",
    "            outputs = pot(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss/train_data.shape[0]}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "pot.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_mask in test_loader:\n",
    "        outputs = pot(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conv\n",
    "##### refer to https://arxiv.org/pdf/1806.03857.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(wkts, types):\n",
    "    train_geoms = [gv.vectorize_wkt(wkt) for wkt in wkts]\n",
    "    \n",
    "    zipped = zip(train_geoms, types)\n",
    "    train_input_sorted = {}\n",
    "    train_labels_sorted = {}\n",
    "\n",
    "    for geom, label in sorted(zipped, key=lambda x: len(x[0]), reverse=True):\n",
    "        seq_len = geom.shape[0]\n",
    "        if seq_len in train_input_sorted:\n",
    "            train_input_sorted[seq_len].append(geom)\n",
    "            train_labels_sorted[seq_len].append(label)\n",
    "        else:\n",
    "            train_input_sorted[seq_len] = [geom]\n",
    "            train_labels_sorted[seq_len] = [label]\n",
    "    \n",
    "    return train_input_sorted, train_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_sorted, train_labels_sorted = prepare_dataset(train_data, train_labels)\n",
    "val_input_sorted, val_labels_sorted = prepare_dataset(val_data, val_labels)\n",
    "test_input_sorted, test_labels_sorted = prepare_dataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.3113838317990303, Val Loss: 16.78943199912707, Val Acc: 0.6021505376344086\n",
      "Epoch: 2, Train Loss: 0.24305299772747926, Val Loss: 16.4078667362531, Val Acc: 0.6021505376344086\n",
      "Epoch: 3, Train Loss: 0.24721264392137526, Val Loss: 15.950941542784372, Val Acc: 0.6021505376344086\n",
      "Epoch: 4, Train Loss: 0.23657709362251417, Val Loss: 16.037045538425446, Val Acc: 0.6021505376344086\n",
      "Epoch: 5, Train Loss: 0.22389136058943612, Val Loss: 15.669473230838776, Val Acc: 0.6021505376344086\n",
      "Epoch: 6, Train Loss: 0.2340073949098587, Val Loss: 15.634151130914688, Val Acc: 0.6021505376344086\n",
      "Epoch: 7, Train Loss: 0.2211507862167699, Val Loss: 15.57765218615532, Val Acc: 0.6021505376344086\n",
      "Epoch: 8, Train Loss: 0.22253327969993864, Val Loss: 15.556291123231253, Val Acc: 0.6021505376344086\n",
      "Epoch: 9, Train Loss: 0.21939667727266038, Val Loss: 15.607308447360992, Val Acc: 0.6021505376344086\n",
      "Epoch: 10, Train Loss: 0.22063241362571717, Val Loss: 15.784707417090734, Val Acc: 0.6021505376344086\n",
      "Epoch: 11, Train Loss: 0.2229548661836556, Val Loss: 15.693618913491568, Val Acc: 0.6021505376344086\n",
      "Epoch: 12, Train Loss: 0.21755188150065286, Val Loss: 15.453877061605453, Val Acc: 0.6021505376344086\n",
      "Epoch: 13, Train Loss: 0.22397739269903727, Val Loss: 15.568012416362762, Val Acc: 0.6021505376344086\n",
      "Epoch: 14, Train Loss: 0.2251525515317917, Val Loss: 15.551967551310858, Val Acc: 0.6021505376344086\n",
      "Epoch: 15, Train Loss: 0.21642087208373206, Val Loss: 15.612149288256964, Val Acc: 0.6021505376344086\n",
      "Epoch: 16, Train Loss: 0.21797365512166705, Val Loss: 15.548900862534841, Val Acc: 0.6021505376344086\n",
      "Epoch: 17, Train Loss: 0.22005591831036977, Val Loss: 15.447581777969996, Val Acc: 0.6021505376344086\n",
      "Epoch: 18, Train Loss: 0.22615152116332735, Val Loss: 15.51613430182139, Val Acc: 0.6021505376344086\n",
      "Epoch: 19, Train Loss: 0.2187285892026765, Val Loss: 15.564254621664682, Val Acc: 0.6021505376344086\n",
      "Epoch: 20, Train Loss: 0.21914701747042792, Val Loss: 15.66496408979098, Val Acc: 0.6021505376344086\n",
      "Epoch: 21, Train Loss: 0.2214862903526851, Val Loss: 15.526630143324534, Val Acc: 0.6021505376344086\n",
      "Epoch: 22, Train Loss: 0.2170728454419545, Val Loss: 15.516670197248459, Val Acc: 0.6021505376344086\n",
      "Epoch: 23, Train Loss: 0.21579279839992524, Val Loss: 15.491316348314285, Val Acc: 0.6021505376344086\n",
      "Epoch: 24, Train Loss: 0.21559329773698535, Val Loss: 15.571852455536524, Val Acc: 0.6021505376344086\n",
      "Epoch: 25, Train Loss: 0.22015158925737655, Val Loss: 15.45841763416926, Val Acc: 0.6021505376344086\n",
      "Epoch: 26, Train Loss: 0.213787288580622, Val Loss: 15.548491030931473, Val Acc: 0.5698924731182796\n",
      "Epoch: 27, Train Loss: 0.21882513919046948, Val Loss: 15.57688644528389, Val Acc: 0.6021505376344086\n",
      "Epoch: 28, Train Loss: 0.21718921661376953, Val Loss: 15.578129380941391, Val Acc: 0.5698924731182796\n",
      "Epoch: 29, Train Loss: 0.21517610839435033, Val Loss: 15.54301110903422, Val Acc: 0.6021505376344086\n",
      "Epoch: 30, Train Loss: 0.21530297338962556, Val Loss: 15.599231789509455, Val Acc: 0.5698924731182796\n",
      "Epoch: 31, Train Loss: 0.21530495371137345, Val Loss: 15.52574231227239, Val Acc: 0.5698924731182796\n",
      "Epoch: 32, Train Loss: 0.21805792919227054, Val Loss: 15.572876622279486, Val Acc: 0.5698924731182796\n",
      "Epoch: 33, Train Loss: 0.21307102637631553, Val Loss: 15.511174768209457, Val Acc: 0.5698924731182796\n",
      "Epoch: 34, Train Loss: 0.21999495663813182, Val Loss: 15.435704946517944, Val Acc: 0.6021505376344086\n",
      "Epoch: 35, Train Loss: 0.2132550836460931, Val Loss: 15.537142227093378, Val Acc: 0.5698924731182796\n",
      "Epoch: 36, Train Loss: 0.21973687337977546, Val Loss: 15.51569601893425, Val Acc: 0.6021505376344086\n",
      "Epoch: 37, Train Loss: 0.2157951536348888, Val Loss: 15.570855557918549, Val Acc: 0.5698924731182796\n",
      "Epoch: 38, Train Loss: 0.2123316793356623, Val Loss: 15.453888446092606, Val Acc: 0.6021505376344086\n",
      "Epoch: 39, Train Loss: 0.21790113747119905, Val Loss: 15.452823142210642, Val Acc: 0.5698924731182796\n",
      "Epoch: 40, Train Loss: 0.21715276326451982, Val Loss: 15.4633915523688, Val Acc: 0.6021505376344086\n",
      "Epoch: 41, Train Loss: 0.21599393108061382, Val Loss: 15.409965107838312, Val Acc: 0.6021505376344086\n",
      "Epoch: 42, Train Loss: 0.21267451916422161, Val Loss: 15.448579440514246, Val Acc: 0.6021505376344086\n",
      "Epoch: 43, Train Loss: 0.2143330294745309, Val Loss: 15.465798914432526, Val Acc: 0.6021505376344086\n",
      "Epoch: 44, Train Loss: 0.21631497293710708, Val Loss: 15.53004073103269, Val Acc: 0.5698924731182796\n",
      "Epoch: 45, Train Loss: 0.21267876033272062, Val Loss: 15.476279546817144, Val Acc: 0.5698924731182796\n",
      "Epoch: 46, Train Loss: 0.21545476479189737, Val Loss: 15.481924027204514, Val Acc: 0.5698924731182796\n",
      "Epoch: 47, Train Loss: 0.21223795239414486, Val Loss: 15.527612417936325, Val Acc: 0.5698924731182796\n",
      "Epoch: 48, Train Loss: 0.21406585999897548, Val Loss: 15.478172580401102, Val Acc: 0.5698924731182796\n",
      "Epoch: 49, Train Loss: 0.2140408570425851, Val Loss: 15.412895600001017, Val Acc: 0.6021505376344086\n",
      "Epoch: 50, Train Loss: 0.21262202961104257, Val Loss: 15.437739501396814, Val Acc: 0.5698924731182796\n",
      "Test Loss: 12.699817180633545, Test Acc: 0.546448087431694\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "sequence_length = 10\n",
    "geom_vector_len = 7  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "num_classes = 10  # Number of output classes\n",
    "batch_size = 32\n",
    "dataset_size = batch_size*50\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    conv_model.train()\n",
    "    train_loss = 0.0\n",
    "    for seq_len in train_input_sorted:\n",
    "        inputs = torch.tensor(train_input_sorted[seq_len], dtype=torch.float32)\n",
    "        labels = torch.tensor(train_labels_sorted[seq_len], dtype=torch.long)\n",
    "        dataset = TensorDataset(inputs, labels)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = conv_model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    conv_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_len in val_input_sorted:\n",
    "            inputs = torch.tensor(val_input_sorted[seq_len], dtype=torch.float32)\n",
    "            labels = torch.tensor(val_labels_sorted[seq_len], dtype=torch.long)\n",
    "            dataset = TensorDataset(inputs, labels)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            for batch_x, batch_y in loader:\n",
    "                outputs = conv_model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / val_labels.shape[0]\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss/train_data.shape[0]}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "# Test\n",
    "conv_model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for seq_len in test_input_sorted:\n",
    "        inputs = torch.tensor(test_input_sorted[seq_len], dtype=torch.float32)\n",
    "        labels = torch.tensor(test_labels_sorted[seq_len], dtype=torch.long)\n",
    "        dataset = TensorDataset(inputs, labels)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch_x, batch_y in loader:\n",
    "            outputs = conv_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = correct / test_labels.shape[0]\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the Conv model: 16266\n",
      "Total number of parameters in the Transformer model: 8938\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters\n",
    "total_params_conv_model = sum(p.numel() for p in conv_model.parameters())\n",
    "print(f\"Total number of parameters in the Conv model: {total_params_conv_model}\")\n",
    "\n",
    "total_params_pot_model = sum(p.numel() for p in pot.parameters())\n",
    "print(f\"Total number of parameters in the Transformer model: {total_params_pot_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
