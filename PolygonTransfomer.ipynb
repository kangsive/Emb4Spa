{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Same MLP applied to all token(position) representations\"\"\"\n",
    "    def __init__(self, emb_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(math.log(10000.0) / emb_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0, \"Embedding dimension must be divided by number of heads\"\n",
    "\n",
    "        # Dimensions initialization\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        # all features are divided into multi head, each head have a part of features\n",
    "        self.head_emb_dim = self.emb_dim // self.num_heads\n",
    "\n",
    "        # Transformation matrixs\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_emb_dim)\n",
    "\n",
    "        # Mask scores (where positions are 0) with near negative inf\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply sofxmax to attention scores\n",
    "        attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Get the final output\n",
    "        output = torch.matmul(attn_scores, V)\n",
    "        return output\n",
    "    \n",
    "    def split(self, x):\n",
    "        # Reshape the input emb_dim (to multi-head, each head owns a part of input features) for multi-head attention\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        # transpose to fix batch_size and num_heads, let seq_len, head_emb_dim participate in matrix multiplication\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_emb_dim).transpose(1, 2)\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size, num_heads, seq_len, head_emb_dim = x.size()\n",
    "        # contiguous() ensures the memory layout of the tensor is contiguous\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.emb_dim)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Split input to multi heads\n",
    "        Q = self.split(self.W_q(Q))\n",
    "        K = self.split(self.W_k(K))\n",
    "        V = self.split(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine outputs and apply transformation\n",
    "        output = self.W_o(self.combine(attn_output))\n",
    "        return output\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_atten = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ffn = PositionWiseFFN(emb_dim, ffn_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_atten(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolygonEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads,\n",
    "                num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(emb_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, emb_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # token_mask = (tokens != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        cls_mask = torch.ones((batch_size, 1, 1, 1), dtype=torch.bool)\n",
    "        mask = torch.cat((cls_mask, mask), dim=3)\n",
    "        \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            x = enc_layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PolygonTransformer(nn.Module):\n",
    "    def __init__(self, num_types, emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = PolygonEncoder(emb_dim, num_heads, num_layers, ffn_dim, max_seq_len, dropout)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(emb_dim, ffn_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(ffn_dim, num_types))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.encoder(x, mask)\n",
    "        x = x[:, 0, :] # grab the class embedding\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# wkts = df['WKT'][:1000].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(df, dataset_size, max_seq_len): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    wkts = df['WKT'][:dataset_size]\n",
    "    types = df['type'][:dataset_size]\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices < start_points\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "max_seq_len = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels, mask = prepare_polygon_dataset(df, dataset_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(tokens, mask, labels)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.06556907102832325\n",
      "Epoch: 2, Loss: 0.04951107915349188\n",
      "Epoch: 3, Loss: 0.04458023964425328\n",
      "Epoch: 4, Loss: 0.04344310496477473\n",
      "Epoch: 5, Loss: 0.04278175996187283\n",
      "Epoch: 6, Loss: 0.04269852227545958\n",
      "Epoch: 7, Loss: 0.04256832739650803\n",
      "Epoch: 8, Loss: 0.04215204495711614\n",
      "Epoch: 9, Loss: 0.042163623526059006\n",
      "Epoch: 10, Loss: 0.04180720868526689\n",
      "Epoch: 11, Loss: 0.04193045148913492\n",
      "Epoch: 12, Loss: 0.04193815628954228\n",
      "Epoch: 13, Loss: 0.042039057285727\n",
      "Epoch: 14, Loss: 0.04204640692512461\n",
      "Epoch: 15, Loss: 0.04220989026479273\n",
      "Epoch: 16, Loss: 0.0421892884593682\n",
      "Epoch: 17, Loss: 0.0422674178810461\n",
      "Epoch: 18, Loss: 0.0417299139953033\n",
      "Epoch: 19, Loss: 0.041834482743969434\n",
      "Epoch: 20, Loss: 0.04229494035910707\n"
     ]
    }
   ],
   "source": [
    "pot = PolygonTransformer(num_types=10,\n",
    "                        emb_dim=7,\n",
    "                        num_heads=1,\n",
    "                        num_layers=6,\n",
    "                        ffn_dim=64, \n",
    "                        max_seq_len=64,\n",
    "                        dropout=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "pot.train()\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for batch_x, batch_mask, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = pot(batch_x, batch_mask)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {running_loss/tokens.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conv\n",
    "##### refer to https://arxiv.org/pdf/1806.03857.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, dataset_size):\n",
    "\n",
    "    wkts = df['WKT'][:dataset_size]\n",
    "    train_geoms = [gv.vectorize_wkt(wkt) for wkt in wkts]\n",
    "    train_labels = df['type'][:dataset_size]\n",
    "    \n",
    "    zipped = zip(train_geoms, train_labels)\n",
    "    train_input_sorted = {}\n",
    "    train_labels_sorted = {}\n",
    "\n",
    "    for geom, label in sorted(zipped, key=lambda x: len(x[0]), reverse=True):\n",
    "        seq_len = geom.shape[0]\n",
    "        if seq_len in train_input_sorted:\n",
    "            train_input_sorted[seq_len].append(geom)\n",
    "            train_labels_sorted[seq_len].append(label)\n",
    "        else:\n",
    "            train_input_sorted[seq_len] = [geom]\n",
    "            train_labels_sorted[seq_len] = [label]\n",
    "    \n",
    "    return train_input_sorted, train_labels_sorted, len(train_geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_sorted, train_labels_sorted, num_samples = prepare_dataset(df, dataset_size=dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2890478185862303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m output \u001b[39m=\u001b[39m conv_model(batch_x)\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, batch_y)\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[39m# Print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "sequence_length = 10\n",
    "geom_vector_len = 7  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "num_classes = 10  # Number of output classes\n",
    "batch_size = 32\n",
    "dataset_size = batch_size*50\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for seq_len in train_input_sorted:\n",
    "        inputs = torch.tensor(train_input_sorted[seq_len], dtype=torch.float32)\n",
    "        labels = torch.tensor(train_labels_sorted[seq_len], dtype=torch.long)\n",
    "        dataset = TensorDataset(inputs, labels)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = conv_model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/train_geoms.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
