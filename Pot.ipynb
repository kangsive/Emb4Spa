{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer using Pytorch Transformer Encoder Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mask is different here, only (batch_size, seq_len) mask where True stands for invalid (mask) attention queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.prepare_dataset import prepare_dataset, prepare_dataset_fixedsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pot(nn.Module):\n",
    "    def __init__(self, fea_dim=7, d_model=30, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(dim_feedforward, num_types))\n",
    "        self.projection = nn.Linear(fea_dim, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None, pre_train=False):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        x = self.projection(x)\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.projection(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.zeros((x.size(0), 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        if not pre_train:\n",
    "            x = x[:, 0, :] # grab the class embedding\n",
    "            x = self.mlp_head(x)\n",
    "        else:\n",
    "            x = x[:, 1:, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, dense_size, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(input_size, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(dense_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "geom_train, geom_test, label_train, label_test, gs = prepare_dataset_fixedsize(dataset_size=2000)\n",
    "\n",
    "train_tokens = torch.tensor(geom_train, dtype=torch.float32)\n",
    "test_tokens = torch.tensor(geom_test, dtype=torch.float32)\n",
    "train_labels= torch.tensor(label_train, dtype=torch.long)\n",
    "test_labels = torch.tensor(label_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(test_tokens, test_labels), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.139152002334595, Train Acc 0.13875, Val Loss: 2.1169678483690535, Val Acc: 0.145\n",
      "Epoch: 2, Train Loss: 2.097270174026489, Train Acc 0.154375, Val Loss: 2.0723682131086076, Val Acc: 0.1525\n",
      "Epoch: 3, Train Loss: 2.0457476806640624, Train Acc 0.18125, Val Loss: 2.0220488820757185, Val Acc: 0.1975\n",
      "Epoch: 4, Train Loss: 1.9825956678390504, Train Acc 0.213125, Val Loss: 1.9853921617780412, Val Acc: 0.195\n",
      "Epoch: 5, Train Loss: 1.9659615755081177, Train Acc 0.20875, Val Loss: 1.9311999593462263, Val Acc: 0.2275\n",
      "Epoch: 6, Train Loss: 1.9479502248764038, Train Acc 0.240625, Val Loss: 1.8938300439289637, Val Acc: 0.275\n",
      "Epoch: 7, Train Loss: 1.9116272449493408, Train Acc 0.24125, Val Loss: 1.885062643459865, Val Acc: 0.305\n",
      "Epoch: 8, Train Loss: 1.8945185375213622, Train Acc 0.255625, Val Loss: 1.9072721004486084, Val Acc: 0.265\n",
      "Epoch: 9, Train Loss: 1.90451171875, Train Acc 0.275625, Val Loss: 1.8680494172232491, Val Acc: 0.27\n",
      "Epoch: 10, Train Loss: 1.8773003435134887, Train Acc 0.283125, Val Loss: 1.870361362184797, Val Acc: 0.2775\n",
      "Epoch: 11, Train Loss: 1.8654389238357545, Train Acc 0.28625, Val Loss: 1.875125425202506, Val Acc: 0.3025\n",
      "Epoch: 12, Train Loss: 1.8607380342483522, Train Acc 0.2775, Val Loss: 1.884733864239284, Val Acc: 0.2925\n",
      "Epoch: 13, Train Loss: 1.8488388681411743, Train Acc 0.303125, Val Loss: 1.8515631641660417, Val Acc: 0.2875\n",
      "Epoch: 14, Train Loss: 1.8552076482772828, Train Acc 0.28875, Val Loss: 1.8490461962563651, Val Acc: 0.315\n",
      "Epoch: 15, Train Loss: 1.8364896535873414, Train Acc 0.29125, Val Loss: 1.8623710870742798, Val Acc: 0.3025\n",
      "Epoch: 16, Train Loss: 1.832700595855713, Train Acc 0.28875, Val Loss: 1.8442773818969727, Val Acc: 0.345\n",
      "Epoch: 17, Train Loss: 1.8411869859695436, Train Acc 0.299375, Val Loss: 1.8435136250087194, Val Acc: 0.33\n",
      "Epoch: 18, Train Loss: 1.8260341358184815, Train Acc 0.305, Val Loss: 1.827055743762425, Val Acc: 0.31\n",
      "Epoch: 19, Train Loss: 1.8205789518356323, Train Acc 0.305, Val Loss: 1.8523557697023665, Val Acc: 0.31\n",
      "Epoch: 20, Train Loss: 1.816336340904236, Train Acc 0.3125, Val Loss: 1.8527616603033883, Val Acc: 0.3175\n",
      "Epoch: 21, Train Loss: 1.8230456590652466, Train Acc 0.299375, Val Loss: 1.8416282279150826, Val Acc: 0.335\n",
      "Epoch: 22, Train Loss: 1.8129509639739991, Train Acc 0.315625, Val Loss: 1.8461143118994576, Val Acc: 0.32\n",
      "Epoch: 23, Train Loss: 1.8078702402114868, Train Acc 0.295625, Val Loss: 1.8327954837254115, Val Acc: 0.3425\n",
      "Epoch: 24, Train Loss: 1.812639489173889, Train Acc 0.319375, Val Loss: 1.8483595166887556, Val Acc: 0.2925\n",
      "Epoch: 25, Train Loss: 1.8088822841644288, Train Acc 0.3225, Val Loss: 1.8378136157989502, Val Acc: 0.345\n",
      "Epoch: 26, Train Loss: 1.7926507425308227, Train Acc 0.30375, Val Loss: 1.8351840121405465, Val Acc: 0.325\n",
      "Epoch: 27, Train Loss: 1.7878174591064453, Train Acc 0.309375, Val Loss: 1.8344717706952776, Val Acc: 0.31\n",
      "Epoch: 28, Train Loss: 1.7855802011489867, Train Acc 0.323125, Val Loss: 1.8408644029072352, Val Acc: 0.31\n",
      "Epoch: 29, Train Loss: 1.789639058113098, Train Acc 0.3125, Val Loss: 1.8600538287843977, Val Acc: 0.315\n",
      "Epoch: 30, Train Loss: 1.78082172870636, Train Acc 0.31125, Val Loss: 1.8599687303815569, Val Acc: 0.32\n",
      "Epoch: 31, Train Loss: 1.7802225923538209, Train Acc 0.315625, Val Loss: 1.8236358165740967, Val Acc: 0.33\n",
      "Epoch: 32, Train Loss: 1.7744349193573, Train Acc 0.3225, Val Loss: 1.8184901816504342, Val Acc: 0.34\n",
      "Epoch: 33, Train Loss: 1.7640516471862793, Train Acc 0.326875, Val Loss: 1.8450519016810827, Val Acc: 0.3225\n",
      "Epoch: 34, Train Loss: 1.7422109460830688, Train Acc 0.351875, Val Loss: 1.826982021331787, Val Acc: 0.3275\n",
      "Epoch: 35, Train Loss: 1.7496183681488038, Train Acc 0.3325, Val Loss: 1.8376151663916451, Val Acc: 0.335\n",
      "Epoch: 36, Train Loss: 1.7517811489105224, Train Acc 0.32625, Val Loss: 1.8264199325016566, Val Acc: 0.37\n",
      "Epoch: 37, Train Loss: 1.7549162673950196, Train Acc 0.33375, Val Loss: 1.821758202144078, Val Acc: 0.34\n",
      "Epoch: 38, Train Loss: 1.7398726272583007, Train Acc 0.3325, Val Loss: 1.825314896447318, Val Acc: 0.345\n",
      "Epoch: 39, Train Loss: 1.7391770124435424, Train Acc 0.33625, Val Loss: 1.858700156211853, Val Acc: 0.315\n",
      "Epoch: 40, Train Loss: 1.7510526990890503, Train Acc 0.35125, Val Loss: 1.853012238230024, Val Acc: 0.3575\n",
      "Epoch: 41, Train Loss: 1.7287484073638917, Train Acc 0.34375, Val Loss: 1.8195528302873885, Val Acc: 0.3375\n",
      "Epoch: 42, Train Loss: 1.7347065210342407, Train Acc 0.34, Val Loss: 1.8168864761080061, Val Acc: 0.3475\n",
      "Epoch: 43, Train Loss: 1.710680651664734, Train Acc 0.369375, Val Loss: 1.846412079674857, Val Acc: 0.355\n",
      "Epoch: 44, Train Loss: 1.7103471994400024, Train Acc 0.3575, Val Loss: 1.8055717604500907, Val Acc: 0.3625\n",
      "Epoch: 45, Train Loss: 1.7262145280838013, Train Acc 0.34875, Val Loss: 1.8234097106116158, Val Acc: 0.32\n",
      "Epoch: 46, Train Loss: 1.7320922374725343, Train Acc 0.345625, Val Loss: 1.8346675634384155, Val Acc: 0.355\n",
      "Epoch: 47, Train Loss: 1.7051528072357178, Train Acc 0.35875, Val Loss: 1.8309372493198939, Val Acc: 0.3475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m eval_loss, eval_acc\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 53\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(pot, train_loader)\n\u001b[1;32m     54\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m evaluate(pot, val_loader)\n\u001b[1;32m     55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device), batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m pot(batch_x)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mPot.forward\u001b[0;34m(self, x, mask, pre_train)\u001b[0m\n\u001b[1;32m     31\u001b[0m         cls_mask \u001b[39m=\u001b[39m cls_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cls_mask, mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x, src_key_padding_mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pre_train:\n\u001b[1;32m     37\u001b[0m     x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m, :] \u001b[39m# grab the class embedding\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:538\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    539\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/transformer.py:546\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    545\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 546\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    547\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    548\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    549\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/functional.py:5163\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5161\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   5162\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m-> 5163\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m dropout(attn_output_weights, p\u001b[39m=\u001b[39;49mdropout_p)\n\u001b[1;32m   5165\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(attn_output_weights, v)\n\u001b[1;32m   5167\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pot = Pot(fea_dim=5, d_model=32, nhead=8, num_layers=1, max_seq_len=64, dim_feedforward=32, dropout=0.1, num_types=9)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=-1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = pot(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=-1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# # Test\n",
    "# test_loss, test_acc = evaluate(pot, test_loader)\n",
    "# print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from Prepare_dataset import prepare_dataset, prepare_dataset_fixedsize\n",
    "\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.119906492233276, Train Acc 0.155, Val Loss: 2.061765159879412, Val Acc: 0.17\n",
      "Epoch: 2, Train Loss: 2.0140987825393677, Train Acc 0.19375, Val Loss: 2.0145457812717984, Val Acc: 0.2125\n",
      "Epoch: 3, Train Loss: 1.9833253288269044, Train Acc 0.216875, Val Loss: 1.9651267869131905, Val Acc: 0.22\n",
      "Epoch: 4, Train Loss: 1.9151890802383422, Train Acc 0.251875, Val Loss: 1.8949920109340124, Val Acc: 0.295\n",
      "Epoch: 5, Train Loss: 1.8478848123550415, Train Acc 0.29375, Val Loss: 1.8531781945909773, Val Acc: 0.2775\n",
      "Epoch: 6, Train Loss: 1.8190205478668213, Train Acc 0.30125, Val Loss: 1.8255900655473982, Val Acc: 0.3075\n",
      "Epoch: 7, Train Loss: 1.7901782369613648, Train Acc 0.309375, Val Loss: 1.812549012047904, Val Acc: 0.2925\n",
      "Epoch: 8, Train Loss: 1.777712230682373, Train Acc 0.305625, Val Loss: 1.812856844493321, Val Acc: 0.3275\n",
      "Epoch: 9, Train Loss: 1.7661158514022828, Train Acc 0.32, Val Loss: 1.804777809551784, Val Acc: 0.315\n",
      "Epoch: 10, Train Loss: 1.762407693862915, Train Acc 0.336875, Val Loss: 1.803687674658639, Val Acc: 0.315\n",
      "Epoch: 11, Train Loss: 1.7696305513381958, Train Acc 0.32625, Val Loss: 1.7889130456107003, Val Acc: 0.3325\n",
      "Epoch: 12, Train Loss: 1.7500522136688232, Train Acc 0.34125, Val Loss: 1.7961162158421107, Val Acc: 0.3325\n",
      "Epoch: 13, Train Loss: 1.7461614894866944, Train Acc 0.338125, Val Loss: 1.787306819643293, Val Acc: 0.33\n",
      "Epoch: 14, Train Loss: 1.7294236946105956, Train Acc 0.344375, Val Loss: 1.7928871086665563, Val Acc: 0.3075\n",
      "Epoch: 15, Train Loss: 1.7408391952514648, Train Acc 0.3325, Val Loss: 1.7835285663604736, Val Acc: 0.32\n",
      "Epoch: 16, Train Loss: 1.7381075239181518, Train Acc 0.345625, Val Loss: 1.810035126549857, Val Acc: 0.3075\n",
      "Epoch: 17, Train Loss: 1.7343422174453735, Train Acc 0.338125, Val Loss: 1.79069641658238, Val Acc: 0.3525\n",
      "Epoch: 18, Train Loss: 1.719166841506958, Train Acc 0.34625, Val Loss: 1.7984281608036585, Val Acc: 0.335\n",
      "Epoch: 19, Train Loss: 1.7209592008590697, Train Acc 0.355, Val Loss: 1.8076933792659216, Val Acc: 0.3125\n",
      "Epoch: 20, Train Loss: 1.716201400756836, Train Acc 0.3575, Val Loss: 1.7968897649220057, Val Acc: 0.3275\n",
      "Epoch: 21, Train Loss: 1.7221292304992675, Train Acc 0.349375, Val Loss: 1.8049916880471366, Val Acc: 0.3025\n",
      "Epoch: 22, Train Loss: 1.7187168407440185, Train Acc 0.35125, Val Loss: 1.7932315383638655, Val Acc: 0.3175\n",
      "Epoch: 23, Train Loss: 1.7184208250045776, Train Acc 0.355, Val Loss: 1.8014299699238367, Val Acc: 0.3275\n",
      "Epoch: 24, Train Loss: 1.701332311630249, Train Acc 0.360625, Val Loss: 1.8017399651663644, Val Acc: 0.3375\n",
      "Epoch: 25, Train Loss: 1.705961561203003, Train Acc 0.34, Val Loss: 1.791755097252982, Val Acc: 0.325\n",
      "Epoch: 26, Train Loss: 1.7064097261428832, Train Acc 0.35125, Val Loss: 1.8027369124548775, Val Acc: 0.3075\n",
      "Epoch: 27, Train Loss: 1.714683084487915, Train Acc 0.339375, Val Loss: 1.8077900069100517, Val Acc: 0.33\n",
      "Epoch: 28, Train Loss: 1.697605905532837, Train Acc 0.3625, Val Loss: 1.830485497202192, Val Acc: 0.3075\n",
      "Epoch: 29, Train Loss: 1.6945204830169678, Train Acc 0.34625, Val Loss: 1.8056185926709856, Val Acc: 0.32\n",
      "Epoch: 30, Train Loss: 1.700880160331726, Train Acc 0.35875, Val Loss: 1.8231068849563599, Val Acc: 0.33\n",
      "Epoch: 31, Train Loss: 1.6882714462280273, Train Acc 0.350625, Val Loss: 1.798770478793553, Val Acc: 0.3225\n",
      "Epoch: 32, Train Loss: 1.6760113143920898, Train Acc 0.355625, Val Loss: 1.8213491099221366, Val Acc: 0.325\n",
      "Epoch: 33, Train Loss: 1.6740338468551637, Train Acc 0.363125, Val Loss: 1.8061330829347884, Val Acc: 0.3175\n",
      "Epoch: 34, Train Loss: 1.6668562650680543, Train Acc 0.36375, Val Loss: 1.7999786308833532, Val Acc: 0.3325\n",
      "Epoch: 35, Train Loss: 1.672137794494629, Train Acc 0.37, Val Loss: 1.8119891711643763, Val Acc: 0.305\n",
      "Epoch: 36, Train Loss: 1.6755460119247436, Train Acc 0.358125, Val Loss: 1.8217225415366036, Val Acc: 0.31\n",
      "Epoch: 37, Train Loss: 1.6858319187164306, Train Acc 0.353125, Val Loss: 1.8025634629385812, Val Acc: 0.32\n",
      "Epoch: 38, Train Loss: 1.694615020751953, Train Acc 0.33625, Val Loss: 1.811018398829869, Val Acc: 0.35\n",
      "Epoch: 39, Train Loss: 1.6644336080551148, Train Acc 0.349375, Val Loss: 1.821344154221671, Val Acc: 0.335\n",
      "Epoch: 40, Train Loss: 1.6556010770797729, Train Acc 0.35875, Val Loss: 1.8155197926930018, Val Acc: 0.33\n",
      "Epoch: 41, Train Loss: 1.6567739009857179, Train Acc 0.371875, Val Loss: 1.8422671897070748, Val Acc: 0.3125\n",
      "Epoch: 42, Train Loss: 1.6484446716308594, Train Acc 0.3575, Val Loss: 1.8060364723205566, Val Acc: 0.3475\n",
      "Epoch: 43, Train Loss: 1.642384638786316, Train Acc 0.369375, Val Loss: 1.8354115997041975, Val Acc: 0.325\n",
      "Epoch: 44, Train Loss: 1.6491976118087768, Train Acc 0.366875, Val Loss: 1.8245159557887487, Val Acc: 0.3275\n",
      "Epoch: 45, Train Loss: 1.6281280422210693, Train Acc 0.379375, Val Loss: 1.8340701035090856, Val Acc: 0.345\n",
      "Epoch: 46, Train Loss: 1.6380049753189088, Train Acc 0.36125, Val Loss: 1.8184526477541243, Val Acc: 0.33\n",
      "Epoch: 47, Train Loss: 1.628494005203247, Train Acc 0.375625, Val Loss: 1.8182971477508545, Val Acc: 0.335\n",
      "Epoch: 48, Train Loss: 1.6319650888442994, Train Acc 0.37625, Val Loss: 1.8243125336510795, Val Acc: 0.34\n",
      "Epoch: 49, Train Loss: 1.6278951978683471, Train Acc 0.3725, Val Loss: 1.83357948916299, Val Acc: 0.3175\n",
      "Epoch: 50, Train Loss: 1.6141138696670532, Train Acc 0.39125, Val Loss: 1.8228493758610316, Val Acc: 0.315\n",
      "Epoch: 51, Train Loss: 1.6111514902114867, Train Acc 0.3725, Val Loss: 1.8418137516294206, Val Acc: 0.3225\n",
      "Epoch: 52, Train Loss: 1.621566801071167, Train Acc 0.3725, Val Loss: 1.8877676725387573, Val Acc: 0.3175\n",
      "Epoch: 53, Train Loss: 1.6153607702255248, Train Acc 0.3825, Val Loss: 1.8356438875198364, Val Acc: 0.345\n",
      "Epoch: 54, Train Loss: 1.6072073125839232, Train Acc 0.39625, Val Loss: 1.8366833754948206, Val Acc: 0.3275\n",
      "Epoch: 55, Train Loss: 1.604950737953186, Train Acc 0.378125, Val Loss: 1.83327739579337, Val Acc: 0.3375\n",
      "Epoch: 56, Train Loss: 1.5867819786071777, Train Acc 0.40375, Val Loss: 1.8625092165810722, Val Acc: 0.3025\n",
      "Epoch: 57, Train Loss: 1.6051167774200439, Train Acc 0.38, Val Loss: 1.847491911479405, Val Acc: 0.32\n",
      "Epoch: 58, Train Loss: 1.5954607677459718, Train Acc 0.3775, Val Loss: 1.8576455797467912, Val Acc: 0.33\n",
      "Epoch: 59, Train Loss: 1.6086929082870483, Train Acc 0.391875, Val Loss: 1.865668569292341, Val Acc: 0.3225\n",
      "Epoch: 60, Train Loss: 1.5719822788238524, Train Acc 0.4, Val Loss: 1.8464330094201225, Val Acc: 0.3325\n",
      "Epoch: 61, Train Loss: 1.5697145128250123, Train Acc 0.398125, Val Loss: 1.8546933616910661, Val Acc: 0.3325\n",
      "Epoch: 62, Train Loss: 1.568846344947815, Train Acc 0.395625, Val Loss: 1.8458055768694197, Val Acc: 0.3375\n",
      "Epoch: 63, Train Loss: 1.5883191585540772, Train Acc 0.394375, Val Loss: 1.8662962913513184, Val Acc: 0.32\n",
      "Epoch: 64, Train Loss: 1.6096083307266236, Train Acc 0.375625, Val Loss: 1.8417022739137923, Val Acc: 0.325\n",
      "Epoch: 65, Train Loss: 1.5691089153289794, Train Acc 0.395625, Val Loss: 1.865050537245614, Val Acc: 0.35\n",
      "Epoch: 66, Train Loss: 1.5584905910491944, Train Acc 0.399375, Val Loss: 1.847341435296195, Val Acc: 0.34\n",
      "Epoch: 67, Train Loss: 1.5505989599227905, Train Acc 0.41, Val Loss: 1.9077758278165544, Val Acc: 0.3125\n",
      "Epoch: 68, Train Loss: 1.5603742361068726, Train Acc 0.395, Val Loss: 1.8866456747055054, Val Acc: 0.3225\n",
      "Epoch: 69, Train Loss: 1.553200626373291, Train Acc 0.395625, Val Loss: 1.8894719226019723, Val Acc: 0.345\n",
      "Epoch: 70, Train Loss: 1.538050389289856, Train Acc 0.416875, Val Loss: 1.8745214257921492, Val Acc: 0.3475\n",
      "Epoch: 71, Train Loss: 1.523108458518982, Train Acc 0.410625, Val Loss: 1.8757731914520264, Val Acc: 0.3325\n",
      "Epoch: 72, Train Loss: 1.5222320699691771, Train Acc 0.4125, Val Loss: 1.8623012474605016, Val Acc: 0.3675\n",
      "Epoch: 73, Train Loss: 1.507431812286377, Train Acc 0.414375, Val Loss: 1.8976550272532873, Val Acc: 0.345\n",
      "Epoch: 74, Train Loss: 1.51818678855896, Train Acc 0.41625, Val Loss: 1.9263575077056885, Val Acc: 0.32\n",
      "Epoch: 75, Train Loss: 1.537883849143982, Train Acc 0.4025, Val Loss: 1.89960059097835, Val Acc: 0.345\n",
      "Epoch: 76, Train Loss: 1.498829689025879, Train Acc 0.438125, Val Loss: 1.9367758546556746, Val Acc: 0.3375\n",
      "Epoch: 77, Train Loss: 1.4966041374206542, Train Acc 0.4275, Val Loss: 1.8934445892061507, Val Acc: 0.3325\n",
      "Epoch: 78, Train Loss: 1.4780748558044434, Train Acc 0.430625, Val Loss: 1.9407757350376673, Val Acc: 0.3475\n",
      "Epoch: 79, Train Loss: 1.484393835067749, Train Acc 0.4325, Val Loss: 1.9327791248049055, Val Acc: 0.34\n",
      "Epoch: 80, Train Loss: 1.4721199464797974, Train Acc 0.436875, Val Loss: 1.9214438540594918, Val Acc: 0.3525\n",
      "Epoch: 81, Train Loss: 1.4773002767562866, Train Acc 0.433125, Val Loss: 1.9342561960220337, Val Acc: 0.36\n",
      "Epoch: 82, Train Loss: 1.4963847970962525, Train Acc 0.420625, Val Loss: 1.946272611618042, Val Acc: 0.335\n",
      "Epoch: 83, Train Loss: 1.4929794073104858, Train Acc 0.414375, Val Loss: 1.9662410531725203, Val Acc: 0.3175\n",
      "Epoch: 84, Train Loss: 1.4707391357421875, Train Acc 0.423125, Val Loss: 1.9759618554796492, Val Acc: 0.32\n",
      "Epoch: 85, Train Loss: 1.4688658475875855, Train Acc 0.4425, Val Loss: 1.9807691744395666, Val Acc: 0.35\n",
      "Epoch: 86, Train Loss: 1.4413102436065675, Train Acc 0.44625, Val Loss: 1.9255445173808508, Val Acc: 0.345\n",
      "Epoch: 87, Train Loss: 1.4569522476196288, Train Acc 0.44375, Val Loss: 1.9545599903379167, Val Acc: 0.34\n",
      "Epoch: 88, Train Loss: 1.4435249328613282, Train Acc 0.4375, Val Loss: 1.9297549213681902, Val Acc: 0.3325\n",
      "Epoch: 89, Train Loss: 1.4250241708755493, Train Acc 0.458125, Val Loss: 1.9232008968080794, Val Acc: 0.345\n",
      "Epoch: 90, Train Loss: 1.4450724411010742, Train Acc 0.44875, Val Loss: 1.9182041031973702, Val Acc: 0.36\n",
      "Epoch: 91, Train Loss: 1.4233023977279664, Train Acc 0.448125, Val Loss: 1.9636537347521101, Val Acc: 0.34\n",
      "Epoch: 92, Train Loss: 1.4138582706451417, Train Acc 0.455625, Val Loss: 1.9736096688679285, Val Acc: 0.345\n",
      "Epoch: 93, Train Loss: 1.416982078552246, Train Acc 0.45625, Val Loss: 1.9946167809622628, Val Acc: 0.3275\n",
      "Epoch: 94, Train Loss: 1.3940260601043701, Train Acc 0.475, Val Loss: 1.9809555666787284, Val Acc: 0.3525\n",
      "Epoch: 95, Train Loss: 1.3906395149230957, Train Acc 0.466875, Val Loss: 1.9978417328425817, Val Acc: 0.33\n",
      "Epoch: 96, Train Loss: 1.413085708618164, Train Acc 0.461875, Val Loss: 1.987433774130685, Val Acc: 0.3525\n",
      "Epoch: 97, Train Loss: 1.4014862394332885, Train Acc 0.465625, Val Loss: 1.9811332396098547, Val Acc: 0.3425\n",
      "Epoch: 98, Train Loss: 1.3803485059738159, Train Acc 0.474375, Val Loss: 1.9950919491904122, Val Acc: 0.345\n",
      "Epoch: 99, Train Loss: 1.3731744861602784, Train Acc 0.48375, Val Loss: 1.9955341986247472, Val Acc: 0.335\n",
      "Epoch: 100, Train Loss: 1.3593107652664185, Train Acc 0.484375, Val Loss: 2.0074517897197177, Val Acc: 0.355\n",
      "Epoch: 101, Train Loss: 1.3552646350860595, Train Acc 0.478125, Val Loss: 2.0103437219347273, Val Acc: 0.355\n",
      "Epoch: 102, Train Loss: 1.3439683818817139, Train Acc 0.491875, Val Loss: 2.001662186213902, Val Acc: 0.35\n",
      "Epoch: 103, Train Loss: 1.3542058086395263, Train Acc 0.489375, Val Loss: 2.025134410176958, Val Acc: 0.3375\n",
      "Epoch: 104, Train Loss: 1.331652240753174, Train Acc 0.49125, Val Loss: 2.0113420486450195, Val Acc: 0.35\n",
      "Epoch: 105, Train Loss: 1.3220891952514648, Train Acc 0.500625, Val Loss: 2.101533089365278, Val Acc: 0.325\n",
      "Epoch: 106, Train Loss: 1.3170721817016602, Train Acc 0.5, Val Loss: 2.0781809261866977, Val Acc: 0.3175\n",
      "Epoch: 107, Train Loss: 1.3232659912109375, Train Acc 0.48, Val Loss: 2.013419985771179, Val Acc: 0.32\n",
      "Epoch: 108, Train Loss: 1.3136119079589843, Train Acc 0.5125, Val Loss: 2.0826527902058194, Val Acc: 0.3425\n",
      "Epoch: 109, Train Loss: 1.2903092765808106, Train Acc 0.505625, Val Loss: 2.0582823242459978, Val Acc: 0.3225\n",
      "Epoch: 110, Train Loss: 1.2992386960983275, Train Acc 0.511875, Val Loss: 2.0713049513953075, Val Acc: 0.3375\n",
      "Epoch: 111, Train Loss: 1.275800495147705, Train Acc 0.508125, Val Loss: 2.1699721132005965, Val Acc: 0.3425\n",
      "Epoch: 112, Train Loss: 1.2977298879623413, Train Acc 0.5, Val Loss: 2.076117753982544, Val Acc: 0.3425\n",
      "Epoch: 113, Train Loss: 1.2785095357894898, Train Acc 0.506875, Val Loss: 2.0472352845328197, Val Acc: 0.375\n",
      "Epoch: 114, Train Loss: 1.2673231840133667, Train Acc 0.501875, Val Loss: 2.0658929688589915, Val Acc: 0.3375\n",
      "Epoch: 115, Train Loss: 1.2743540239334106, Train Acc 0.51875, Val Loss: 2.138748458453587, Val Acc: 0.3625\n",
      "Epoch: 116, Train Loss: 1.2378849840164186, Train Acc 0.531875, Val Loss: 2.1011667251586914, Val Acc: 0.3425\n",
      "Epoch: 117, Train Loss: 1.246349844932556, Train Acc 0.524375, Val Loss: 2.1762728691101074, Val Acc: 0.325\n",
      "Epoch: 118, Train Loss: 1.2421838188171386, Train Acc 0.518125, Val Loss: 2.2546916348593578, Val Acc: 0.3225\n",
      "Epoch: 119, Train Loss: 1.2363308620452882, Train Acc 0.54375, Val Loss: 2.1626791783741544, Val Acc: 0.3325\n",
      "Epoch: 120, Train Loss: 1.2291051959991455, Train Acc 0.531875, Val Loss: 2.2226082938058034, Val Acc: 0.345\n",
      "Epoch: 121, Train Loss: 1.2284199404716492, Train Acc 0.53125, Val Loss: 2.2098006861550465, Val Acc: 0.3325\n",
      "Epoch: 122, Train Loss: 1.2122200775146483, Train Acc 0.538125, Val Loss: 2.2465443270547047, Val Acc: 0.32\n",
      "Epoch: 123, Train Loss: 1.2041317558288573, Train Acc 0.53875, Val Loss: 2.166244779314314, Val Acc: 0.33\n",
      "Epoch: 124, Train Loss: 1.197840895652771, Train Acc 0.54625, Val Loss: 2.1928744656699046, Val Acc: 0.3275\n",
      "Epoch: 125, Train Loss: 1.1947670364379883, Train Acc 0.53625, Val Loss: 2.1732812779290334, Val Acc: 0.315\n",
      "Epoch: 126, Train Loss: 1.1605648612976074, Train Acc 0.55, Val Loss: 2.191875542913164, Val Acc: 0.32\n",
      "Epoch: 127, Train Loss: 1.158774631023407, Train Acc 0.568125, Val Loss: 2.238332850592477, Val Acc: 0.3325\n",
      "Epoch: 128, Train Loss: 1.1439380025863648, Train Acc 0.55375, Val Loss: 2.2291017941066196, Val Acc: 0.3325\n",
      "Epoch: 129, Train Loss: 1.1544486999511718, Train Acc 0.551875, Val Loss: 2.289569514138358, Val Acc: 0.3175\n",
      "Epoch: 130, Train Loss: 1.2069540953636169, Train Acc 0.529375, Val Loss: 2.274015051977975, Val Acc: 0.325\n",
      "Epoch: 131, Train Loss: 1.1922222471237183, Train Acc 0.54375, Val Loss: 2.3546875544956754, Val Acc: 0.3425\n",
      "Epoch: 132, Train Loss: 1.1362779211997986, Train Acc 0.57625, Val Loss: 2.292885627065386, Val Acc: 0.3475\n",
      "Epoch: 133, Train Loss: 1.1422084474563599, Train Acc 0.575625, Val Loss: 2.2795410667146956, Val Acc: 0.3325\n",
      "Epoch: 134, Train Loss: 1.1220668029785157, Train Acc 0.579375, Val Loss: 2.330502510070801, Val Acc: 0.3325\n",
      "Epoch: 135, Train Loss: 1.0965522170066833, Train Acc 0.60625, Val Loss: 2.294250692640032, Val Acc: 0.325\n",
      "Epoch: 136, Train Loss: 1.094277629852295, Train Acc 0.585, Val Loss: 2.324599027633667, Val Acc: 0.33\n",
      "Epoch: 137, Train Loss: 1.1043561291694641, Train Acc 0.581875, Val Loss: 2.394176040376936, Val Acc: 0.3175\n",
      "Epoch: 138, Train Loss: 1.1123010063171386, Train Acc 0.581875, Val Loss: 2.434251035962786, Val Acc: 0.3375\n",
      "Epoch: 139, Train Loss: 1.1138011455535888, Train Acc 0.57375, Val Loss: 2.4014039720807756, Val Acc: 0.345\n",
      "Epoch: 140, Train Loss: 1.069270122051239, Train Acc 0.586875, Val Loss: 2.501546553203038, Val Acc: 0.3425\n",
      "Epoch: 141, Train Loss: 1.081724421977997, Train Acc 0.586875, Val Loss: 2.4443729604993547, Val Acc: 0.3425\n",
      "Epoch: 142, Train Loss: 1.0965258502960205, Train Acc 0.5875, Val Loss: 2.4580733435494557, Val Acc: 0.3225\n",
      "Epoch: 143, Train Loss: 1.0623430013656616, Train Acc 0.58125, Val Loss: 2.419274227959769, Val Acc: 0.32\n",
      "Epoch: 144, Train Loss: 1.036582453250885, Train Acc 0.60875, Val Loss: 2.467900378363473, Val Acc: 0.3325\n",
      "Epoch: 145, Train Loss: 1.0396877312660218, Train Acc 0.610625, Val Loss: 2.5007707391466414, Val Acc: 0.3325\n",
      "Epoch: 146, Train Loss: 1.0202846312522889, Train Acc 0.594375, Val Loss: 2.459503105708531, Val Acc: 0.315\n",
      "Epoch: 147, Train Loss: 1.0777717590332032, Train Acc 0.5875, Val Loss: 2.496502774102347, Val Acc: 0.3075\n",
      "Epoch: 148, Train Loss: 1.008258168697357, Train Acc 0.613125, Val Loss: 2.5379297052110945, Val Acc: 0.3475\n",
      "Epoch: 149, Train Loss: 1.0163878965377808, Train Acc 0.61625, Val Loss: 2.5713352135249545, Val Acc: 0.3225\n",
      "Epoch: 150, Train Loss: 1.0510250043869018, Train Acc 0.604375, Val Loss: 2.6036896194730486, Val Acc: 0.33\n",
      "Epoch: 151, Train Loss: 0.9811180353164672, Train Acc 0.63, Val Loss: 2.638754027230399, Val Acc: 0.3325\n",
      "Epoch: 152, Train Loss: 0.9815618062019348, Train Acc 0.628125, Val Loss: 2.608019999095372, Val Acc: 0.335\n",
      "Epoch: 153, Train Loss: 0.9831165671348572, Train Acc 0.6225, Val Loss: 2.6804043224879672, Val Acc: 0.335\n",
      "Epoch: 154, Train Loss: 0.9975226616859436, Train Acc 0.62375, Val Loss: 2.7026851177215576, Val Acc: 0.32\n",
      "Epoch: 155, Train Loss: 0.9650195145606995, Train Acc 0.625, Val Loss: 2.7448477915355136, Val Acc: 0.29\n",
      "Epoch: 156, Train Loss: 0.971378526687622, Train Acc 0.624375, Val Loss: 2.685809714453561, Val Acc: 0.3325\n",
      "Epoch: 157, Train Loss: 0.9610795593261718, Train Acc 0.644375, Val Loss: 2.73010322025844, Val Acc: 0.35\n",
      "Epoch: 158, Train Loss: 0.9346641802787781, Train Acc 0.65375, Val Loss: 2.6023766653878346, Val Acc: 0.3225\n",
      "Epoch: 159, Train Loss: 0.9495897150039673, Train Acc 0.650625, Val Loss: 2.8136204992021834, Val Acc: 0.325\n",
      "Epoch: 160, Train Loss: 0.9292273402214051, Train Acc 0.649375, Val Loss: 2.7284864357539584, Val Acc: 0.325\n",
      "Epoch: 161, Train Loss: 0.9528676986694335, Train Acc 0.644375, Val Loss: 2.807709046772548, Val Acc: 0.3\n",
      "Epoch: 162, Train Loss: 0.9310603046417236, Train Acc 0.64375, Val Loss: 2.948585101536342, Val Acc: 0.3275\n",
      "Epoch: 163, Train Loss: 0.9383071136474609, Train Acc 0.64125, Val Loss: 2.6787048407963345, Val Acc: 0.3225\n",
      "Epoch: 164, Train Loss: 0.9879714345932007, Train Acc 0.61625, Val Loss: 2.9233769348689487, Val Acc: 0.3125\n",
      "Epoch: 165, Train Loss: 0.9145695686340332, Train Acc 0.641875, Val Loss: 2.761612892150879, Val Acc: 0.3225\n",
      "Epoch: 166, Train Loss: 0.9073143744468689, Train Acc 0.661875, Val Loss: 2.8267097813742503, Val Acc: 0.3325\n",
      "Epoch: 167, Train Loss: 0.9101099324226379, Train Acc 0.6575, Val Loss: 2.8709167752947127, Val Acc: 0.32\n",
      "Epoch: 168, Train Loss: 0.9074640464782715, Train Acc 0.65375, Val Loss: 3.00112407548087, Val Acc: 0.365\n",
      "Epoch: 169, Train Loss: 0.8814617538452149, Train Acc 0.67125, Val Loss: 2.977416958127703, Val Acc: 0.315\n",
      "Epoch: 170, Train Loss: 0.8831850957870483, Train Acc 0.666875, Val Loss: 2.914325407573155, Val Acc: 0.285\n",
      "Epoch: 171, Train Loss: 0.8814110875129699, Train Acc 0.67625, Val Loss: 2.89983115877424, Val Acc: 0.315\n",
      "Epoch: 172, Train Loss: 0.8447089219093322, Train Acc 0.668125, Val Loss: 3.10701516696385, Val Acc: 0.325\n",
      "Epoch: 173, Train Loss: 0.8621770405769348, Train Acc 0.671875, Val Loss: 2.977231127875192, Val Acc: 0.3125\n",
      "Epoch: 174, Train Loss: 0.8610365128517151, Train Acc 0.67375, Val Loss: 3.070308651242937, Val Acc: 0.31\n",
      "Epoch: 175, Train Loss: 0.839776029586792, Train Acc 0.684375, Val Loss: 2.9890237195151195, Val Acc: 0.3075\n",
      "Epoch: 176, Train Loss: 0.8625179672241211, Train Acc 0.67375, Val Loss: 3.0780112402779713, Val Acc: 0.3225\n",
      "Epoch: 177, Train Loss: 0.8617454552650452, Train Acc 0.679375, Val Loss: 3.1298882961273193, Val Acc: 0.3175\n",
      "Epoch: 178, Train Loss: 0.8375525450706482, Train Acc 0.688125, Val Loss: 3.0070970399039134, Val Acc: 0.325\n",
      "Epoch: 179, Train Loss: 0.8495671033859253, Train Acc 0.68375, Val Loss: 3.0002464226314, Val Acc: 0.3425\n",
      "Epoch: 180, Train Loss: 0.8612034749984742, Train Acc 0.67875, Val Loss: 3.119259834289551, Val Acc: 0.33\n",
      "Epoch: 181, Train Loss: 0.8408399629592895, Train Acc 0.6825, Val Loss: 3.0745940889631, Val Acc: 0.3325\n",
      "Epoch: 182, Train Loss: 0.8481977677345276, Train Acc 0.67375, Val Loss: 3.212259428841727, Val Acc: 0.3075\n",
      "Epoch: 183, Train Loss: 0.8581214427947998, Train Acc 0.669375, Val Loss: 3.2824104172842845, Val Acc: 0.3375\n",
      "Epoch: 184, Train Loss: 0.8218478751182556, Train Acc 0.6875, Val Loss: 3.070528677531651, Val Acc: 0.315\n",
      "Epoch: 185, Train Loss: 0.8235616827011109, Train Acc 0.68875, Val Loss: 3.212151186806815, Val Acc: 0.3125\n",
      "Epoch: 186, Train Loss: 0.8265586757659912, Train Acc 0.678125, Val Loss: 3.2709131922040666, Val Acc: 0.3325\n",
      "Epoch: 187, Train Loss: 0.8086760997772217, Train Acc 0.689375, Val Loss: 3.2684507029397145, Val Acc: 0.27\n",
      "Epoch: 188, Train Loss: 0.82909343957901, Train Acc 0.684375, Val Loss: 3.328273432595389, Val Acc: 0.3\n",
      "Epoch: 189, Train Loss: 0.8052791714668274, Train Acc 0.706875, Val Loss: 3.2146285602024625, Val Acc: 0.32\n",
      "Epoch: 190, Train Loss: 0.7881894326210022, Train Acc 0.70625, Val Loss: 3.387337854930333, Val Acc: 0.3175\n",
      "Epoch: 191, Train Loss: 0.7608939576148986, Train Acc 0.70875, Val Loss: 3.3179960591452464, Val Acc: 0.2875\n",
      "Epoch: 192, Train Loss: 0.7569652009010315, Train Acc 0.70625, Val Loss: 3.349172149385725, Val Acc: 0.325\n",
      "Epoch: 193, Train Loss: 0.7439084839820862, Train Acc 0.7225, Val Loss: 3.5065221105303084, Val Acc: 0.3325\n",
      "Epoch: 194, Train Loss: 0.7393169093132019, Train Acc 0.71125, Val Loss: 3.5120977333613803, Val Acc: 0.3\n",
      "Epoch: 195, Train Loss: 0.7785225057601929, Train Acc 0.7075, Val Loss: 3.3512958799089705, Val Acc: 0.305\n",
      "Epoch: 196, Train Loss: 0.7736708617210388, Train Acc 0.698125, Val Loss: 3.454322099685669, Val Acc: 0.2925\n",
      "Epoch: 197, Train Loss: 0.7573333382606506, Train Acc 0.714375, Val Loss: 3.546414818082537, Val Acc: 0.3225\n",
      "Epoch: 198, Train Loss: 0.757087984085083, Train Acc 0.71375, Val Loss: 3.554709162030901, Val Acc: 0.31\n",
      "Epoch: 199, Train Loss: 0.7353987622261048, Train Acc 0.7275, Val Loss: 3.3694897379193987, Val Acc: 0.295\n",
      "Epoch: 200, Train Loss: 0.7273931789398194, Train Acc 0.71375, Val Loss: 3.7579020091465543, Val Acc: 0.2925\n",
      "Epoch: 201, Train Loss: 0.742784321308136, Train Acc 0.70875, Val Loss: 3.519709382738386, Val Acc: 0.305\n",
      "Epoch: 202, Train Loss: 0.7280270624160766, Train Acc 0.728125, Val Loss: 3.7727323940822055, Val Acc: 0.315\n",
      "Epoch: 203, Train Loss: 0.7147479820251464, Train Acc 0.729375, Val Loss: 3.7825916835239957, Val Acc: 0.33\n",
      "Epoch: 204, Train Loss: 0.7043018555641174, Train Acc 0.736875, Val Loss: 3.540484666824341, Val Acc: 0.28\n",
      "Epoch: 205, Train Loss: 0.7155208373069764, Train Acc 0.733125, Val Loss: 3.758978469031198, Val Acc: 0.3175\n",
      "Epoch: 206, Train Loss: 0.695860025882721, Train Acc 0.729375, Val Loss: 3.716580901827131, Val Acc: 0.3\n",
      "Epoch: 207, Train Loss: 0.7065369701385498, Train Acc 0.739375, Val Loss: 3.8376262528555736, Val Acc: 0.3275\n",
      "Epoch: 208, Train Loss: 0.6720329666137695, Train Acc 0.7525, Val Loss: 3.6465433665684293, Val Acc: 0.3025\n",
      "Epoch: 209, Train Loss: 0.6691870307922363, Train Acc 0.75, Val Loss: 3.8841467925480435, Val Acc: 0.3225\n",
      "Epoch: 210, Train Loss: 0.6761859703063965, Train Acc 0.745, Val Loss: 3.891198226383754, Val Acc: 0.3375\n",
      "Epoch: 211, Train Loss: 0.6808926033973693, Train Acc 0.74, Val Loss: 3.9340223244258334, Val Acc: 0.305\n",
      "Epoch: 212, Train Loss: 0.7100389218330383, Train Acc 0.72625, Val Loss: 3.935352529798235, Val Acc: 0.3125\n",
      "Epoch: 213, Train Loss: 0.6830311727523803, Train Acc 0.73375, Val Loss: 4.047675030572074, Val Acc: 0.305\n",
      "Epoch: 214, Train Loss: 0.6860185289382934, Train Acc 0.744375, Val Loss: 3.800194944654192, Val Acc: 0.3275\n",
      "Epoch: 215, Train Loss: 0.6854897117614747, Train Acc 0.744375, Val Loss: 3.962408883231027, Val Acc: 0.3\n",
      "Epoch: 216, Train Loss: 0.6723606562614441, Train Acc 0.745625, Val Loss: 3.9906083856310164, Val Acc: 0.3125\n",
      "Epoch: 217, Train Loss: 0.6775544989109039, Train Acc 0.73625, Val Loss: 4.004897287913731, Val Acc: 0.295\n",
      "Epoch: 218, Train Loss: 0.6734573090076447, Train Acc 0.753125, Val Loss: 4.0862358297620505, Val Acc: 0.3225\n",
      "Epoch: 219, Train Loss: 0.6741077387332917, Train Acc 0.740625, Val Loss: 3.9995933260236467, Val Acc: 0.3175\n",
      "Epoch: 220, Train Loss: 0.6878727126121521, Train Acc 0.74625, Val Loss: 3.9052603585379466, Val Acc: 0.285\n",
      "Epoch: 221, Train Loss: 0.7122707772254944, Train Acc 0.73375, Val Loss: 4.271600144250052, Val Acc: 0.295\n",
      "Epoch: 222, Train Loss: 0.6770284676551819, Train Acc 0.73875, Val Loss: 4.40225168636867, Val Acc: 0.31\n",
      "Epoch: 223, Train Loss: 0.6646172547340393, Train Acc 0.75125, Val Loss: 4.224421330860683, Val Acc: 0.2975\n",
      "Epoch: 224, Train Loss: 0.6605693709850311, Train Acc 0.755625, Val Loss: 4.093380859919956, Val Acc: 0.31\n",
      "Epoch: 225, Train Loss: 0.6356270515918732, Train Acc 0.765, Val Loss: 4.156797988074167, Val Acc: 0.29\n",
      "Epoch: 226, Train Loss: 0.6324949538707734, Train Acc 0.76375, Val Loss: 3.944448028291975, Val Acc: 0.3\n",
      "Epoch: 227, Train Loss: 0.6382607209682465, Train Acc 0.765625, Val Loss: 4.2010233742850165, Val Acc: 0.3025\n",
      "Epoch: 228, Train Loss: 0.6164910757541656, Train Acc 0.76875, Val Loss: 4.101120097296579, Val Acc: 0.2825\n",
      "Epoch: 229, Train Loss: 0.6376065051555634, Train Acc 0.75875, Val Loss: 4.2241659845624655, Val Acc: 0.3275\n",
      "Epoch: 230, Train Loss: 0.6564247298240662, Train Acc 0.74875, Val Loss: 4.081632001059396, Val Acc: 0.2725\n",
      "Epoch: 231, Train Loss: 0.6098451673984527, Train Acc 0.7725, Val Loss: 4.397303989955357, Val Acc: 0.2975\n",
      "Epoch: 232, Train Loss: 0.5936410915851593, Train Acc 0.77, Val Loss: 4.834755999701364, Val Acc: 0.2975\n",
      "Epoch: 233, Train Loss: 0.6151592004299163, Train Acc 0.77875, Val Loss: 4.362663166863578, Val Acc: 0.3175\n",
      "Epoch: 234, Train Loss: 0.586235089302063, Train Acc 0.78375, Val Loss: 4.847789662224906, Val Acc: 0.305\n",
      "Epoch: 235, Train Loss: 0.6517774558067322, Train Acc 0.75375, Val Loss: 4.23485381262643, Val Acc: 0.2925\n",
      "Epoch: 236, Train Loss: 0.586832971572876, Train Acc 0.7825, Val Loss: 4.694136381149292, Val Acc: 0.2925\n",
      "Epoch: 237, Train Loss: 0.5934295535087586, Train Acc 0.773125, Val Loss: 4.426635367529733, Val Acc: 0.295\n",
      "Epoch: 238, Train Loss: 0.6141288471221924, Train Acc 0.783125, Val Loss: 4.347724437713623, Val Acc: 0.29\n",
      "Epoch: 239, Train Loss: 0.6054577684402466, Train Acc 0.76125, Val Loss: 4.205739464078631, Val Acc: 0.2975\n",
      "Epoch: 240, Train Loss: 0.6228083682060241, Train Acc 0.76, Val Loss: 4.555132797786167, Val Acc: 0.295\n",
      "Epoch: 241, Train Loss: 0.5701075851917267, Train Acc 0.785625, Val Loss: 4.468768358230591, Val Acc: 0.29\n",
      "Epoch: 242, Train Loss: 0.5592043042182923, Train Acc 0.78625, Val Loss: 4.5757061413356235, Val Acc: 0.335\n",
      "Epoch: 243, Train Loss: 0.6260018944740295, Train Acc 0.771875, Val Loss: 4.27177814074925, Val Acc: 0.285\n",
      "Epoch: 244, Train Loss: 0.6120602476596833, Train Acc 0.76625, Val Loss: 4.538437230246408, Val Acc: 0.3075\n",
      "Epoch: 245, Train Loss: 0.5563322985172272, Train Acc 0.7925, Val Loss: 4.700302192143032, Val Acc: 0.3125\n",
      "Epoch: 246, Train Loss: 0.5761920928955078, Train Acc 0.781875, Val Loss: 4.681267295564924, Val Acc: 0.295\n",
      "Epoch: 247, Train Loss: 0.5726889729499817, Train Acc 0.786875, Val Loss: 4.413014786584037, Val Acc: 0.2725\n",
      "Epoch: 248, Train Loss: 0.5615187692642212, Train Acc 0.795625, Val Loss: 4.793378693716867, Val Acc: 0.275\n",
      "Epoch: 249, Train Loss: 0.5507388317584991, Train Acc 0.804375, Val Loss: 4.69551226070949, Val Acc: 0.275\n",
      "Epoch: 250, Train Loss: 0.5592454075813293, Train Acc 0.799375, Val Loss: 4.4974972520555765, Val Acc: 0.305\n",
      "Epoch: 251, Train Loss: 0.5594274151325226, Train Acc 0.7875, Val Loss: 4.737031187329974, Val Acc: 0.2975\n",
      "Epoch: 252, Train Loss: 0.562515572309494, Train Acc 0.795, Val Loss: 4.8885979652404785, Val Acc: 0.325\n",
      "Epoch: 253, Train Loss: 0.5586392962932587, Train Acc 0.78875, Val Loss: 4.822514057159424, Val Acc: 0.2875\n",
      "Epoch: 254, Train Loss: 0.5493251609802247, Train Acc 0.799375, Val Loss: 4.722034863063267, Val Acc: 0.32\n",
      "Epoch: 255, Train Loss: 0.5412177264690399, Train Acc 0.806875, Val Loss: 4.660501071384975, Val Acc: 0.3075\n",
      "Epoch: 256, Train Loss: 0.5388430750370026, Train Acc 0.8, Val Loss: 4.780239071164813, Val Acc: 0.3175\n",
      "Epoch: 257, Train Loss: 0.5539608156681061, Train Acc 0.79875, Val Loss: 4.903293779918125, Val Acc: 0.2925\n",
      "Epoch: 258, Train Loss: 0.5520983600616455, Train Acc 0.79375, Val Loss: 4.73748551096235, Val Acc: 0.2975\n",
      "Epoch: 259, Train Loss: 0.5405873715877533, Train Acc 0.795625, Val Loss: 5.007436582020351, Val Acc: 0.31\n",
      "Epoch: 260, Train Loss: 0.5439170861244201, Train Acc 0.7875, Val Loss: 4.900644166128976, Val Acc: 0.3025\n",
      "Epoch: 261, Train Loss: 0.5161906027793884, Train Acc 0.81625, Val Loss: 4.984197820935931, Val Acc: 0.2975\n",
      "Epoch: 262, Train Loss: 0.5279792404174805, Train Acc 0.800625, Val Loss: 4.646086522511074, Val Acc: 0.27\n",
      "Epoch: 263, Train Loss: 0.5268981349468231, Train Acc 0.803125, Val Loss: 5.0622694832938055, Val Acc: 0.295\n",
      "Epoch: 264, Train Loss: 0.5027953600883484, Train Acc 0.81125, Val Loss: 5.047624860491071, Val Acc: 0.28\n",
      "Epoch: 265, Train Loss: 0.5257425725460052, Train Acc 0.80875, Val Loss: 5.218954154423305, Val Acc: 0.3075\n",
      "Epoch: 266, Train Loss: 0.49698354959487917, Train Acc 0.82125, Val Loss: 5.196921484810965, Val Acc: 0.285\n",
      "Epoch: 267, Train Loss: 0.5056746470928192, Train Acc 0.814375, Val Loss: 5.228437389646258, Val Acc: 0.2875\n",
      "Epoch: 268, Train Loss: 0.5167206168174744, Train Acc 0.809375, Val Loss: 5.410257305417742, Val Acc: 0.325\n",
      "Epoch: 269, Train Loss: 0.4914927053451538, Train Acc 0.816875, Val Loss: 4.985228981290545, Val Acc: 0.29\n",
      "Epoch: 270, Train Loss: 0.4791738176345825, Train Acc 0.82125, Val Loss: 5.361458097185407, Val Acc: 0.31\n",
      "Epoch: 271, Train Loss: 0.49248759627342226, Train Acc 0.82, Val Loss: 5.443985632487705, Val Acc: 0.32\n",
      "Epoch: 272, Train Loss: 0.567704906463623, Train Acc 0.795, Val Loss: 5.256947313036237, Val Acc: 0.3\n",
      "Epoch: 273, Train Loss: 0.55558389544487, Train Acc 0.794375, Val Loss: 5.6150312423706055, Val Acc: 0.31\n",
      "Epoch: 274, Train Loss: 0.5444531309604644, Train Acc 0.795625, Val Loss: 5.352945906775338, Val Acc: 0.275\n",
      "Epoch: 275, Train Loss: 0.4794273281097412, Train Acc 0.820625, Val Loss: 5.55496655191694, Val Acc: 0.295\n",
      "Epoch: 276, Train Loss: 0.4697119116783142, Train Acc 0.82375, Val Loss: 5.764328071049282, Val Acc: 0.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m eval_loss, eval_acc\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 57\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(conv_model, train_loader)\n\u001b[1;32m     58\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m evaluate(conv_model, val_loader)\n\u001b[1;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     23\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device), batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     27\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mCompareModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     31\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_avgpool(x)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/envs/nlp_a4/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "geom_vector_len = 5  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "num_classes = 9  # Number of output classes\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.001)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 300\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(conv_model, train_loader)\n",
    "    val_loss, val_acc = evaluate(conv_model, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(pot.state_dict(), 'pot_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it as feature extractor (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "pot.load_state_dict(torch.load(\"pot_model.pth\"))\n",
    "pot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_hidden = pot(train_tokens, train_mask, pre_train=True).view(train_tokens.size(0), -1)\n",
    "    val_hidden = pot(val_tokens, val_mask, pre_train=True).view(val_tokens.size(0), -1)\n",
    "    test_hidden = pot(test_tokens, test_mask, pre_train=True).view(test_tokens.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_hidden, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_hidden, val_labels))\n",
    "test_loader = DataLoader(TensorDataset(test_hidden, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.9963074207305909, Train Acc 0.50625, Val Loss: 1.30093060284853, Val Acc: 0.62\n",
      "Epoch: 2, Train Loss: 1.2693264412879943, Train Acc 0.5925, Val Loss: 1.0721332759410143, Val Acc: 0.68\n",
      "Epoch: 3, Train Loss: 1.166937198638916, Train Acc 0.605, Val Loss: 1.0162230451405048, Val Acc: 0.7\n",
      "Epoch: 4, Train Loss: 1.1719644212722777, Train Acc 0.59125, Val Loss: 1.1128932654857635, Val Acc: 0.71\n",
      "Epoch: 5, Train Loss: 1.166082215309143, Train Acc 0.6225, Val Loss: 1.0086301210522652, Val Acc: 0.72\n",
      "Epoch: 6, Train Loss: 1.1365370631217957, Train Acc 0.6125, Val Loss: 1.1069583275914192, Val Acc: 0.66\n",
      "Epoch: 7, Train Loss: 1.178542513847351, Train Acc 0.60125, Val Loss: 1.2399723929166795, Val Acc: 0.59\n",
      "Epoch: 8, Train Loss: 1.1612643957138062, Train Acc 0.59125, Val Loss: 1.0868311071395873, Val Acc: 0.69\n",
      "Epoch: 9, Train Loss: 1.1284274244308472, Train Acc 0.60875, Val Loss: 1.0686833444237709, Val Acc: 0.66\n",
      "Epoch: 10, Train Loss: 1.1235071158409118, Train Acc 0.61125, Val Loss: 1.0323871737718582, Val Acc: 0.7\n",
      "Epoch: 11, Train Loss: 1.1324586868286133, Train Acc 0.61625, Val Loss: 1.0419702281057834, Val Acc: 0.69\n",
      "Epoch: 12, Train Loss: 1.1420544934272767, Train Acc 0.60625, Val Loss: 1.0410168534517288, Val Acc: 0.72\n",
      "Epoch: 13, Train Loss: 1.1209018397331239, Train Acc 0.63, Val Loss: 1.0741409802436828, Val Acc: 0.7\n",
      "Epoch: 14, Train Loss: 1.112939703464508, Train Acc 0.61375, Val Loss: 1.0233982527256011, Val Acc: 0.71\n",
      "Epoch: 15, Train Loss: 1.1448683261871337, Train Acc 0.60625, Val Loss: 1.0169035314023496, Val Acc: 0.71\n",
      "Epoch: 16, Train Loss: 1.1269723868370056, Train Acc 0.61375, Val Loss: 1.0167317280173302, Val Acc: 0.69\n",
      "Epoch: 17, Train Loss: 1.1318548393249512, Train Acc 0.60625, Val Loss: 1.1504305997490882, Val Acc: 0.66\n",
      "Epoch: 18, Train Loss: 1.1318707847595215, Train Acc 0.60375, Val Loss: 1.0020979772508145, Val Acc: 0.71\n",
      "Epoch: 19, Train Loss: 1.111134376525879, Train Acc 0.62125, Val Loss: 1.1472789430618286, Val Acc: 0.62\n",
      "Epoch: 20, Train Loss: 1.1136483645439148, Train Acc 0.61, Val Loss: 1.040769821703434, Val Acc: 0.72\n",
      "Test Loss: 0.9882621133327484, Test Acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(train_hidden.size(1), 128, 10, 0.5)\n",
    "\n",
    "if USE_GPU:\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, loader):\n",
    "    model.eval()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(classifier, train_loader)\n",
    "    val_loss, val_acc = evaluate(classifier, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(classifier, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
