{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer using Pytorch Transformer Encoder Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mask is different here, only (batch_size, seq_len) mask where True stands for invalid (mask) attention queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from Prepare_dataset import prepare_dataset, prepare_dataset_fixedsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pot(nn.Module):\n",
    "    def __init__(self, d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(dim_feedforward, num_types))\n",
    "\n",
    "    def forward(self, x, mask=None, pre_train=False):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.zeros((x.size(0), 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        if not pre_train:\n",
    "            x = x[:, 0, :] # grab the class embedding\n",
    "            x = self.mlp_head(x)\n",
    "        else:\n",
    "            x = x[:, 1:, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, dense_size, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(input_size, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(dense_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "geom_train, geom_test, label_train, label_test = prepare_dataset_fixedsize()\n",
    "\n",
    "train_tokens = torch.tensor(geom_train, dtype=torch.float32)\n",
    "test_tokens = torch.tensor(geom_test, dtype=torch.float32)\n",
    "train_labels= torch.tensor(label_train, dtype=torch.long)\n",
    "test_labels = torch.tensor(label_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(test_tokens, test_labels), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.132174949645996, Train Acc 0.1125, Val Loss: 2.1659020355769565, Val Acc: 0.155\n",
      "Epoch: 2, Train Loss: 2.11169979095459, Train Acc 0.13375, Val Loss: 2.1387602601732527, Val Acc: 0.155\n",
      "Epoch: 3, Train Loss: 2.1025127267837522, Train Acc 0.13875, Val Loss: 2.1521988255637035, Val Acc: 0.155\n",
      "Epoch: 4, Train Loss: 2.1022899961471557, Train Acc 0.155, Val Loss: 2.14817408152989, Val Acc: 0.155\n",
      "Epoch: 5, Train Loss: 2.100671796798706, Train Acc 0.15375, Val Loss: 2.1449833597455705, Val Acc: 0.1\n",
      "Epoch: 6, Train Loss: 2.09905508518219, Train Acc 0.15, Val Loss: 2.1561831406184604, Val Acc: 0.135\n",
      "Epoch: 7, Train Loss: 2.1047661304473877, Train Acc 0.145, Val Loss: 2.1525798865727017, Val Acc: 0.155\n",
      "Epoch: 8, Train Loss: 2.1030706119537355, Train Acc 0.1375, Val Loss: 2.1513047558920726, Val Acc: 0.135\n",
      "Epoch: 9, Train Loss: 2.0929880237579344, Train Acc 0.13375, Val Loss: 2.1139160905565535, Val Acc: 0.135\n",
      "Epoch: 10, Train Loss: 2.0875554132461547, Train Acc 0.16375, Val Loss: 2.0989482402801514, Val Acc: 0.12\n",
      "Epoch: 11, Train Loss: 2.082908773422241, Train Acc 0.1575, Val Loss: 2.14270555973053, Val Acc: 0.16\n",
      "Epoch: 12, Train Loss: 2.0627278518676757, Train Acc 0.17, Val Loss: 2.051258019038609, Val Acc: 0.19\n",
      "Epoch: 13, Train Loss: 2.038834834098816, Train Acc 0.175, Val Loss: 2.0437207562582835, Val Acc: 0.175\n",
      "Epoch: 14, Train Loss: 2.039505524635315, Train Acc 0.16625, Val Loss: 2.038821220397949, Val Acc: 0.18\n",
      "Epoch: 15, Train Loss: 2.0478087568283083, Train Acc 0.16375, Val Loss: 2.0631434747150967, Val Acc: 0.195\n",
      "Epoch: 16, Train Loss: 2.036503105163574, Train Acc 0.18125, Val Loss: 2.0217013359069824, Val Acc: 0.19\n",
      "Epoch: 17, Train Loss: 2.039282703399658, Train Acc 0.1925, Val Loss: 2.029324701854161, Val Acc: 0.195\n",
      "Epoch: 18, Train Loss: 2.0302671289443968, Train Acc 0.1925, Val Loss: 2.0510179655892506, Val Acc: 0.205\n",
      "Epoch: 19, Train Loss: 2.03662691116333, Train Acc 0.17125, Val Loss: 2.029467667852129, Val Acc: 0.19\n",
      "Epoch: 20, Train Loss: 2.0186912298202513, Train Acc 0.18, Val Loss: 2.0562081847872054, Val Acc: 0.185\n",
      "Epoch: 21, Train Loss: 2.023888068199158, Train Acc 0.19125, Val Loss: 2.04267190183912, Val Acc: 0.205\n",
      "Epoch: 22, Train Loss: 2.0191104507446287, Train Acc 0.19125, Val Loss: 2.039598277636937, Val Acc: 0.195\n",
      "Epoch: 23, Train Loss: 2.017811026573181, Train Acc 0.17375, Val Loss: 2.0518278224127635, Val Acc: 0.215\n",
      "Epoch: 24, Train Loss: 2.0260663270950316, Train Acc 0.1775, Val Loss: 2.048453007425581, Val Acc: 0.195\n",
      "Epoch: 25, Train Loss: 2.0255707406997683, Train Acc 0.205, Val Loss: 2.060535022190639, Val Acc: 0.21\n",
      "Epoch: 26, Train Loss: 2.0117869758605957, Train Acc 0.19, Val Loss: 2.0282846859523227, Val Acc: 0.215\n",
      "Epoch: 27, Train Loss: 2.0097725200653076, Train Acc 0.1925, Val Loss: 2.0314027411597118, Val Acc: 0.185\n",
      "Epoch: 28, Train Loss: 2.004839301109314, Train Acc 0.20625, Val Loss: 2.0367743287767683, Val Acc: 0.185\n",
      "Epoch: 29, Train Loss: 2.0063565969467163, Train Acc 0.20125, Val Loss: 2.030179364340646, Val Acc: 0.185\n",
      "Epoch: 30, Train Loss: 2.021406469345093, Train Acc 0.19125, Val Loss: 2.014273473194667, Val Acc: 0.225\n",
      "Epoch: 31, Train Loss: 2.0113156032562256, Train Acc 0.1975, Val Loss: 2.0178519998277937, Val Acc: 0.205\n",
      "Epoch: 32, Train Loss: 2.0047641468048094, Train Acc 0.1975, Val Loss: 2.0196566581726074, Val Acc: 0.2\n",
      "Epoch: 33, Train Loss: 1.995999493598938, Train Acc 0.19375, Val Loss: 2.0601681641169955, Val Acc: 0.21\n",
      "Epoch: 34, Train Loss: 2.0326380920410156, Train Acc 0.185, Val Loss: 2.0377800294331143, Val Acc: 0.2\n",
      "Epoch: 35, Train Loss: 2.0225618267059327, Train Acc 0.18, Val Loss: 2.1021251508167813, Val Acc: 0.19\n",
      "Epoch: 36, Train Loss: 2.0357754755020143, Train Acc 0.16875, Val Loss: 2.029887454850333, Val Acc: 0.205\n",
      "Epoch: 37, Train Loss: 2.0104332304000856, Train Acc 0.1875, Val Loss: 2.0241532496043613, Val Acc: 0.18\n",
      "Epoch: 38, Train Loss: 2.016957926750183, Train Acc 0.1925, Val Loss: 2.0402498756136214, Val Acc: 0.21\n",
      "Epoch: 39, Train Loss: 2.035691637992859, Train Acc 0.19625, Val Loss: 2.0246784346444264, Val Acc: 0.21\n",
      "Epoch: 40, Train Loss: 2.0045606088638306, Train Acc 0.19625, Val Loss: 2.03682017326355, Val Acc: 0.18\n",
      "Epoch: 41, Train Loss: 2.013354654312134, Train Acc 0.185, Val Loss: 2.0200802087783813, Val Acc: 0.2\n",
      "Epoch: 42, Train Loss: 2.021292414665222, Train Acc 0.17375, Val Loss: 2.0558021068573, Val Acc: 0.21\n",
      "Epoch: 43, Train Loss: 1.9960213947296142, Train Acc 0.21, Val Loss: 2.0152000188827515, Val Acc: 0.195\n",
      "Epoch: 44, Train Loss: 2.017828063964844, Train Acc 0.18875, Val Loss: 2.0350355250494823, Val Acc: 0.185\n",
      "Epoch: 45, Train Loss: 2.0190003299713135, Train Acc 0.18625, Val Loss: 2.029264671461923, Val Acc: 0.18\n",
      "Epoch: 46, Train Loss: 2.013554139137268, Train Acc 0.1875, Val Loss: 2.0282870020185197, Val Acc: 0.19\n",
      "Epoch: 47, Train Loss: 1.9994576168060303, Train Acc 0.19125, Val Loss: 2.0329510143824985, Val Acc: 0.2\n",
      "Epoch: 48, Train Loss: 2.0103907060623167, Train Acc 0.19875, Val Loss: 2.0158769403185164, Val Acc: 0.2\n",
      "Epoch: 49, Train Loss: 2.0067780876159667, Train Acc 0.19875, Val Loss: 2.0194021122796193, Val Acc: 0.215\n",
      "Epoch: 50, Train Loss: 2.014491057395935, Train Acc 0.2025, Val Loss: 2.0236184426716397, Val Acc: 0.22\n",
      "Epoch: 51, Train Loss: 2.0100127029418946, Train Acc 0.1925, Val Loss: 2.0050887891224454, Val Acc: 0.195\n",
      "Epoch: 52, Train Loss: 2.010358362197876, Train Acc 0.20875, Val Loss: 2.024005242756435, Val Acc: 0.22\n",
      "Epoch: 53, Train Loss: 1.9996451711654664, Train Acc 0.215, Val Loss: 1.999974318913051, Val Acc: 0.24\n",
      "Epoch: 54, Train Loss: 2.020046238899231, Train Acc 0.16875, Val Loss: 2.0494355133601596, Val Acc: 0.2\n",
      "Epoch: 55, Train Loss: 2.0095741033554075, Train Acc 0.20625, Val Loss: 2.018269896507263, Val Acc: 0.23\n",
      "Epoch: 56, Train Loss: 2.001529197692871, Train Acc 0.19125, Val Loss: 2.0398863554000854, Val Acc: 0.22\n",
      "Epoch: 57, Train Loss: 2.003281764984131, Train Acc 0.2, Val Loss: 2.048673459461757, Val Acc: 0.215\n",
      "Epoch: 58, Train Loss: 2.0239761066436768, Train Acc 0.19625, Val Loss: 2.0142493418284824, Val Acc: 0.245\n",
      "Epoch: 59, Train Loss: 1.9925967693328857, Train Acc 0.2075, Val Loss: 2.035983988216945, Val Acc: 0.205\n",
      "Epoch: 60, Train Loss: 2.006705527305603, Train Acc 0.2075, Val Loss: 2.0173297269003734, Val Acc: 0.22\n",
      "Epoch: 61, Train Loss: 2.0151864862442017, Train Acc 0.19, Val Loss: 2.014418823378427, Val Acc: 0.21\n",
      "Epoch: 62, Train Loss: 2.006385726928711, Train Acc 0.20375, Val Loss: 2.020147613116673, Val Acc: 0.21\n",
      "Epoch: 63, Train Loss: 2.006112117767334, Train Acc 0.20375, Val Loss: 2.0537267242159163, Val Acc: 0.19\n",
      "Epoch: 64, Train Loss: 2.003745980262756, Train Acc 0.20875, Val Loss: 2.0347612415041243, Val Acc: 0.19\n",
      "Epoch: 65, Train Loss: 1.9909307909011842, Train Acc 0.19, Val Loss: 2.0186049257005965, Val Acc: 0.215\n",
      "Epoch: 66, Train Loss: 1.993080735206604, Train Acc 0.2075, Val Loss: 2.0071261610303606, Val Acc: 0.205\n",
      "Epoch: 67, Train Loss: 1.995843892097473, Train Acc 0.20125, Val Loss: 2.041910869734628, Val Acc: 0.19\n",
      "Epoch: 68, Train Loss: 2.007300386428833, Train Acc 0.19375, Val Loss: 1.9980095284325736, Val Acc: 0.205\n",
      "Epoch: 69, Train Loss: 2.0059400272369383, Train Acc 0.19375, Val Loss: 2.0288193566458568, Val Acc: 0.2\n",
      "Epoch: 70, Train Loss: 2.0010820531845095, Train Acc 0.2025, Val Loss: 2.023823176111494, Val Acc: 0.22\n",
      "Epoch: 71, Train Loss: 2.004849133491516, Train Acc 0.2125, Val Loss: 2.0484561920166016, Val Acc: 0.215\n",
      "Epoch: 72, Train Loss: 2.014022932052612, Train Acc 0.19125, Val Loss: 2.023905481610979, Val Acc: 0.22\n",
      "Epoch: 73, Train Loss: 1.9963245248794557, Train Acc 0.205, Val Loss: 2.0164355039596558, Val Acc: 0.205\n",
      "Epoch: 74, Train Loss: 1.9967933130264282, Train Acc 0.19875, Val Loss: 2.0219450337546214, Val Acc: 0.2\n",
      "Epoch: 75, Train Loss: 2.007194938659668, Train Acc 0.20875, Val Loss: 2.0096906423568726, Val Acc: 0.225\n",
      "Epoch: 76, Train Loss: 2.0014838123321534, Train Acc 0.185, Val Loss: 2.0630887746810913, Val Acc: 0.175\n",
      "Epoch: 77, Train Loss: 2.003638105392456, Train Acc 0.18875, Val Loss: 2.021282843181065, Val Acc: 0.205\n",
      "Epoch: 78, Train Loss: 2.0090259456634523, Train Acc 0.19625, Val Loss: 2.004996657371521, Val Acc: 0.205\n",
      "Epoch: 79, Train Loss: 2.006693754196167, Train Acc 0.19625, Val Loss: 2.0275903940200806, Val Acc: 0.2\n",
      "Epoch: 80, Train Loss: 2.008395581245422, Train Acc 0.21125, Val Loss: 2.029035517147609, Val Acc: 0.23\n",
      "Epoch: 81, Train Loss: 1.9992741298675538, Train Acc 0.19375, Val Loss: 2.0176308836255754, Val Acc: 0.215\n",
      "Epoch: 82, Train Loss: 1.9961571979522705, Train Acc 0.20625, Val Loss: 2.035734159605844, Val Acc: 0.19\n",
      "Epoch: 83, Train Loss: 1.9972579622268676, Train Acc 0.215, Val Loss: 2.002735836165292, Val Acc: 0.22\n",
      "Epoch: 84, Train Loss: 1.9987503147125245, Train Acc 0.205, Val Loss: 2.024887067931039, Val Acc: 0.22\n",
      "Epoch: 85, Train Loss: 1.9946856498718262, Train Acc 0.2125, Val Loss: 2.015090618814741, Val Acc: 0.205\n",
      "Epoch: 86, Train Loss: 1.9938499975204467, Train Acc 0.22625, Val Loss: 2.0183444874627248, Val Acc: 0.21\n",
      "Epoch: 87, Train Loss: 1.998748950958252, Train Acc 0.19875, Val Loss: 2.006111809185573, Val Acc: 0.215\n",
      "Epoch: 88, Train Loss: 1.9917949390411378, Train Acc 0.2325, Val Loss: 1.9969499962670463, Val Acc: 0.225\n",
      "Epoch: 89, Train Loss: 2.0064249801635743, Train Acc 0.195, Val Loss: 2.0103153501238142, Val Acc: 0.205\n",
      "Epoch: 90, Train Loss: 1.984037585258484, Train Acc 0.21, Val Loss: 2.0204964024680003, Val Acc: 0.22\n",
      "Epoch: 91, Train Loss: 1.992146644592285, Train Acc 0.18875, Val Loss: 2.007511190005711, Val Acc: 0.21\n",
      "Epoch: 92, Train Loss: 2.0005373573303222, Train Acc 0.19625, Val Loss: 2.0119259357452393, Val Acc: 0.195\n",
      "Epoch: 93, Train Loss: 2.0000031709671022, Train Acc 0.1775, Val Loss: 2.0111532722200667, Val Acc: 0.22\n",
      "Epoch: 94, Train Loss: 1.9927496004104615, Train Acc 0.20875, Val Loss: 2.001545173781259, Val Acc: 0.24\n",
      "Epoch: 95, Train Loss: 1.9935601949691772, Train Acc 0.20625, Val Loss: 2.0122077805655345, Val Acc: 0.22\n",
      "Epoch: 96, Train Loss: 1.999419059753418, Train Acc 0.19, Val Loss: 2.0280323709760393, Val Acc: 0.185\n",
      "Epoch: 97, Train Loss: 2.001678853034973, Train Acc 0.20625, Val Loss: 2.0024997677121843, Val Acc: 0.235\n",
      "Epoch: 98, Train Loss: 1.9955531978607177, Train Acc 0.18625, Val Loss: 2.0188341481345042, Val Acc: 0.205\n",
      "Epoch: 99, Train Loss: 2.001025524139404, Train Acc 0.2, Val Loss: 2.00045599256243, Val Acc: 0.22\n",
      "Epoch: 100, Train Loss: 2.00775785446167, Train Acc 0.2025, Val Loss: 2.0066482509885515, Val Acc: 0.205\n"
     ]
    }
   ],
   "source": [
    "pot = Pot(d_model=5, nhead=1, num_layers=1, max_seq_len=64, dim_feedforward=64, dropout=0.3, num_types=9)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.005, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = pot(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# # Test\n",
    "# test_loss, test_acc = evaluate(pot, test_loader)\n",
    "# print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(pot.state_dict(), 'pot_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it as feature extractor (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "pot.load_state_dict(torch.load(\"pot_model.pth\"))\n",
    "pot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_hidden = pot(train_tokens, train_mask, pre_train=True).view(train_tokens.size(0), -1)\n",
    "    val_hidden = pot(val_tokens, val_mask, pre_train=True).view(val_tokens.size(0), -1)\n",
    "    test_hidden = pot(test_tokens, test_mask, pre_train=True).view(test_tokens.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_hidden, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_hidden, val_labels))\n",
    "test_loader = DataLoader(TensorDataset(test_hidden, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.9963074207305909, Train Acc 0.50625, Val Loss: 1.30093060284853, Val Acc: 0.62\n",
      "Epoch: 2, Train Loss: 1.2693264412879943, Train Acc 0.5925, Val Loss: 1.0721332759410143, Val Acc: 0.68\n",
      "Epoch: 3, Train Loss: 1.166937198638916, Train Acc 0.605, Val Loss: 1.0162230451405048, Val Acc: 0.7\n",
      "Epoch: 4, Train Loss: 1.1719644212722777, Train Acc 0.59125, Val Loss: 1.1128932654857635, Val Acc: 0.71\n",
      "Epoch: 5, Train Loss: 1.166082215309143, Train Acc 0.6225, Val Loss: 1.0086301210522652, Val Acc: 0.72\n",
      "Epoch: 6, Train Loss: 1.1365370631217957, Train Acc 0.6125, Val Loss: 1.1069583275914192, Val Acc: 0.66\n",
      "Epoch: 7, Train Loss: 1.178542513847351, Train Acc 0.60125, Val Loss: 1.2399723929166795, Val Acc: 0.59\n",
      "Epoch: 8, Train Loss: 1.1612643957138062, Train Acc 0.59125, Val Loss: 1.0868311071395873, Val Acc: 0.69\n",
      "Epoch: 9, Train Loss: 1.1284274244308472, Train Acc 0.60875, Val Loss: 1.0686833444237709, Val Acc: 0.66\n",
      "Epoch: 10, Train Loss: 1.1235071158409118, Train Acc 0.61125, Val Loss: 1.0323871737718582, Val Acc: 0.7\n",
      "Epoch: 11, Train Loss: 1.1324586868286133, Train Acc 0.61625, Val Loss: 1.0419702281057834, Val Acc: 0.69\n",
      "Epoch: 12, Train Loss: 1.1420544934272767, Train Acc 0.60625, Val Loss: 1.0410168534517288, Val Acc: 0.72\n",
      "Epoch: 13, Train Loss: 1.1209018397331239, Train Acc 0.63, Val Loss: 1.0741409802436828, Val Acc: 0.7\n",
      "Epoch: 14, Train Loss: 1.112939703464508, Train Acc 0.61375, Val Loss: 1.0233982527256011, Val Acc: 0.71\n",
      "Epoch: 15, Train Loss: 1.1448683261871337, Train Acc 0.60625, Val Loss: 1.0169035314023496, Val Acc: 0.71\n",
      "Epoch: 16, Train Loss: 1.1269723868370056, Train Acc 0.61375, Val Loss: 1.0167317280173302, Val Acc: 0.69\n",
      "Epoch: 17, Train Loss: 1.1318548393249512, Train Acc 0.60625, Val Loss: 1.1504305997490882, Val Acc: 0.66\n",
      "Epoch: 18, Train Loss: 1.1318707847595215, Train Acc 0.60375, Val Loss: 1.0020979772508145, Val Acc: 0.71\n",
      "Epoch: 19, Train Loss: 1.111134376525879, Train Acc 0.62125, Val Loss: 1.1472789430618286, Val Acc: 0.62\n",
      "Epoch: 20, Train Loss: 1.1136483645439148, Train Acc 0.61, Val Loss: 1.040769821703434, Val Acc: 0.72\n",
      "Test Loss: 0.9882621133327484, Test Acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(train_hidden.size(1), 128, 10, 0.5)\n",
    "\n",
    "if USE_GPU:\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, loader):\n",
    "    model.eval()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(classifier, train_loader)\n",
    "    val_loss, val_acc = evaluate(classifier, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(classifier, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
