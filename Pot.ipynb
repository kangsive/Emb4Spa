{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer using Pytorch Transformer Encoder Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mask is different here, only (batch_size, seq_len) mask where True stands for invalid (mask) attention queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pot(nn.Module):\n",
    "    def __init__(self, d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(dim_feedforward, num_types))\n",
    "\n",
    "    def forward(self, x, mask=None, pre_train=False):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.ones((batch_size, 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        if not pre_train:\n",
    "            x = x[:, 0, :] # grab the class embedding\n",
    "            x = self.mlp_head(x)\n",
    "        else:\n",
    "            x = x[:, 1:, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, dense_size, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(input_size, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(dense_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid wkt string, skip it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "def count_points(wkt):\n",
    "    try:\n",
    "        num_points = gv.num_points_from_wkt(wkt)\n",
    "        # gv.vectorize_wkt(wkt)\n",
    "        return num_points\n",
    "    except:\n",
    "        print(\"Invalid wkt string, skip it\")\n",
    "        return np.inf\n",
    "\n",
    "filtered_df = df[df['WKT'].apply(lambda x: count_points(x) <= max_seq_len)]\n",
    "df = filtered_df\n",
    "\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(df, val_split_ratio, test_split_ratio):\n",
    "\n",
    "    data, labels = np.array(df['WKT'].tolist()), np.array(df['type'].tolist())\n",
    "\n",
    "    num_val = int(val_split_ratio * len(df))\n",
    "    num_test = int(test_split_ratio * len(df))\n",
    "\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = indices[num_val+num_test:], indices[:num_val], indices[num_val:num_val+num_test]\n",
    "\n",
    "    train_data, train_labels = data[train_indices], labels[train_indices]\n",
    "    val_data, val_labels = data[val_indices], labels[val_indices]\n",
    "    test_data, test_labels = data[test_indices], labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "ori_train_data, ori_train_labels, ori_val_data, ori_val_labels, ori_test_data, ori_test_labels = dataset_split(df, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(wkts, types, max_seq_len, train=True): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices >= start_points\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    if train:\n",
    "        gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_labels, train_mask = prepare_polygon_dataset(ori_train_data, ori_train_labels, max_seq_len)\n",
    "val_tokens, val_labels, val_mask = prepare_polygon_dataset(ori_val_data, ori_val_labels, max_seq_len, train=False)\n",
    "test_tokens, test_labels, test_mask = prepare_polygon_dataset(ori_test_data, ori_test_labels, max_seq_len, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels, train_mask), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens, val_labels, val_mask))\n",
    "test_loader = DataLoader(TensorDataset(test_tokens, test_labels, test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.6232487869262695, Train Acc 0.485, Val Loss: 1.1538843685388565, Val Acc: 0.66\n",
      "Epoch: 2, Train Loss: 1.3718886661529541, Train Acc 0.51125, Val Loss: 1.2054683375358581, Val Acc: 0.66\n",
      "Epoch: 3, Train Loss: 1.283057587146759, Train Acc 0.54375, Val Loss: 1.1345443686842918, Val Acc: 0.61\n",
      "Epoch: 4, Train Loss: 1.2025516653060913, Train Acc 0.57375, Val Loss: 1.1275383463501931, Val Acc: 0.66\n",
      "Epoch: 5, Train Loss: 1.2055569076538086, Train Acc 0.56125, Val Loss: 1.0388881599903106, Val Acc: 0.66\n",
      "Epoch: 6, Train Loss: 1.1731059885025024, Train Acc 0.5875, Val Loss: 1.0553244864940643, Val Acc: 0.66\n",
      "Epoch: 7, Train Loss: 1.1578207039833068, Train Acc 0.585, Val Loss: 1.0129314029216767, Val Acc: 0.72\n",
      "Epoch: 8, Train Loss: 1.1330295300483704, Train Acc 0.61375, Val Loss: 0.9939808475971222, Val Acc: 0.72\n",
      "Epoch: 9, Train Loss: 1.1526591968536377, Train Acc 0.595, Val Loss: 0.9989190724492073, Val Acc: 0.71\n",
      "Epoch: 10, Train Loss: 1.1432475090026855, Train Acc 0.58375, Val Loss: 1.0376057353615762, Val Acc: 0.66\n",
      "Epoch: 11, Train Loss: 1.1417534780502319, Train Acc 0.60625, Val Loss: 1.0652352637052536, Val Acc: 0.66\n",
      "Epoch: 12, Train Loss: 1.148755841255188, Train Acc 0.6, Val Loss: 1.0352267119288445, Val Acc: 0.71\n",
      "Epoch: 13, Train Loss: 1.1257763361930848, Train Acc 0.61375, Val Loss: 1.026122708171606, Val Acc: 0.65\n",
      "Epoch: 14, Train Loss: 1.1443905210494996, Train Acc 0.62, Val Loss: 1.0005321383476258, Val Acc: 0.72\n",
      "Epoch: 15, Train Loss: 1.1343252778053283, Train Acc 0.61375, Val Loss: 1.0622106766700745, Val Acc: 0.65\n",
      "Epoch: 16, Train Loss: 1.136517915725708, Train Acc 0.605, Val Loss: 1.057493687272072, Val Acc: 0.66\n",
      "Epoch: 17, Train Loss: 1.132713737487793, Train Acc 0.605, Val Loss: 1.0091810601949691, Val Acc: 0.68\n",
      "Epoch: 18, Train Loss: 1.1438436698913574, Train Acc 0.6025, Val Loss: 1.059304268360138, Val Acc: 0.66\n",
      "Epoch: 19, Train Loss: 1.1452696704864502, Train Acc 0.60375, Val Loss: 1.0152427092194558, Val Acc: 0.69\n",
      "Epoch: 20, Train Loss: 1.1282357883453369, Train Acc 0.61625, Val Loss: 1.0156431567668915, Val Acc: 0.7\n",
      "Test Loss: 0.9627675825357437, Test Acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y, batch_mask in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "            outputs = pot(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(pot, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(pot.state_dict(), 'pot_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it as feature extractor (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "pot.load_state_dict(torch.load(\"pot_model.pth\"))\n",
    "pot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_hidden = pot(train_tokens, train_mask, pre_train=True).view(train_tokens.size(0), -1)\n",
    "    val_hidden = pot(val_tokens, val_mask, pre_train=True).view(val_tokens.size(0), -1)\n",
    "    test_hidden = pot(test_tokens, test_mask, pre_train=True).view(test_tokens.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_hidden, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_hidden, val_labels))\n",
    "test_loader = DataLoader(TensorDataset(test_hidden, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.9963074207305909, Train Acc 0.50625, Val Loss: 1.30093060284853, Val Acc: 0.62\n",
      "Epoch: 2, Train Loss: 1.2693264412879943, Train Acc 0.5925, Val Loss: 1.0721332759410143, Val Acc: 0.68\n",
      "Epoch: 3, Train Loss: 1.166937198638916, Train Acc 0.605, Val Loss: 1.0162230451405048, Val Acc: 0.7\n",
      "Epoch: 4, Train Loss: 1.1719644212722777, Train Acc 0.59125, Val Loss: 1.1128932654857635, Val Acc: 0.71\n",
      "Epoch: 5, Train Loss: 1.166082215309143, Train Acc 0.6225, Val Loss: 1.0086301210522652, Val Acc: 0.72\n",
      "Epoch: 6, Train Loss: 1.1365370631217957, Train Acc 0.6125, Val Loss: 1.1069583275914192, Val Acc: 0.66\n",
      "Epoch: 7, Train Loss: 1.178542513847351, Train Acc 0.60125, Val Loss: 1.2399723929166795, Val Acc: 0.59\n",
      "Epoch: 8, Train Loss: 1.1612643957138062, Train Acc 0.59125, Val Loss: 1.0868311071395873, Val Acc: 0.69\n",
      "Epoch: 9, Train Loss: 1.1284274244308472, Train Acc 0.60875, Val Loss: 1.0686833444237709, Val Acc: 0.66\n",
      "Epoch: 10, Train Loss: 1.1235071158409118, Train Acc 0.61125, Val Loss: 1.0323871737718582, Val Acc: 0.7\n",
      "Epoch: 11, Train Loss: 1.1324586868286133, Train Acc 0.61625, Val Loss: 1.0419702281057834, Val Acc: 0.69\n",
      "Epoch: 12, Train Loss: 1.1420544934272767, Train Acc 0.60625, Val Loss: 1.0410168534517288, Val Acc: 0.72\n",
      "Epoch: 13, Train Loss: 1.1209018397331239, Train Acc 0.63, Val Loss: 1.0741409802436828, Val Acc: 0.7\n",
      "Epoch: 14, Train Loss: 1.112939703464508, Train Acc 0.61375, Val Loss: 1.0233982527256011, Val Acc: 0.71\n",
      "Epoch: 15, Train Loss: 1.1448683261871337, Train Acc 0.60625, Val Loss: 1.0169035314023496, Val Acc: 0.71\n",
      "Epoch: 16, Train Loss: 1.1269723868370056, Train Acc 0.61375, Val Loss: 1.0167317280173302, Val Acc: 0.69\n",
      "Epoch: 17, Train Loss: 1.1318548393249512, Train Acc 0.60625, Val Loss: 1.1504305997490882, Val Acc: 0.66\n",
      "Epoch: 18, Train Loss: 1.1318707847595215, Train Acc 0.60375, Val Loss: 1.0020979772508145, Val Acc: 0.71\n",
      "Epoch: 19, Train Loss: 1.111134376525879, Train Acc 0.62125, Val Loss: 1.1472789430618286, Val Acc: 0.62\n",
      "Epoch: 20, Train Loss: 1.1136483645439148, Train Acc 0.61, Val Loss: 1.040769821703434, Val Acc: 0.72\n",
      "Test Loss: 0.9882621133327484, Test Acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(train_hidden.size(1), 128, 10, 0.5)\n",
    "\n",
    "if USE_GPU:\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def train(model, loader):\n",
    "    model.eval()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(classifier, train_loader)\n",
    "    val_loss, val_acc = evaluate(classifier, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(classifier, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
