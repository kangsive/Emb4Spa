{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Transformer using Pytorch Transformer Encoder Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mask is different here, only (batch_size, seq_len) mask where True stands for invalid (mask) attention queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pot(nn.Module):\n",
    "    def __init__(self, d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + max_seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(dim_feedforward, num_types))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "                # token_mask = (tokens != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        class_embedding = self.class_embedding.repeat(batch_size, 1, 1)\n",
    "        x = torch.cat([class_embedding, x], dim=1)\n",
    "        # print(x.shape, self.pos_embedding[:, :seq_len+1].shape)\n",
    "        x = x + self.pos_embedding[:, :seq_len+1]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Create a new tensor with True values in the first column (for cls token)\n",
    "        if mask is not None:\n",
    "            cls_mask = torch.ones((batch_size, 1), dtype=torch.bool)\n",
    "            if USE_GPU:\n",
    "                cls_mask = cls_mask.to(device)\n",
    "            mask = torch.cat((cls_mask, mask), dim=1)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        x = x[:, 0, :] # grab the class embedding\n",
    "        x = self.mlp_head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid wkt string, skip it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "gs = GeomScaler()\n",
    "types_dict = {'PK':0, 'MR': 1, 'KL':2, 'NV':3, 'WA':4, 'LG':5, 'HO':6, 'GR':7, 'REC':8, 'PGK':9}\n",
    "df = pd.read_csv(\"archaeology.csv\")\n",
    "df['type'] = df['Aardspoor'].map(types_dict)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "def count_points(wkt):\n",
    "    try:\n",
    "        num_points = gv.num_points_from_wkt(wkt)\n",
    "        # gv.vectorize_wkt(wkt)\n",
    "        return num_points\n",
    "    except:\n",
    "        print(\"Invalid wkt string, skip it\")\n",
    "        return np.inf\n",
    "\n",
    "filtered_df = df[df['WKT'].apply(lambda x: count_points(x) <= max_seq_len)]\n",
    "df = filtered_df\n",
    "\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(df, val_split_ratio, test_split_ratio):\n",
    "\n",
    "    data, labels = np.array(df['WKT'].tolist()), np.array(df['type'].tolist())\n",
    "\n",
    "    num_val = int(val_split_ratio * len(df))\n",
    "    num_test = int(test_split_ratio * len(df))\n",
    "\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = indices[num_val+num_test:], indices[:num_val], indices[num_val:num_val+num_test]\n",
    "\n",
    "    train_data, train_labels = data[train_indices], labels[train_indices]\n",
    "    val_data, val_labels = data[val_indices], labels[val_indices]\n",
    "    test_data, test_labels = data[test_indices], labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "ori_train_data, ori_train_labels, ori_val_data, ori_val_labels, ori_test_data, ori_test_labels = dataset_split(df, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_polygon_dataset(wkts, types, max_seq_len, train=True): # TODO - 1. split into train, validate, test. 2. randomly sample\n",
    "    geoms, labels, start_points = [], [], []\n",
    "    for i, wkt in enumerate(wkts):\n",
    "        num_point = gv.num_points_from_wkt(wkt)\n",
    "        if  num_point > max_seq_len:\n",
    "             continue\n",
    "        geom = gv.vectorize_wkt(wkt, max_points=max_seq_len, fixed_size=True)\n",
    "        geoms.append(geom)\n",
    "        labels.append(types[i])\n",
    "        start_points.append(num_point)\n",
    "\n",
    "    start_points = torch.tensor(start_points).unsqueeze(1)\n",
    "    indices = torch.arange(max_seq_len).unsqueeze(0)\n",
    "    mask = indices >= start_points\n",
    "    tokens = np.stack(geoms, axis=0)\n",
    "    if train:\n",
    "        gs.fit(tokens)\n",
    "    tokens = gs.transform(tokens)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_labels, train_mask = prepare_polygon_dataset(ori_train_data, ori_train_labels, max_seq_len)\n",
    "val_tokens, val_labels, val_mask = prepare_polygon_dataset(ori_val_data, ori_val_labels, max_seq_len, train=False)\n",
    "test_tokens, test_labels, test_mask = prepare_polygon_dataset(ori_test_data, ori_test_labels, max_seq_len, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels, train_mask), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tokens, val_labels, val_mask))\n",
    "test_loader = DataLoader(TensorDataset(test_tokens, test_labels, test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.5517314338684083, Train Acc 0.50375, Val Loss: 1.3070946848392486, Val Acc: 0.53\n",
      "Epoch: 2, Train Loss: 1.367800521850586, Train Acc 0.52625, Val Loss: 1.2718898367881775, Val Acc: 0.53\n",
      "Epoch: 3, Train Loss: 1.341175582408905, Train Acc 0.525, Val Loss: 1.2165230885148048, Val Acc: 0.54\n",
      "Epoch: 4, Train Loss: 1.2440640807151795, Train Acc 0.57, Val Loss: 1.134313284754753, Val Acc: 0.63\n",
      "Epoch: 5, Train Loss: 1.192763397693634, Train Acc 0.5675, Val Loss: 1.170486791729927, Val Acc: 0.57\n",
      "Epoch: 6, Train Loss: 1.1788130807876587, Train Acc 0.5675, Val Loss: 1.1226997366547584, Val Acc: 0.57\n",
      "Epoch: 7, Train Loss: 1.1652851653099061, Train Acc 0.59, Val Loss: 1.1382062074542045, Val Acc: 0.57\n",
      "Epoch: 8, Train Loss: 1.1698787546157836, Train Acc 0.5925, Val Loss: 1.1251955422759057, Val Acc: 0.59\n",
      "Epoch: 9, Train Loss: 1.1512342691421509, Train Acc 0.6, Val Loss: 1.1300003844499589, Val Acc: 0.57\n",
      "Epoch: 10, Train Loss: 1.1560446310043335, Train Acc 0.58875, Val Loss: 1.1431664615869521, Val Acc: 0.57\n",
      "Epoch: 11, Train Loss: 1.1690537571907043, Train Acc 0.595, Val Loss: 1.1270540949702264, Val Acc: 0.57\n",
      "Epoch: 12, Train Loss: 1.1476451325416566, Train Acc 0.6075, Val Loss: 1.1765918973088265, Val Acc: 0.55\n",
      "Epoch: 13, Train Loss: 1.1424604392051696, Train Acc 0.60625, Val Loss: 1.1310358148813249, Val Acc: 0.57\n",
      "Epoch: 14, Train Loss: 1.1450900387763978, Train Acc 0.6, Val Loss: 1.1419227889180183, Val Acc: 0.57\n",
      "Epoch: 15, Train Loss: 1.1446117329597474, Train Acc 0.6025, Val Loss: 1.1308247050642968, Val Acc: 0.57\n",
      "Epoch: 16, Train Loss: 1.1396910381317138, Train Acc 0.61125, Val Loss: 1.1399915954470634, Val Acc: 0.58\n",
      "Epoch: 17, Train Loss: 1.1352251553535462, Train Acc 0.60375, Val Loss: 1.140150760114193, Val Acc: 0.59\n",
      "Epoch: 18, Train Loss: 1.1467577838897705, Train Acc 0.61, Val Loss: 1.1980142784118653, Val Acc: 0.54\n",
      "Epoch: 19, Train Loss: 1.1345232510566712, Train Acc 0.60125, Val Loss: 1.1568509623408318, Val Acc: 0.56\n",
      "Epoch: 20, Train Loss: 1.127573173046112, Train Acc 0.61, Val Loss: 1.1605033692717552, Val Acc: 0.56\n",
      "Epoch: 21, Train Loss: 1.1189226603507996, Train Acc 0.605, Val Loss: 1.1371011990308761, Val Acc: 0.58\n",
      "Epoch: 22, Train Loss: 1.121181893348694, Train Acc 0.605, Val Loss: 1.1417248949408532, Val Acc: 0.56\n",
      "Epoch: 23, Train Loss: 1.1247052955627441, Train Acc 0.5925, Val Loss: 1.0915295568108558, Val Acc: 0.57\n",
      "Epoch: 24, Train Loss: 1.1041243410110473, Train Acc 0.61, Val Loss: 1.1006173184514045, Val Acc: 0.54\n",
      "Epoch: 25, Train Loss: 1.1208046197891235, Train Acc 0.6125, Val Loss: 1.108271517008543, Val Acc: 0.57\n",
      "Epoch: 26, Train Loss: 1.1244981050491334, Train Acc 0.5975, Val Loss: 1.0884386914968491, Val Acc: 0.58\n",
      "Epoch: 27, Train Loss: 1.1081186318397522, Train Acc 0.605, Val Loss: 1.1008475759625436, Val Acc: 0.57\n",
      "Epoch: 28, Train Loss: 1.1096651101112365, Train Acc 0.60875, Val Loss: 1.1062361791729927, Val Acc: 0.57\n",
      "Epoch: 29, Train Loss: 1.0926522731781005, Train Acc 0.62375, Val Loss: 1.097909612953663, Val Acc: 0.57\n",
      "Epoch: 30, Train Loss: 1.0924261713027954, Train Acc 0.6225, Val Loss: 1.0972538569569588, Val Acc: 0.57\n",
      "Epoch: 31, Train Loss: 1.1027132868766785, Train Acc 0.60875, Val Loss: 1.1127487936615943, Val Acc: 0.59\n",
      "Epoch: 32, Train Loss: 1.10079909324646, Train Acc 0.61625, Val Loss: 1.1717384961247443, Val Acc: 0.58\n",
      "Epoch: 33, Train Loss: 1.105682191848755, Train Acc 0.6175, Val Loss: 1.0679941672086715, Val Acc: 0.61\n",
      "Epoch: 34, Train Loss: 1.075620913505554, Train Acc 0.63375, Val Loss: 1.1061603906750679, Val Acc: 0.59\n",
      "Epoch: 35, Train Loss: 1.092500717639923, Train Acc 0.61375, Val Loss: 1.092045081704855, Val Acc: 0.58\n",
      "Epoch: 36, Train Loss: 1.0774425411224364, Train Acc 0.6125, Val Loss: 1.0942260649800302, Val Acc: 0.62\n",
      "Epoch: 37, Train Loss: 1.0924347829818726, Train Acc 0.6175, Val Loss: 1.061551187336445, Val Acc: 0.61\n",
      "Epoch: 38, Train Loss: 1.0810932421684265, Train Acc 0.63125, Val Loss: 1.1105371338129044, Val Acc: 0.61\n",
      "Epoch: 39, Train Loss: 1.0736441469192506, Train Acc 0.62125, Val Loss: 1.049196984767914, Val Acc: 0.61\n",
      "Epoch: 40, Train Loss: 1.073830590248108, Train Acc 0.63625, Val Loss: 1.0830514794588089, Val Acc: 0.6\n",
      "Epoch: 41, Train Loss: 1.0760272288322448, Train Acc 0.6275, Val Loss: 1.0761795511841774, Val Acc: 0.62\n",
      "Epoch: 42, Train Loss: 1.0693186187744141, Train Acc 0.63625, Val Loss: 1.0892902401089668, Val Acc: 0.61\n",
      "Epoch: 43, Train Loss: 1.061089277267456, Train Acc 0.625, Val Loss: 1.0768578091263772, Val Acc: 0.58\n",
      "Epoch: 44, Train Loss: 1.0816773200035095, Train Acc 0.61125, Val Loss: 1.0478779941797256, Val Acc: 0.61\n",
      "Epoch: 45, Train Loss: 1.0824454927444458, Train Acc 0.62875, Val Loss: 1.0033617055416106, Val Acc: 0.63\n",
      "Epoch: 46, Train Loss: 1.0760072255134583, Train Acc 0.625, Val Loss: 1.1253533497452737, Val Acc: 0.6\n",
      "Epoch: 47, Train Loss: 1.0710421633720397, Train Acc 0.6325, Val Loss: 1.0724527463316917, Val Acc: 0.63\n",
      "Epoch: 48, Train Loss: 1.0622164416313171, Train Acc 0.645, Val Loss: 1.098534568399191, Val Acc: 0.6\n",
      "Epoch: 49, Train Loss: 1.0747478747367858, Train Acc 0.61875, Val Loss: 1.054696660786867, Val Acc: 0.61\n",
      "Epoch: 50, Train Loss: 1.0736546659469604, Train Acc 0.63125, Val Loss: 1.1069464707374572, Val Acc: 0.58\n",
      "Epoch: 51, Train Loss: 1.0660543537139893, Train Acc 0.63375, Val Loss: 1.0522190257906914, Val Acc: 0.62\n",
      "Epoch: 52, Train Loss: 1.0655960941314697, Train Acc 0.63125, Val Loss: 1.0560574784874917, Val Acc: 0.6\n",
      "Epoch: 53, Train Loss: 1.058797013759613, Train Acc 0.63375, Val Loss: 1.0495553693175317, Val Acc: 0.62\n",
      "Epoch: 54, Train Loss: 1.0752241706848145, Train Acc 0.63, Val Loss: 1.0358213019371032, Val Acc: 0.62\n",
      "Epoch: 55, Train Loss: 1.0658988857269287, Train Acc 0.63125, Val Loss: 1.0609486654400826, Val Acc: 0.64\n",
      "Epoch: 56, Train Loss: 1.0760755133628845, Train Acc 0.62375, Val Loss: 1.0417075242847205, Val Acc: 0.62\n",
      "Epoch: 57, Train Loss: 1.059459707736969, Train Acc 0.64625, Val Loss: 1.1069050009548664, Val Acc: 0.62\n",
      "Epoch: 58, Train Loss: 1.0576392960548402, Train Acc 0.63375, Val Loss: 1.0905303965508937, Val Acc: 0.62\n",
      "Epoch: 59, Train Loss: 1.0690685415267944, Train Acc 0.62375, Val Loss: 1.058396683782339, Val Acc: 0.63\n",
      "Epoch: 60, Train Loss: 1.0627764058113098, Train Acc 0.64625, Val Loss: 1.061400149911642, Val Acc: 0.62\n",
      "Epoch: 61, Train Loss: 1.0595084929466247, Train Acc 0.625, Val Loss: 1.0485110537707805, Val Acc: 0.62\n",
      "Epoch: 62, Train Loss: 1.0580859112739562, Train Acc 0.635, Val Loss: 1.018718840777874, Val Acc: 0.62\n",
      "Epoch: 63, Train Loss: 1.0424169206619263, Train Acc 0.65375, Val Loss: 1.090128310918808, Val Acc: 0.61\n",
      "Epoch: 64, Train Loss: 1.0696748518943786, Train Acc 0.625, Val Loss: 1.0705160441994668, Val Acc: 0.62\n",
      "Epoch: 65, Train Loss: 1.0701849555969238, Train Acc 0.63375, Val Loss: 1.0421412101387977, Val Acc: 0.62\n",
      "Epoch: 66, Train Loss: 1.0514386820793151, Train Acc 0.64625, Val Loss: 1.0459907352924347, Val Acc: 0.64\n",
      "Epoch: 67, Train Loss: 1.0653396248817444, Train Acc 0.635, Val Loss: 1.0833205172419549, Val Acc: 0.62\n",
      "Epoch: 68, Train Loss: 1.0578256154060364, Train Acc 0.63125, Val Loss: 1.065440468788147, Val Acc: 0.64\n",
      "Epoch: 69, Train Loss: 1.0531369876861572, Train Acc 0.63125, Val Loss: 1.0716861808300018, Val Acc: 0.62\n",
      "Epoch: 70, Train Loss: 1.0566541051864624, Train Acc 0.64375, Val Loss: 1.0429230116307735, Val Acc: 0.62\n",
      "Epoch: 71, Train Loss: 1.0493340301513672, Train Acc 0.6375, Val Loss: 1.056801187247038, Val Acc: 0.63\n",
      "Epoch: 72, Train Loss: 1.0428484106063842, Train Acc 0.63625, Val Loss: 1.020921266078949, Val Acc: 0.63\n",
      "Epoch: 73, Train Loss: 1.0469464755058289, Train Acc 0.64125, Val Loss: 1.0456183365732432, Val Acc: 0.65\n",
      "Epoch: 74, Train Loss: 1.0474844145774842, Train Acc 0.65125, Val Loss: 1.07457491889596, Val Acc: 0.64\n",
      "Epoch: 75, Train Loss: 1.0717939639091492, Train Acc 0.62625, Val Loss: 1.0859276260435582, Val Acc: 0.6\n",
      "Epoch: 76, Train Loss: 1.0545061373710631, Train Acc 0.6425, Val Loss: 1.0422825660556554, Val Acc: 0.6\n",
      "Epoch: 77, Train Loss: 1.0568816041946412, Train Acc 0.655, Val Loss: 1.0721095734089614, Val Acc: 0.64\n",
      "Epoch: 78, Train Loss: 1.0533640360832215, Train Acc 0.63875, Val Loss: 1.0442824602127074, Val Acc: 0.62\n",
      "Epoch: 79, Train Loss: 1.0465138578414916, Train Acc 0.64125, Val Loss: 1.081475709527731, Val Acc: 0.6\n",
      "Epoch: 80, Train Loss: 1.051232817173004, Train Acc 0.6325, Val Loss: 1.0085385075956583, Val Acc: 0.62\n",
      "Epoch: 81, Train Loss: 1.0423556852340699, Train Acc 0.64, Val Loss: 1.0500127501040697, Val Acc: 0.6\n",
      "Epoch: 82, Train Loss: 1.0466503953933717, Train Acc 0.625, Val Loss: 0.9820748694241047, Val Acc: 0.61\n",
      "Epoch: 83, Train Loss: 1.027434422969818, Train Acc 0.65, Val Loss: 1.06384871378541, Val Acc: 0.62\n",
      "Epoch: 84, Train Loss: 1.053585159778595, Train Acc 0.63375, Val Loss: 1.067545204758644, Val Acc: 0.65\n",
      "Epoch: 85, Train Loss: 1.0297384977340698, Train Acc 0.64375, Val Loss: 1.0903788689523934, Val Acc: 0.6\n",
      "Epoch: 86, Train Loss: 1.0323747277259827, Train Acc 0.65125, Val Loss: 1.0409329052269458, Val Acc: 0.62\n",
      "Epoch: 87, Train Loss: 1.0673642754554749, Train Acc 0.63, Val Loss: 1.044784127175808, Val Acc: 0.62\n",
      "Epoch: 88, Train Loss: 1.0341392517089845, Train Acc 0.65, Val Loss: 1.0558352992683648, Val Acc: 0.6\n",
      "Epoch: 89, Train Loss: 1.0424702739715577, Train Acc 0.645, Val Loss: 1.0329238083213568, Val Acc: 0.6\n",
      "Epoch: 90, Train Loss: 1.023487138748169, Train Acc 0.64875, Val Loss: 1.0315769369900227, Val Acc: 0.59\n",
      "Epoch: 91, Train Loss: 1.0466382098197937, Train Acc 0.645, Val Loss: 1.058379258736968, Val Acc: 0.59\n",
      "Epoch: 92, Train Loss: 1.0397205090522765, Train Acc 0.6375, Val Loss: 1.045846911817789, Val Acc: 0.62\n",
      "Epoch: 93, Train Loss: 1.0321462488174438, Train Acc 0.64125, Val Loss: 1.0179932409524917, Val Acc: 0.6\n",
      "Epoch: 94, Train Loss: 1.0382267546653747, Train Acc 0.64, Val Loss: 1.0175970562174916, Val Acc: 0.6\n",
      "Epoch: 95, Train Loss: 1.0423982548713684, Train Acc 0.64375, Val Loss: 1.0573023594915867, Val Acc: 0.6\n",
      "Epoch: 96, Train Loss: 1.0438802981376647, Train Acc 0.64125, Val Loss: 1.09689174041152, Val Acc: 0.6\n",
      "Epoch: 97, Train Loss: 1.0440647959709168, Train Acc 0.64125, Val Loss: 1.0829919494688511, Val Acc: 0.62\n",
      "Epoch: 98, Train Loss: 1.0459668040275574, Train Acc 0.63875, Val Loss: 1.0768596874177456, Val Acc: 0.6\n",
      "Epoch: 99, Train Loss: 1.033929603099823, Train Acc 0.64625, Val Loss: 1.0707195914536713, Val Acc: 0.6\n",
      "Epoch: 100, Train Loss: 1.0423488926887512, Train Acc 0.645, Val Loss: 1.037029836550355, Val Acc: 0.6\n",
      "Test Loss: 0.9445931981503963, Test Acc: 0.7\n"
     ]
    }
   ],
   "source": [
    "pot = Pot(d_model=7, nhead=1, num_layers=3, max_seq_len=64, dim_feedforward=64, dropout=0.1, num_types=10)\n",
    "\n",
    "if USE_GPU:\n",
    "    pot = pot.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(pot.parameters(), lr=0.004)\n",
    "optimizer = optim.Adam(pot.parameters(), lr=0.004, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y, batch_mask in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pot(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y, batch_mask = batch_x.to(device), batch_y.to(device), batch_mask.to(device)\n",
    "            outputs = pot(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(pot, train_loader)\n",
    "    val_loss, val_acc = evaluate(pot, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = evaluate(pot, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
