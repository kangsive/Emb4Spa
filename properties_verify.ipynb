{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely import Polygon, affinity\n",
    "from polygenerator import random_polygon\n",
    "from utils.vectorizer import vectorize_wkt\n",
    "\n",
    "\n",
    "def normalize(data, method=\"min_max\"):\n",
    "    if method == \"min_max\":\n",
    "        # Compute min and max values along the num_features dimension\n",
    "        min_vals = np.min(data[:, :, :2], axis=1, keepdims=True)\n",
    "        max_vals = np.max(data[:, :, :2], axis=1, keepdims=True)\n",
    "        # Perform min-max normalization\n",
    "        data[:, :, :2] -= min_vals\n",
    "        data[:, :, :2] /=  max_vals - min_vals\n",
    "\n",
    "    else:\n",
    "        mean = np.mean(data[:, :, :2], axis=1, keepdims=True)\n",
    "        std = np.std(data[:, :, :2], axis=1, keepdims=True)\n",
    "\n",
    "        data[:, :, :2] -= mean\n",
    "        data[:, :, :2] /=  std\n",
    "    return data\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from potae import PoTAE\n",
    "import torch\n",
    "\n",
    "pretrain_model = \"weights/potae_repeat2_lr0.0001_dmodel384_bs512_epoch200_runname-mild-darkness-78.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "emb_model = PoTAE(fea_dim=7, d_model=384, num_heads=6, hidden_dim=64,\n",
    "                    ffn_dim=1024, layer_repeat=2, dropout=0.1, max_seq_len=64).to(device)\n",
    "emb_model.load_state_dict(torch.load(pretrain_model, map_location=device))\n",
    "emb_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Polygon(random_polygon(10))\n",
    "va = vectorize_wkt(a.wkt, max_points=64, fixed_size=True, simplify=True)\n",
    "va = normalize(np.expand_dims(va, axis=0))\n",
    "b = affinity.scale(a, 0.5, 0.5)\n",
    "# b = affinity.translate(a, 10, 10)\n",
    "# b = affinity.rotate(a, 30)\n",
    "vb = vectorize_wkt(b.wkt, max_points=64, fixed_size=True, simplify=True)\n",
    "vb = normalize(np.expand_dims(vb, axis=0))\n",
    "\n",
    "tokens = np.concatenate([va, vb], axis=0)\n",
    "tokens = torch.tensor(tokens, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings, _, _ = emb_model(tokens)\n",
    "\n",
    "cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the Transformer model b: 28612167\n"
     ]
    }
   ],
   "source": [
    "total_params_pot_model = sum(p.numel() for p in emb_model.parameters())\n",
    "print(f\"Total number of parameters in the Transformer model b: {total_params_pot_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Total number of parameters in the Transformer model s: 142061\n"
     ]
    }
   ],
   "source": [
    "emb_model2 = PoTAE(fea_dim=7, d_model=36, num_heads=4, hidden_dim=64,\n",
    "                    ffn_dim=32, layer_repeat=1, dropout=0.1, max_seq_len=64).to(device)\n",
    "total_params_pot_model = sum(p.numel() for p in emb_model2.parameters())\n",
    "print(f\"Total number of parameters in the Transformer model s: {total_params_pot_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalp up: 201\n"
     ]
    }
   ],
   "source": [
    "# 142061 -- > 28612167, potae: 0.14M --> 28M, emb_model: 0.71M --> 14.3M\n",
    "print(f\"scalp up: {28612167 // 142061}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
