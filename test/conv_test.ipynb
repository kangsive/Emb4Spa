{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingkang/envs/nlp_a4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from Prepare_dataset import prepare_dataset, prepare_dataset_fixedsize\n",
    "\n",
    "\n",
    "class CompareModel(nn.Module):\n",
    "    def __init__(self, emb_dim, dense_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 32, kernel_size=5, padding=2)  # Assuming input channels=1\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.dense1 = nn.Linear(64, dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(dense_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, geom_vector_len)\n",
    "        # Convolutional layers\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # No need to add softmax (already included in CrossEntropyLossFunction), otherwise it will be double softmax and converge slower\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 0 0]\n",
      " [4 5 0 0 0]\n",
      " [6 7 8 9 0]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0):\n",
    "#     if maxlen is None:\n",
    "#         maxlen = max(len(seq) for seq in sequences)\n",
    "\n",
    "#     padded_sequences = []\n",
    "#     for seq in sequences:\n",
    "#         if len(seq) >= maxlen:\n",
    "#             if truncating == 'pre':\n",
    "#                 padded_seq = seq[-maxlen:]\n",
    "#             else:\n",
    "#                 padded_seq = seq[:maxlen]\n",
    "#         else:\n",
    "#             if padding == 'pre':\n",
    "#                 padded_seq = [value] * (maxlen - len(seq)) + seq\n",
    "#             else:\n",
    "#                 padded_seq = seq + [value] * (maxlen - len(seq))\n",
    "#         padded_sequences.append(padded_seq)\n",
    "    \n",
    "#     return np.array(padded_sequences)\n",
    "\n",
    "# # Example usage\n",
    "# sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=5, padding='post', value=0)\n",
    "\n",
    "# print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# train_loaded = np.load(\"archaeology_train_v8.npz\", allow_pickle=True)\n",
    "# train_geoms = train_loaded['geoms']\n",
    "# train_labels = train_loaded['feature_type']\n",
    "\n",
    "# batch_size = 32\n",
    "# dataset_size = 1000\n",
    "# train_geoms = train_geoms[:1000]\n",
    "# train_labels = train_labels[:1000]\n",
    "\n",
    "# # Normalize\n",
    "# import geom_scaler\n",
    "\n",
    "# gs = geom_scaler.scale(train_geoms)\n",
    "# train_geoms = geom_scaler.transform(train_geoms, gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped = zip(train_geoms, train_labels)\n",
    "# train_input_sorted = {}\n",
    "# train_labels_sorted = {}\n",
    "\n",
    "# for geom, label in sorted(zipped, key=lambda x: len(x[0]), reverse=True):\n",
    "#     sequence_len = geom.shape[0]\n",
    "#     smallest_size_subset = sorted(train_input_sorted.keys())[0] if train_input_sorted else None\n",
    "\n",
    "#     if not smallest_size_subset:  # This is the first data point\n",
    "#         train_input_sorted[sequence_len] = [geom]\n",
    "#         train_labels_sorted[sequence_len] = [label]\n",
    "#         continue\n",
    "\n",
    "#     if sequence_len in train_input_sorted:  # the entry exists, append\n",
    "#         train_input_sorted[sequence_len].append(geom)\n",
    "#         train_labels_sorted[sequence_len].append(label)\n",
    "#         continue\n",
    "\n",
    "#     # the size subset does not exist yet\n",
    "#     # append the data to the smallest size subset if it isn't batch-sized yet\n",
    "#     if len(train_input_sorted[smallest_size_subset]) < batch_size:\n",
    "#         print(geom)\n",
    "#         geom = pad_sequences([geom], smallest_size_subset)[0]  # make it the same size as the rest in the subset\n",
    "#         train_input_sorted[smallest_size_subset].append(geom)\n",
    "#         train_labels_sorted[smallest_size_subset].append(label)\n",
    "#     else:\n",
    "#         train_input_sorted[sequence_len] = [geom]\n",
    "#         train_labels_sorted[sequence_len] = [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_geometry import vectorizer as gv\n",
    "from deep_geometry import GeomScaler\n",
    "\n",
    "\n",
    "max_seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "geom_train, geom_test, label_train, label_test = prepare_dataset_fixedsize()\n",
    "\n",
    "train_tokens = torch.tensor(geom_train, dtype=torch.float32)\n",
    "test_tokens = torch.tensor(geom_test, dtype=torch.float32)\n",
    "train_labels= torch.tensor(label_train, dtype=torch.long)\n",
    "test_labels = torch.tensor(label_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tokens, train_labels), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(test_tokens, test_labels), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = True if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.179936170578003, Train Acc 0.1375, Val Loss: 2.1592420169285367, Val Acc: 0.105\n",
      "Epoch: 2, Train Loss: 2.109309597015381, Train Acc 0.15625, Val Loss: 2.088898471423558, Val Acc: 0.16\n",
      "Epoch: 3, Train Loss: 2.062109155654907, Train Acc 0.15125, Val Loss: 2.033884252820696, Val Acc: 0.21\n",
      "Epoch: 4, Train Loss: 2.0190877771377562, Train Acc 0.20125, Val Loss: 2.0060332162039622, Val Acc: 0.19\n",
      "Epoch: 5, Train Loss: 2.0122658586502076, Train Acc 0.23, Val Loss: 2.006356017930167, Val Acc: 0.2\n",
      "Epoch: 6, Train Loss: 1.9931556606292724, Train Acc 0.23125, Val Loss: 1.9960494552339827, Val Acc: 0.215\n",
      "Epoch: 7, Train Loss: 1.9912152910232543, Train Acc 0.21, Val Loss: 1.9718333312443324, Val Acc: 0.215\n",
      "Epoch: 8, Train Loss: 1.9654785919189453, Train Acc 0.2475, Val Loss: 1.9672352245875768, Val Acc: 0.225\n",
      "Epoch: 9, Train Loss: 1.9493554019927979, Train Acc 0.2375, Val Loss: 1.9623815332140242, Val Acc: 0.215\n",
      "Epoch: 10, Train Loss: 1.93159939289093, Train Acc 0.2375, Val Loss: 1.9351187092917306, Val Acc: 0.245\n",
      "Epoch: 11, Train Loss: 1.9183145570755005, Train Acc 0.26375, Val Loss: 1.937045386859349, Val Acc: 0.225\n",
      "Epoch: 12, Train Loss: 1.8752222442626953, Train Acc 0.27875, Val Loss: 1.9189989226205009, Val Acc: 0.235\n",
      "Epoch: 13, Train Loss: 1.8724177360534668, Train Acc 0.29, Val Loss: 1.893771461078099, Val Acc: 0.24\n",
      "Epoch: 14, Train Loss: 1.840471715927124, Train Acc 0.2775, Val Loss: 1.875658324786595, Val Acc: 0.26\n",
      "Epoch: 15, Train Loss: 1.8201546096801757, Train Acc 0.31375, Val Loss: 1.8609261853354317, Val Acc: 0.255\n",
      "Epoch: 16, Train Loss: 1.8168195581436157, Train Acc 0.315, Val Loss: 1.8684599569865636, Val Acc: 0.245\n",
      "Epoch: 17, Train Loss: 1.8151938724517822, Train Acc 0.29, Val Loss: 1.8589119740894862, Val Acc: 0.25\n",
      "Epoch: 18, Train Loss: 1.7862705993652344, Train Acc 0.32125, Val Loss: 1.8551595721926009, Val Acc: 0.24\n",
      "Epoch: 19, Train Loss: 1.817487406730652, Train Acc 0.3175, Val Loss: 1.8583468539374215, Val Acc: 0.265\n",
      "Epoch: 20, Train Loss: 1.8012168025970459, Train Acc 0.3, Val Loss: 1.8586086886269706, Val Acc: 0.265\n",
      "Epoch: 21, Train Loss: 1.779612045288086, Train Acc 0.3075, Val Loss: 1.8436538321631295, Val Acc: 0.27\n",
      "Epoch: 22, Train Loss: 1.7962660264968873, Train Acc 0.31, Val Loss: 1.8598623956952776, Val Acc: 0.255\n",
      "Epoch: 23, Train Loss: 1.7939637660980225, Train Acc 0.31875, Val Loss: 1.8409289121627808, Val Acc: 0.275\n",
      "Epoch: 24, Train Loss: 1.7576468181610108, Train Acc 0.33, Val Loss: 1.8379200186048235, Val Acc: 0.245\n",
      "Epoch: 25, Train Loss: 1.7800836372375488, Train Acc 0.325, Val Loss: 1.8466874871935164, Val Acc: 0.27\n",
      "Epoch: 26, Train Loss: 1.7889772129058839, Train Acc 0.325, Val Loss: 1.840845091002328, Val Acc: 0.265\n",
      "Epoch: 27, Train Loss: 1.7844629096984863, Train Acc 0.29875, Val Loss: 1.8425733872822352, Val Acc: 0.255\n",
      "Epoch: 28, Train Loss: 1.7692409801483153, Train Acc 0.3275, Val Loss: 1.8366541010992867, Val Acc: 0.245\n",
      "Epoch: 29, Train Loss: 1.7565352725982666, Train Acc 0.33125, Val Loss: 1.8514861379350935, Val Acc: 0.255\n",
      "Epoch: 30, Train Loss: 1.753762903213501, Train Acc 0.32625, Val Loss: 1.8336892809186662, Val Acc: 0.255\n",
      "Epoch: 31, Train Loss: 1.743628854751587, Train Acc 0.315, Val Loss: 1.8490647077560425, Val Acc: 0.255\n",
      "Epoch: 32, Train Loss: 1.7458807134628296, Train Acc 0.3325, Val Loss: 1.848181094442095, Val Acc: 0.275\n",
      "Epoch: 33, Train Loss: 1.7396576404571533, Train Acc 0.32875, Val Loss: 1.8351436001913888, Val Acc: 0.24\n",
      "Epoch: 34, Train Loss: 1.7725532674789428, Train Acc 0.33625, Val Loss: 1.844226871218, Val Acc: 0.255\n",
      "Epoch: 35, Train Loss: 1.7586658096313477, Train Acc 0.33125, Val Loss: 1.8379743950707572, Val Acc: 0.265\n",
      "Epoch: 36, Train Loss: 1.7575737285614013, Train Acc 0.3225, Val Loss: 1.855972170829773, Val Acc: 0.3\n",
      "Epoch: 37, Train Loss: 1.7496458673477173, Train Acc 0.33875, Val Loss: 1.8452703952789307, Val Acc: 0.265\n",
      "Epoch: 38, Train Loss: 1.7316169214248658, Train Acc 0.35125, Val Loss: 1.8308116027287074, Val Acc: 0.28\n",
      "Epoch: 39, Train Loss: 1.7295332527160645, Train Acc 0.3325, Val Loss: 1.8468688215528215, Val Acc: 0.3\n",
      "Epoch: 40, Train Loss: 1.7480252885818481, Train Acc 0.32625, Val Loss: 1.8521980387823922, Val Acc: 0.27\n",
      "Epoch: 41, Train Loss: 1.728381814956665, Train Acc 0.34875, Val Loss: 1.850393567766462, Val Acc: 0.26\n",
      "Epoch: 42, Train Loss: 1.7268194437026978, Train Acc 0.34125, Val Loss: 1.8505361420767648, Val Acc: 0.29\n",
      "Epoch: 43, Train Loss: 1.7238509273529052, Train Acc 0.3425, Val Loss: 1.839830824307033, Val Acc: 0.3\n",
      "Epoch: 44, Train Loss: 1.7206415128707886, Train Acc 0.3425, Val Loss: 1.8480454172406877, Val Acc: 0.285\n",
      "Epoch: 45, Train Loss: 1.7221741056442261, Train Acc 0.33875, Val Loss: 1.8459552867071969, Val Acc: 0.255\n",
      "Epoch: 46, Train Loss: 1.7359532117843628, Train Acc 0.35375, Val Loss: 1.8501663548605782, Val Acc: 0.305\n",
      "Epoch: 47, Train Loss: 1.7443891096115112, Train Acc 0.3425, Val Loss: 1.8620125566210066, Val Acc: 0.3\n",
      "Epoch: 48, Train Loss: 1.705217170715332, Train Acc 0.3675, Val Loss: 1.8487786054611206, Val Acc: 0.3\n",
      "Epoch: 49, Train Loss: 1.7198034238815307, Train Acc 0.3375, Val Loss: 1.8393525566373552, Val Acc: 0.27\n",
      "Epoch: 50, Train Loss: 1.7212907934188844, Train Acc 0.31875, Val Loss: 1.8458773919514246, Val Acc: 0.315\n",
      "Epoch: 51, Train Loss: 1.7035824632644654, Train Acc 0.35125, Val Loss: 1.8528285537447249, Val Acc: 0.295\n",
      "Epoch: 52, Train Loss: 1.7306319761276245, Train Acc 0.32625, Val Loss: 1.8741941111428397, Val Acc: 0.305\n",
      "Epoch: 53, Train Loss: 1.7089516353607177, Train Acc 0.37625, Val Loss: 1.8454000098364693, Val Acc: 0.295\n",
      "Epoch: 54, Train Loss: 1.7187552547454834, Train Acc 0.3375, Val Loss: 1.854905162538801, Val Acc: 0.305\n",
      "Epoch: 55, Train Loss: 1.7259318590164185, Train Acc 0.355, Val Loss: 1.8479347569601876, Val Acc: 0.3\n",
      "Epoch: 56, Train Loss: 1.734500346183777, Train Acc 0.34375, Val Loss: 1.8421056951795305, Val Acc: 0.3\n",
      "Epoch: 57, Train Loss: 1.7224302434921264, Train Acc 0.3425, Val Loss: 1.859222446169172, Val Acc: 0.305\n",
      "Epoch: 58, Train Loss: 1.727464656829834, Train Acc 0.35375, Val Loss: 1.8593141862324305, Val Acc: 0.31\n",
      "Epoch: 59, Train Loss: 1.7072451639175414, Train Acc 0.35125, Val Loss: 1.8380780730928694, Val Acc: 0.275\n",
      "Epoch: 60, Train Loss: 1.7121845769882202, Train Acc 0.36125, Val Loss: 1.8586309467043196, Val Acc: 0.32\n",
      "Epoch: 61, Train Loss: 1.6885371160507203, Train Acc 0.345, Val Loss: 1.862901278904506, Val Acc: 0.31\n",
      "Epoch: 62, Train Loss: 1.7117647552490234, Train Acc 0.36875, Val Loss: 1.8572858231408256, Val Acc: 0.3\n",
      "Epoch: 63, Train Loss: 1.6801503038406371, Train Acc 0.37375, Val Loss: 1.8608423471450806, Val Acc: 0.305\n",
      "Epoch: 64, Train Loss: 1.7058498287200927, Train Acc 0.3575, Val Loss: 1.853681938988822, Val Acc: 0.27\n",
      "Epoch: 65, Train Loss: 1.7144171905517578, Train Acc 0.3225, Val Loss: 1.8528388908931188, Val Acc: 0.295\n",
      "Epoch: 66, Train Loss: 1.6870509719848632, Train Acc 0.34375, Val Loss: 1.8574324675968714, Val Acc: 0.275\n",
      "Epoch: 67, Train Loss: 1.695212926864624, Train Acc 0.3625, Val Loss: 1.8708236387797765, Val Acc: 0.285\n",
      "Epoch: 68, Train Loss: 1.6957687711715699, Train Acc 0.37375, Val Loss: 1.8696832997458321, Val Acc: 0.305\n",
      "Epoch: 69, Train Loss: 1.7089659595489501, Train Acc 0.3525, Val Loss: 1.8857367038726807, Val Acc: 0.3\n",
      "Epoch: 70, Train Loss: 1.7055689477920533, Train Acc 0.36, Val Loss: 1.8406367983136858, Val Acc: 0.285\n",
      "Epoch: 71, Train Loss: 1.705232057571411, Train Acc 0.34, Val Loss: 1.8739365168980189, Val Acc: 0.29\n",
      "Epoch: 72, Train Loss: 1.6748232650756836, Train Acc 0.36125, Val Loss: 1.8796388932636805, Val Acc: 0.29\n",
      "Epoch: 73, Train Loss: 1.6895358610153197, Train Acc 0.365, Val Loss: 1.8658581972122192, Val Acc: 0.315\n",
      "Epoch: 74, Train Loss: 1.6845124626159669, Train Acc 0.365, Val Loss: 1.8876981735229492, Val Acc: 0.305\n",
      "Epoch: 75, Train Loss: 1.6973133325576781, Train Acc 0.36, Val Loss: 1.8629504782812936, Val Acc: 0.31\n",
      "Epoch: 76, Train Loss: 1.6598375701904298, Train Acc 0.37125, Val Loss: 1.858802284513201, Val Acc: 0.315\n",
      "Epoch: 77, Train Loss: 1.699057698249817, Train Acc 0.3475, Val Loss: 1.8626365321023124, Val Acc: 0.315\n",
      "Epoch: 78, Train Loss: 1.6784022998809816, Train Acc 0.37375, Val Loss: 1.8473329033170427, Val Acc: 0.305\n",
      "Epoch: 79, Train Loss: 1.6850694608688355, Train Acc 0.3675, Val Loss: 1.8852790253502982, Val Acc: 0.32\n",
      "Epoch: 80, Train Loss: 1.6832176160812378, Train Acc 0.35875, Val Loss: 1.8707467658179147, Val Acc: 0.31\n",
      "Epoch: 81, Train Loss: 1.6769912004470826, Train Acc 0.36875, Val Loss: 1.8464798416410173, Val Acc: 0.31\n",
      "Epoch: 82, Train Loss: 1.6831180953979492, Train Acc 0.3725, Val Loss: 1.8631726162774223, Val Acc: 0.285\n",
      "Epoch: 83, Train Loss: 1.6977481746673584, Train Acc 0.36625, Val Loss: 1.8834939513887679, Val Acc: 0.295\n",
      "Epoch: 84, Train Loss: 1.6747795009613038, Train Acc 0.375, Val Loss: 1.8980155842644828, Val Acc: 0.3\n",
      "Epoch: 85, Train Loss: 1.6788842868804932, Train Acc 0.37, Val Loss: 1.8698647022247314, Val Acc: 0.32\n",
      "Epoch: 86, Train Loss: 1.6737529373168945, Train Acc 0.375, Val Loss: 1.8823348454066686, Val Acc: 0.32\n",
      "Epoch: 87, Train Loss: 1.6697393941879273, Train Acc 0.38125, Val Loss: 1.8569989034107752, Val Acc: 0.32\n",
      "Epoch: 88, Train Loss: 1.6691984510421753, Train Acc 0.38375, Val Loss: 1.8748904126031058, Val Acc: 0.31\n",
      "Epoch: 89, Train Loss: 1.6871134805679322, Train Acc 0.365, Val Loss: 1.8686251470020838, Val Acc: 0.32\n",
      "Epoch: 90, Train Loss: 1.6823862171173096, Train Acc 0.355, Val Loss: 1.9128705944333757, Val Acc: 0.305\n",
      "Epoch: 91, Train Loss: 1.6670064067840575, Train Acc 0.37875, Val Loss: 1.8654919181551253, Val Acc: 0.32\n",
      "Epoch: 92, Train Loss: 1.6585704183578491, Train Acc 0.3675, Val Loss: 1.9060116495404924, Val Acc: 0.295\n",
      "Epoch: 93, Train Loss: 1.6637616872787475, Train Acc 0.35, Val Loss: 1.8788902248655046, Val Acc: 0.31\n",
      "Epoch: 94, Train Loss: 1.6603698587417604, Train Acc 0.3725, Val Loss: 1.8967843396323067, Val Acc: 0.315\n",
      "Epoch: 95, Train Loss: 1.645010371208191, Train Acc 0.40375, Val Loss: 1.8835091420582362, Val Acc: 0.29\n",
      "Epoch: 96, Train Loss: 1.6645154905319215, Train Acc 0.37625, Val Loss: 1.8937804698944092, Val Acc: 0.315\n",
      "Epoch: 97, Train Loss: 1.6696299266815187, Train Acc 0.3675, Val Loss: 1.8889145169939314, Val Acc: 0.29\n",
      "Epoch: 98, Train Loss: 1.6603168392181396, Train Acc 0.3675, Val Loss: 1.8842778035572596, Val Acc: 0.315\n",
      "Epoch: 99, Train Loss: 1.6557776308059693, Train Acc 0.3925, Val Loss: 1.887674365724836, Val Acc: 0.3\n",
      "Epoch: 100, Train Loss: 1.6409220123291015, Train Acc 0.3825, Val Loss: 1.8977072749819075, Val Acc: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "geom_vector_len = 5  # Assuming geom_vector_len is known\n",
    "dense_size = 64  # Size of the dense layer\n",
    "dropout = 0.5  # Dropout rate\n",
    "num_classes = 9  # Number of output classes\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "conv_model = CompareModel(emb_dim=geom_vector_len, dense_size=dense_size, dropout=dropout, output_size=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training process\n",
    "num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for seq_len in train_input_sorted:\n",
    "#         inputs = torch.tensor(train_input_sorted[seq_len], dtype=torch.float32)\n",
    "#         labels = torch.tensor(train_labels_sorted[seq_len], dtype=torch.long)\n",
    "#         dataset = TensorDataset(inputs, labels)\n",
    "#         loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         for batch_x, batch_y in loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             output = conv_model(batch_x)\n",
    "#             loss = criterion(output, batch_y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # Print statistics\n",
    "#             running_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {running_loss/train_geoms.shape[0]}\")\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        if USE_GPU:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            if USE_GPU:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    eval_loss /= len(loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(conv_model, train_loader)\n",
    "    val_loss, val_acc = evaluate(conv_model, val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Train Acc {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
